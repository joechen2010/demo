2025-01-01 00:18:34.613 Could not detect default configuration classes for test class [com.demo.kafka.KafkaSpec]: KafkaSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:18:34.713 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.kafka.KafkaSpec
2025-01-01 00:18:34.981 Registered kafka:type=kafka.Log4jController MBean
2025-01-01 00:18:35.032 
2025-01-01 00:18:35.032   ______                  _                                          
2025-01-01 00:18:35.032  |___  /                 | |                                         
2025-01-01 00:18:35.032     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __   
2025-01-01 00:18:35.035    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__|
2025-01-01 00:18:35.036   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |    
2025-01-01 00:18:35.036  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_|
2025-01-01 00:18:35.036                                               | |                     
2025-01-01 00:18:35.036                                               |_|                     
2025-01-01 00:18:35.036 
2025-01-01 00:18:40.341 Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:18:40.341 Server environment:host.name=Joe
2025-01-01 00:18:40.341 Server environment:java.version=17.0.12
2025-01-01 00:18:40.341 Server environment:java.vendor=Oracle Corporation
2025-01-01 00:18:40.341 Server environment:java.home=C:\work\jdk-17
2025-01-01 00:18:40.341 Server environment:java.class.path=C:\work\ideaIC-2024.2.0.2.win\lib\idea_rt.jar;C:\work\ideaIC-2024.2.0.2.win\plugins\junit\lib\junit5-rt.jar;C:\work\ideaIC-2024.2.0.2.win\plugins\junit\lib\junit-rt.jar;C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\Users\joech\.m2\repository\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\Users\joech\.m2\repository\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\Users\joech\.m2\repository\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\Users\joech\.m2\repository\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\Users\joech\.m2\repository\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\Users\joech\.m2\repository\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\Users\joech\.m2\repository\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\Users\joech\.m2\repository\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\Users\joech\.m2\repository\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\Users\joech\.m2\repository\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\Users\joech\.m2\repository\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\Users\joech\.m2\repository\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\Users\joech\.m2\repository\com\google\guava\guava\19.0\guava-19.0.jar;C:\Users\joech\.m2\repository\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\Users\joech\.m2\repository\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\Users\joech\.m2\repository\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\Users\joech\.m2\repository\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\Users\joech\.m2\repository\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\Users\joech\.m2\repository\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\Users\joech\.m2\repository\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\Users\joech\.m2\repository\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\Users\joech\.m2\repository\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\Users\joech\.m2\repository\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\Users\joech\.m2\repository\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\Users\joech\.m2\repository\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\Users\joech\.m2\repository\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\Users\joech\.m2\repository\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\Users\joech\.m2\repository\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\Users\joech\.m2\repository\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\Users\joech\.m2\repository\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\Users\joech\.m2\repository\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\Users\joech\.m2\repository\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\Users\joech\.m2\repository\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\Users\joech\.m2\repository\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\Users\joech\.m2\repository\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\Users\joech\.m2\repository\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\Users\joech\.m2\repository\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\Users\joech\.m2\repository\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\Users\joech\.m2\repository\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\Users\joech\.m2\repository\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\Users\joech\.m2\repository\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\Users\joech\.m2\repository\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\Users\joech\.m2\repository\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\Users\joech\.m2\repository\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\Users\joech\.m2\repository\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\Users\joech\.m2\repository\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\Users\joech\.m2\repository\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\Users\joech\.m2\repository\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\Users\joech\.m2\repository\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\Users\joech\.m2\repository\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\Users\joech\.m2\repository\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\Users\joech\.m2\repository\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\Users\joech\.m2\repository\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\Users\joech\.m2\repository\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\Users\joech\.m2\repository\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\Users\joech\.m2\repository\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\Users\joech\.m2\repository\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\Users\joech\.m2\repository\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\Users\joech\.m2\repository\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\Users\joech\.m2\repository\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\Users\joech\.m2\repository\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\Users\joech\.m2\repository\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\Users\joech\.m2\repository\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\Users\joech\.m2\repository\jline\jline\2.14.6\jline-2.14.6.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\Users\joech\.m2\repository\junit\junit\4.13.2\junit-4.13.2.jar;C:\Users\joech\.m2\repository\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\Users\joech\.m2\repository\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\Users\joech\.m2\repository\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\Users\joech\.m2\repository\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\Users\joech\.m2\repository\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\Users\joech\.m2\repository\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\Users\joech\.m2\repository\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\Users\joech\.m2\repository\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\Users\joech\.m2\repository\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\Users\joech\.m2\repository\org\json\json\20220320\json-20220320.jar;C:\Users\joech\.m2\repository\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\Users\joech\.m2\repository\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\Users\joech\.m2\repository\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\Users\joech\.m2\repository\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\Users\joech\.m2\repository\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\Users\joech\.m2\repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\Users\joech\.m2\repository\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\Users\joech\.m2\repository\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\Users\joech\.m2\repository\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\Users\joech\.m2\repository\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\Users\joech\.m2\repository\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\Users\joech\.m2\repository\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\Users\joech\.m2\repository\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\Users\joech\.m2\repository\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\Users\joech\.m2\repository\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\Users\joech\.m2\repository\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\Users\joech\.m2\repository\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\Users\joech\.m2\repository\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\Users\joech\.m2\repository\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar
2025-01-01 00:18:40.342 Server environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:18:40.342 Server environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:18:40.342 Server environment:java.compiler=<NA>
2025-01-01 00:18:40.342 Server environment:os.name=Windows 11
2025-01-01 00:18:40.342 Server environment:os.arch=amd64
2025-01-01 00:18:40.342 Server environment:os.version=10.0
2025-01-01 00:18:40.342 Server environment:user.name=joech
2025-01-01 00:18:40.342 Server environment:user.home=C:\Users\joech
2025-01-01 00:18:40.342 Server environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:18:40.342 Server environment:os.memory.free=82MB
2025-01-01 00:18:40.342 Server environment:os.memory.max=15816MB
2025-01-01 00:18:40.342 Server environment:os.memory.total=136MB
2025-01-01 00:18:40.342 zookeeper.enableEagerACLCheck = false
2025-01-01 00:18:40.343 zookeeper.digest.enabled = true
2025-01-01 00:18:40.343 zookeeper.closeSessionTxn.enabled = true
2025-01-01 00:18:40.344 zookeeper.flushDelay=0
2025-01-01 00:18:40.344 zookeeper.maxWriteQueuePollTime=0
2025-01-01 00:18:40.344 zookeeper.maxBatchSize=1000
2025-01-01 00:18:40.344 zookeeper.intBufferStartingSizeBytes = 1024
2025-01-01 00:18:40.345 zookeeper.snapshot.trust.empty : false
2025-01-01 00:18:40.356 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:18:40.356 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:18:40.357 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:18:40.357 zookeeper.commitLogCount=500
2025-01-01 00:18:40.359 Weighed connection throttling is disabled
2025-01-01 00:18:40.360 minSessionTimeout set to 1600
2025-01-01 00:18:40.361 maxSessionTimeout set to 16000
2025-01-01 00:18:40.362 Response cache size is initialized with value 400.
2025-01-01 00:18:40.362 Response cache size is initialized with value 400.
2025-01-01 00:18:40.363 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:18:40.363 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:18:40.363 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:18:40.363 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:18:40.363 zookeeper.pathStats.delay = 5
2025-01-01 00:18:40.363 zookeeper.pathStats.enabled = false
2025-01-01 00:18:40.364 The max bytes for all large requests are set to 104857600
2025-01-01 00:18:40.364 The large request threshold is set to -1
2025-01-01 00:18:40.364 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-11096913564276344400\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-12879722780734636327\version-2
2025-01-01 00:18:40.386 maxCnxns is not configured, using default value 0.
2025-01-01 00:18:40.387 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:18:40.389 binding to port /127.0.0.1:0
2025-01-01 00:18:40.399 zookeeper.snapshot.compression.method = CHECKED
2025-01-01 00:18:40.399 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-12879722780734636327\version-2\snapshot.0
2025-01-01 00:18:40.403 Snapshot loaded in 10 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:18:40.403 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-12879722780734636327\version-2\snapshot.0
2025-01-01 00:18:40.403 Snapshot taken in 1 ms
2025-01-01 00:18:40.410 zookeeper.request_throttler.shutdownTimeout = 10000
2025-01-01 00:18:40.410 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:18:40.533 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60071
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:18:40.546 Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2025-01-01 00:18:40.586 starting
2025-01-01 00:18:40.589 Connecting to zookeeper on 127.0.0.1:60071
2025-01-01 00:18:40.599 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60071.
2025-01-01 00:18:45.836 Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:18:45.836 Client environment:host.name=Joe
2025-01-01 00:18:45.837 Client environment:java.version=17.0.12
2025-01-01 00:18:45.837 Client environment:java.vendor=Oracle Corporation
2025-01-01 00:18:45.837 Client environment:java.home=C:\work\jdk-17
2025-01-01 00:18:45.837 Client environment:java.class.path=C:\work\ideaIC-2024.2.0.2.win\lib\idea_rt.jar;C:\work\ideaIC-2024.2.0.2.win\plugins\junit\lib\junit5-rt.jar;C:\work\ideaIC-2024.2.0.2.win\plugins\junit\lib\junit-rt.jar;C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\Users\joech\.m2\repository\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\Users\joech\.m2\repository\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\Users\joech\.m2\repository\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\Users\joech\.m2\repository\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\Users\joech\.m2\repository\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\Users\joech\.m2\repository\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\Users\joech\.m2\repository\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\Users\joech\.m2\repository\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\Users\joech\.m2\repository\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\Users\joech\.m2\repository\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\Users\joech\.m2\repository\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\Users\joech\.m2\repository\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\Users\joech\.m2\repository\com\google\guava\guava\19.0\guava-19.0.jar;C:\Users\joech\.m2\repository\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\Users\joech\.m2\repository\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\Users\joech\.m2\repository\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\Users\joech\.m2\repository\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\Users\joech\.m2\repository\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\Users\joech\.m2\repository\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\Users\joech\.m2\repository\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\Users\joech\.m2\repository\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\Users\joech\.m2\repository\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\Users\joech\.m2\repository\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\Users\joech\.m2\repository\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\Users\joech\.m2\repository\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\Users\joech\.m2\repository\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\Users\joech\.m2\repository\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\Users\joech\.m2\repository\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\Users\joech\.m2\repository\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\Users\joech\.m2\repository\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\Users\joech\.m2\repository\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\Users\joech\.m2\repository\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\Users\joech\.m2\repository\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\Users\joech\.m2\repository\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\Users\joech\.m2\repository\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\Users\joech\.m2\repository\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\Users\joech\.m2\repository\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\Users\joech\.m2\repository\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\Users\joech\.m2\repository\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\Users\joech\.m2\repository\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\Users\joech\.m2\repository\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\Users\joech\.m2\repository\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\Users\joech\.m2\repository\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\Users\joech\.m2\repository\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\Users\joech\.m2\repository\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\Users\joech\.m2\repository\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\Users\joech\.m2\repository\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\Users\joech\.m2\repository\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\Users\joech\.m2\repository\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\Users\joech\.m2\repository\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\Users\joech\.m2\repository\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\Users\joech\.m2\repository\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\Users\joech\.m2\repository\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\Users\joech\.m2\repository\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\Users\joech\.m2\repository\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\Users\joech\.m2\repository\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\Users\joech\.m2\repository\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\Users\joech\.m2\repository\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\Users\joech\.m2\repository\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\Users\joech\.m2\repository\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\Users\joech\.m2\repository\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\Users\joech\.m2\repository\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\Users\joech\.m2\repository\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\Users\joech\.m2\repository\jline\jline\2.14.6\jline-2.14.6.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\Users\joech\.m2\repository\junit\junit\4.13.2\junit-4.13.2.jar;C:\Users\joech\.m2\repository\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\Users\joech\.m2\repository\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\Users\joech\.m2\repository\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\Users\joech\.m2\repository\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\Users\joech\.m2\repository\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\Users\joech\.m2\repository\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\Users\joech\.m2\repository\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\Users\joech\.m2\repository\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\Users\joech\.m2\repository\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\Users\joech\.m2\repository\org\json\json\20220320\json-20220320.jar;C:\Users\joech\.m2\repository\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\Users\joech\.m2\repository\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\Users\joech\.m2\repository\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\Users\joech\.m2\repository\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\Users\joech\.m2\repository\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\Users\joech\.m2\repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\Users\joech\.m2\repository\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\Users\joech\.m2\repository\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\Users\joech\.m2\repository\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\Users\joech\.m2\repository\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\Users\joech\.m2\repository\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\Users\joech\.m2\repository\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\Users\joech\.m2\repository\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\Users\joech\.m2\repository\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\Users\joech\.m2\repository\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\Users\joech\.m2\repository\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\Users\joech\.m2\repository\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\Users\joech\.m2\repository\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\Users\joech\.m2\repository\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar
2025-01-01 00:18:45.837 Client environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:18:45.837 Client environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:18:45.837 Client environment:java.compiler=<NA>
2025-01-01 00:18:45.837 Client environment:os.name=Windows 11
2025-01-01 00:18:45.837 Client environment:os.arch=amd64
2025-01-01 00:18:45.837 Client environment:os.version=10.0
2025-01-01 00:18:45.837 Client environment:user.name=joech
2025-01-01 00:18:45.837 Client environment:user.home=C:\Users\joech
2025-01-01 00:18:45.837 Client environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:18:45.837 Client environment:os.memory.free=108MB
2025-01-01 00:18:45.837 Client environment:os.memory.max=15816MB
2025-01-01 00:18:45.837 Client environment:os.memory.total=136MB
2025-01-01 00:18:45.840 Initiating client connection, connectString=127.0.0.1:60071 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@23ca36d
2025-01-01 00:18:45.842 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:18:45.848 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:18:45.849 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:18:45.849 Opening socket connection to server /127.0.0.1:60071.
2025-01-01 00:18:45.850 Socket connection established, initiating session, client: /127.0.0.1:60072, server: /127.0.0.1:60071
2025-01-01 00:18:45.854 Creating new log file: log.1
2025-01-01 00:18:45.862 ZooKeeper audit is disabled.
2025-01-01 00:18:45.865 Session establishment complete on server /127.0.0.1:60071, session id = 0x100181e35540000, negotiated timeout = 16000
2025-01-01 00:18:45.868 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:18:46.026 Cluster ID = 17LMvHd6S2OKA_l57AO0Gw
2025-01-01 00:18:46.028 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177\meta.properties
2025-01-01 00:18:46.055 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60071
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:18:46.075 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:18:46.075 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:18:46.075 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:18:46.077 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:18:46.094 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177)
2025-01-01 00:18:46.096 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177 since no clean shutdown file was found
2025-01-01 00:18:46.102 Loaded 0 logs in 8ms.
2025-01-01 00:18:46.102 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:18:46.103 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:18:46.110 Starting the log cleaner
2025-01-01 00:18:46.117 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:18:46.127 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:18:46.134 Feature ZK node at path: /feature does not exist
2025-01-01 00:18:46.151 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:18:46.326 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:18:46.330 Awaiting socket connections on localhost:60073.
2025-01-01 00:18:46.348 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:18:46.352 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:18:46.362 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:18:46.362 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:18:46.363 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:18:46.363 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:18:46.372 [LogDirFailureHandler]: Starting
2025-01-01 00:18:46.382 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:18:46.393 Stat of the created znode at /brokers/ids/0 is: 25,25,1735661926390,1735661926390,1,0,0,72084112060710912,204,0,25

2025-01-01 00:18:46.394 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60073, czxid (broker epoch): 25
2025-01-01 00:18:46.420 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:18:46.422 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:18:46.427 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:18:46.427 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:18:46.433 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:18:46.436 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:18:46.439 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:18:46.439 [GroupCoordinator 0]: Starting up.
2025-01-01 00:18:46.440 Feature ZK node created at path: /feature
2025-01-01 00:18:46.442 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:18:46.457 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:18:46.459 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:18:46.459 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:18:46.460 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:18:46.460 [Controller id=0] Registering handlers
2025-01-01 00:18:46.463 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:18:46.465 [Controller id=0] Deleting isr change notifications
2025-01-01 00:18:46.467 [Controller id=0] Initializing controller context
2025-01-01 00:18:46.476 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:18:46.478 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:18:46.488 [RequestSendThread controllerId=0] Starting
2025-01-01 00:18:46.490 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:18:46.490 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:18:46.490 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:18:46.490 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:18:46.492 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:18:46.492 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:18:46.492 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:18:46.492 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:18:46.493 [Controller id=0] Sending update metadata request
2025-01-01 00:18:46.495 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:18:46.495 [/config/changes-event-process-thread]: Starting
2025-01-01 00:18:46.501 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:18:46.501 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:18:46.501 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:18:46.504 Kafka version: 3.4.1
2025-01-01 00:18:46.504 Kafka commitId: 8a516edc2755df89
2025-01-01 00:18:46.504 Kafka startTimeMs: 1735661926503
2025-01-01 00:18:46.505 [KafkaServer id=0] started
2025-01-01 00:18:46.505 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:18:46.505 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:18:46.506 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:18:46.507 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60073 (id: 0 rack: null) for sending state change requests
2025-01-01 00:18:46.509 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:18:46.510 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60073]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:18:46.512 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:18:46.513 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:18:46.513 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:18:46.513 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:18:46.514 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:18:46.520 [Controller id=0] Starting the controller scheduler
2025-01-01 00:18:46.528 Kafka version: 3.4.1
2025-01-01 00:18:46.528 Kafka commitId: 8a516edc2755df89
2025-01-01 00:18:46.528 Kafka startTimeMs: 1735661926528
2025-01-01 00:18:46.555 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60073 (id: 0 rack: null)
2025-01-01 00:18:46.555 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60073 (id: 0 rack: null)
2025-01-01 00:18:46.586 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0))
2025-01-01 00:18:46.600 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(GTL1oBdJQze830T1kf1GGw),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), pong-message-topic-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:18:46.601 [Controller id=0] New partition creation callback for pong-message-topic-0,pong-message-topic-1
2025-01-01 00:18:46.602 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:18:46.602 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:18:46.602 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:18:46.605 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:18:46.621 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:18:46.622 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:18:46.623 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions
2025-01-01 00:18:46.624 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 2 partitions
2025-01-01 00:18:46.625 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:18:46.628 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:18:46.644 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0, pong-message-topic-1)
2025-01-01 00:18:46.645 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
2025-01-01 00:18:46.695 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177] Loading producer state till offset 0 with message format version 2
2025-01-01 00:18:46.705 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177\pong-message-topic-0 with properties {}
2025-01-01 00:18:46.705 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:18:46.707 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:18:46.708 [Broker id=0] Leader pong-message-topic-0 with topic id Some(GTL1oBdJQze830T1kf1GGw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:18:46.717 [LogLoader partition=pong-message-topic-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177] Loading producer state till offset 0 with message format version 2
2025-01-01 00:18:46.718 Created log for partition pong-message-topic-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177\pong-message-topic-1 with properties {}
2025-01-01 00:18:46.718 [Partition pong-message-topic-1 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-1
2025-01-01 00:18:46.718 [Partition pong-message-topic-1 broker=0] Log loaded for partition pong-message-topic-1 with initial high watermark 0
2025-01-01 00:18:46.718 [Broker id=0] Leader pong-message-topic-1 with topic id Some(GTL1oBdJQze830T1kf1GGw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:18:46.724 [Broker id=0] Finished LeaderAndIsr request in 98ms correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:18:46.730 [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:18:46.735 App info kafka.admin.client for adminclient-1 unregistered
2025-01-01 00:18:46.737 Metrics scheduler closed
2025-01-01 00:18:46.738 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:18:46.738 Metrics reporters closed
2025-01-01 00:18:46.942 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/Users/joech/.m2/repository/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/Users/joech/.m2/repository/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:18:46.962 Starting KafkaSpec using Java 17.0.12 with PID 280 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:18:46.963 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:18:47.465 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:18:47.466 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:18:47.547 Finished Spring Data repository scanning in 74 ms. Found 1 JPA repository interfaces.
2025-01-01 00:18:47.561 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:18:47.562 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:18:47.573 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:18:47.574 Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2025-01-01 00:18:47.944 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:18:48.008 HHH000412: Hibernate ORM core version 6.2.9.Final
2025-01-01 00:18:48.010 HHH000406: Using bytecode reflection optimizer
2025-01-01 00:18:48.106 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:18:48.205 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:18:48.215 HikariPool-1 - Starting...
2025-01-01 00:18:48.314 HikariPool-1 - Added connection conn0: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:18:48.315 HikariPool-1 - Start completed.
2025-01-01 00:18:48.440 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:18:48.799 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:18:48.849 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:18:49.844 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60073]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:18:49.868 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:18:49.868 Kafka version: 3.4.1
2025-01-01 00:18:49.868 Kafka commitId: 8a516edc2755df89
2025-01-01 00:18:49.868 Kafka startTimeMs: 1735661929868
2025-01-01 00:18:49.869 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:18:49.879 Started KafkaSpec in 3.108 seconds (process running for 16.413)
2025-01-01 00:18:49.888 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to GTL1oBdJQze830T1kf1GGw
2025-01-01 00:18:49.888 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to GTL1oBdJQze830T1kf1GGw
2025-01-01 00:18:49.890 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cluster ID: 17LMvHd6S2OKA_l57AO0Gw
2025-01-01 00:18:49.891 Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-01-01 00:18:49.896 [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(RzbQhRkOQiebbEoPUMWNXw),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:18:49.896 [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-01-01 00:18:49.896 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:18:49.896 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:18:49.896 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:18:49.896 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:18:49.896 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:18:49.896 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:18:49.899 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:18:49.904 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:18:49.904 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:18:49.904 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:18:49.904 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:18:49.904 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:18:49.904 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-01-01 00:18:49.904 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-01-01 00:18:49.905 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:18:49.905 [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:18:49.906 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-01-01 00:18:49.906 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-01-01 00:18:49.914 [LogLoader partition=__consumer_offsets-3, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177] Loading producer state till offset 0 with message format version 2
2025-01-01 00:18:49.918 Created log for partition __consumer_offsets-3 in C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177\__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:18:49.919 [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-01-01 00:18:49.919 [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-01-01 00:18:49.919 [Broker id=0] Leader __consumer_offsets-3 with topic id Some(RzbQhRkOQiebbEoPUMWNXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:18:49.926 [LogLoader partition=__consumer_offsets-2, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177] Loading producer state till offset 0 with message format version 2
2025-01-01 00:18:49.927 Created log for partition __consumer_offsets-2 in C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177\__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:18:49.927 [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-01-01 00:18:49.927 [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-01-01 00:18:49.927 [Broker id=0] Leader __consumer_offsets-2 with topic id Some(RzbQhRkOQiebbEoPUMWNXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:18:49.935 [LogLoader partition=__consumer_offsets-4, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177] Loading producer state till offset 0 with message format version 2
2025-01-01 00:18:49.935 Created log for partition __consumer_offsets-4 in C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177\__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:18:49.935 [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-01-01 00:18:49.935 [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-01-01 00:18:49.936 [Broker id=0] Leader __consumer_offsets-4 with topic id Some(RzbQhRkOQiebbEoPUMWNXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:18:49.943 [LogLoader partition=__consumer_offsets-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177] Loading producer state till offset 0 with message format version 2
2025-01-01 00:18:49.945 Created log for partition __consumer_offsets-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177\__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:18:49.945 [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-01-01 00:18:49.945 [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-01-01 00:18:49.945 [Broker id=0] Leader __consumer_offsets-1 with topic id Some(RzbQhRkOQiebbEoPUMWNXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:18:49.954 [LogLoader partition=__consumer_offsets-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177] Loading producer state till offset 0 with message format version 2
2025-01-01 00:18:49.955 Created log for partition __consumer_offsets-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.8125aa35-643c-4384-ae22-f43b9909e7812232377660049926177\__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:18:49.955 [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-01-01 00:18:49.955 [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-01-01 00:18:49.955 [Broker id=0] Leader __consumer_offsets-0 with topic id Some(RzbQhRkOQiebbEoPUMWNXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:18:49.957 [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-01-01 00:18:49.958 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-01-01 00:18:49.959 [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-01-01 00:18:49.959 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-01-01 00:18:49.959 [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-01-01 00:18:49.959 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-01-01 00:18:49.959 [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-01-01 00:18:49.959 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-01-01 00:18:49.959 [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-01-01 00:18:49.959 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-01-01 00:18:49.959 [Broker id=0] Finished LeaderAndIsr request in 54ms correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:18:49.962 [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-01-01 00:18:49.963 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 5 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
2025-01-01 00:18:49.964 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler.
2025-01-01 00:18:49.964 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler.
2025-01-01 00:18:49.964 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler.
2025-01-01 00:18:49.964 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler.
2025-01-01 00:18:49.991 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Discovered group coordinator localhost:60073 (id: 2147483647 rack: null)
2025-01-01 00:18:49.992 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:18:50.007 [GroupCoordinator 0]: Dynamic member with unknown member id joins group pong-service in Empty state. Created a new member id consumer-pong-service-1-5d0b60aa-d3b1-46a8-8695-66cfe4578a51 and request the member to rejoin with this id.
2025-01-01 00:18:50.010 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: need to re-join with the given member-id: consumer-pong-service-1-5d0b60aa-d3b1-46a8-8695-66cfe4578a51
2025-01-01 00:18:50.010 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2025-01-01 00:18:50.010 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:18:50.015 [GroupCoordinator 0]: Preparing to rebalance group pong-service in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-pong-service-1-5d0b60aa-d3b1-46a8-8695-66cfe4578a51 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)
2025-01-01 00:18:50.018 [GroupCoordinator 0]: Stabilized group pong-service generation 1 (__consumer_offsets-2) with 1 members
2025-01-01 00:18:50.019 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully joined group with generation Generation{generationId=1, memberId='consumer-pong-service-1-5d0b60aa-d3b1-46a8-8695-66cfe4578a51', protocol='range'}
2025-01-01 00:18:50.020 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Finished assignment for group at generation 1: {consumer-pong-service-1-5d0b60aa-d3b1-46a8-8695-66cfe4578a51=Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])}
2025-01-01 00:18:50.025 [GroupCoordinator 0]: Assignment received from leader consumer-pong-service-1-5d0b60aa-d3b1-46a8-8695-66cfe4578a51 for group pong-service for generation 1. The group has 1 members, 0 of which are static.
2025-01-01 00:18:50.057 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully synced group in generation Generation{generationId=1, memberId='consumer-pong-service-1-5d0b60aa-d3b1-46a8-8695-66cfe4578a51', protocol='range'}
2025-01-01 00:18:50.057 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Notifying assignor about the new Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])
2025-01-01 00:18:50.059 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Adding newly assigned partitions: pong-message-topic-0, pong-message-topic-1
2025-01-01 00:18:50.063 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:18:50.063 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:18:50.067 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:18:50.067 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:18:50.072 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60073 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:18:50.073 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60073 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:18:50.081 pong-service: partitions assigned: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:18:50.334 Idempotence will be disabled because acks is set to 0, not set to 'all'.
2025-01-01 00:18:50.334 ProducerConfig values: 
	acks = 0
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:60073]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-01-01 00:18:50.344 Kafka version: 3.4.1
2025-01-01 00:18:50.344 Kafka commitId: 8a516edc2755df89
2025-01-01 00:18:50.344 Kafka startTimeMs: 1735661930344
2025-01-01 00:18:50.347 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to GTL1oBdJQze830T1kf1GGw
2025-01-01 00:18:50.347 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to GTL1oBdJQze830T1kf1GGw
2025-01-01 00:18:50.347 [Producer clientId=producer-1] Cluster ID: 17LMvHd6S2OKA_l57AO0Gw
2025-01-01 00:18:50.500 [KafkaServer id=0] shutting down
2025-01-01 00:18:50.501 [KafkaServer id=0] Starting controlled shutdown
2025-01-01 00:18:50.505 [Controller id=0] Shutting down broker 0
2025-01-01 00:18:50.506 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:18:50.508 [KafkaServer id=0] Controlled shutdown request returned successfully after 5ms
2025-01-01 00:18:50.509 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:18:50.509 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:18:50.509 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:18:50.509 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:18:50.511 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:18:50.511 [Producer clientId=producer-1] Node -1 disconnected.
2025-01-01 00:18:50.511 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:18:50.511 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:18:50.511 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cancelled in-flight FETCH request with correlation id 15 due to node 0 being disconnected (elapsed time since creation: 137ms, elapsed time since send: 137ms, request timeout: 30000ms)
2025-01-01 00:18:50.511 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cancelled in-flight METADATA request with correlation id 17 due to node 0 being disconnected (elapsed time since creation: 40ms, elapsed time since send: 40ms, request timeout: 30000ms)
2025-01-01 00:18:50.512 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 2147483647 disconnected.
2025-01-01 00:18:50.512 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:18:50.512 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:18:50.512 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:18:50.512 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Error sending fetch request (sessionId=1587394522, epoch=1) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
2025-01-01 00:18:50.515 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Group coordinator localhost:60073 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2025-01-01 00:18:50.515 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:18:50.517 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:18:50.517 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:18:50.517 [KafkaApi-0] Shutdown complete.
2025-01-01 00:18:50.518 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:18:50.518 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:18:50.518 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:18:50.519 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:18:50.519 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:18:50.519 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:18:50.519 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:18:50.519 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:18:50.520 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:18:50.520 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:18:50.521 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:18:50.521 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:18:50.521 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:18:50.521 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:18:50.521 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:18:50.521 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:18:50.521 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:18:50.522 [ReplicaManager broker=0] Shutting down
2025-01-01 00:18:50.522 [LogDirFailureHandler]: Shutting down
2025-01-01 00:18:50.522 [LogDirFailureHandler]: Stopped
2025-01-01 00:18:50.522 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:18:50.522 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:18:50.523 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:18:50.524 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:18:50.524 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:18:50.524 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:18:50.524 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:18:50.524 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:18:50.524 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:18:50.524 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:18:50.524 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:18:50.524 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:18:50.525 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:18:50.525 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:18:50.525 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:18:50.525 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:18:50.525 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:18:50.530 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:18:50.530 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:18:50.530 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:18:50.530 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:18:50.531 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:18:50.531 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:18:50.531 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:18:50.531 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:18:50.531 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:18:50.531 Shutting down.
2025-01-01 00:18:50.532 Shutting down the log cleaner.
2025-01-01 00:18:50.532 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:18:50.533 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:18:50.533 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:18:50.554 [ProducerStateManager partition=pong-message-topic-1] Wrote producer snapshot at offset 1 with 0 producer ids in 2 ms.
2025-01-01 00:18:50.562 [ProducerStateManager partition=__consumer_offsets-2] Wrote producer snapshot at offset 4 with 0 producer ids in 1 ms.
2025-01-01 00:18:50.572 Shutdown complete.
2025-01-01 00:18:50.572 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:18:50.572 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:18:50.572 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:18:50.574 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:18:50.574 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:18:50.575 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:18:50.575 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:18:50.575 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:18:50.576 [Controller id=0] Resigned
2025-01-01 00:18:50.576 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:18:50.576 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:18:50.576 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:18:50.577 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:18:50.600 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:18:50.600 [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:60073) could not be established. Broker may not be available.
2025-01-01 00:18:50.612 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:18:50.612 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60073) could not be established. Broker may not be available.
2025-01-01 00:18:50.680 Session: 0x100181e35540000 closed
2025-01-01 00:18:50.681 EventThread shut down for session: 0x100181e35540000
2025-01-01 00:18:50.681 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:18:50.682 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:18:50.682 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:18:50.682 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:18:50.682 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:18:50.683 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:18:50.683 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:18:50.683 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:18:50.683 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:18:50.683 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:18:50.683 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:18:50.683 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:18:50.683 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:18:50.683 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:18:50.689 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:18:50.690 Metrics scheduler closed
2025-01-01 00:18:50.690 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:18:50.690 Metrics reporters closed
2025-01-01 00:18:50.690 Broker and topic stats closed
2025-01-01 00:18:50.690 App info kafka.server for 0 unregistered
2025-01-01 00:18:50.690 [KafkaServer id=0] shut down completed
2025-01-01 00:18:50.698 ConnnectionExpirerThread interrupted
2025-01-01 00:18:50.698 selector thread exitted run method
2025-01-01 00:18:50.698 accept thread exitted run method
2025-01-01 00:18:50.698 selector thread exitted run method
2025-01-01 00:18:50.698 shutting down
2025-01-01 00:18:50.699 Shutting down
2025-01-01 00:18:50.699 Draining request throttler queue
2025-01-01 00:18:50.699 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:18:50.699 Shutting down
2025-01-01 00:18:50.699 Shutting down
2025-01-01 00:18:50.699 Shutting down
2025-01-01 00:18:50.699 PrepRequestProcessor exited loop!
2025-01-01 00:18:50.699 SyncRequestProcessor exited!
2025-01-01 00:18:50.699 shutdown of request processor complete
2025-01-01 00:18:50.711 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Revoke previously assigned partitions pong-message-topic-0, pong-message-topic-1
2025-01-01 00:18:50.711 pong-service: partitions revoked: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:18:50.712 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:18:50.712 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:18:50.712 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:18:50.712 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:18:50.713 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:18:50.713 Metrics scheduler closed
2025-01-01 00:18:50.713 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:18:50.713 Metrics reporters closed
2025-01-01 00:18:50.713 App info kafka.consumer for consumer-pong-service-1 unregistered
2025-01-01 00:18:50.713 pong-service: Consumer stopped
2025-01-01 00:18:50.714 [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-01 00:18:50.715 Metrics scheduler closed
2025-01-01 00:18:50.715 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:18:50.715 Metrics reporters closed
2025-01-01 00:18:50.715 App info kafka.producer for producer-1 unregistered
2025-01-01 00:18:50.728 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:18:50.729 HikariPool-1 - Shutdown initiated...
2025-01-01 00:18:50.730 HikariPool-1 - Shutdown completed.
2025-01-01 00:21:14.423 Could not detect default configuration classes for test class [com.demo.kafka.KafkaSpec]: KafkaSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:21:14.484 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.kafka.KafkaSpec
2025-01-01 00:21:14.756 Registered kafka:type=kafka.Log4jController MBean
2025-01-01 00:21:14.808 
2025-01-01 00:21:14.809   ______                  _                                          
2025-01-01 00:21:14.809  |___  /                 | |                                         
2025-01-01 00:21:14.809     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __   
2025-01-01 00:21:14.809    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__|
2025-01-01 00:21:14.809   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |    
2025-01-01 00:21:14.809  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_|
2025-01-01 00:21:14.809                                               | |                     
2025-01-01 00:21:14.809                                               |_|                     
2025-01-01 00:21:14.809 
2025-01-01 00:21:20.065 Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:21:20.065 Server environment:host.name=Joe
2025-01-01 00:21:20.065 Server environment:java.version=17.0.12
2025-01-01 00:21:20.065 Server environment:java.vendor=Oracle Corporation
2025-01-01 00:21:20.065 Server environment:java.home=C:\work\jdk-17
2025-01-01 00:21:20.065 Server environment:java.class.path=C:\work\ideaIC-2024.2.0.2.win\lib\idea_rt.jar;C:\work\ideaIC-2024.2.0.2.win\plugins\junit\lib\junit5-rt.jar;C:\work\ideaIC-2024.2.0.2.win\plugins\junit\lib\junit-rt.jar;C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\Users\joech\.m2\repository\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\Users\joech\.m2\repository\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\Users\joech\.m2\repository\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\Users\joech\.m2\repository\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\Users\joech\.m2\repository\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\Users\joech\.m2\repository\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\Users\joech\.m2\repository\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\Users\joech\.m2\repository\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\Users\joech\.m2\repository\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\Users\joech\.m2\repository\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\Users\joech\.m2\repository\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\Users\joech\.m2\repository\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\Users\joech\.m2\repository\com\google\guava\guava\19.0\guava-19.0.jar;C:\Users\joech\.m2\repository\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\Users\joech\.m2\repository\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\Users\joech\.m2\repository\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\Users\joech\.m2\repository\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\Users\joech\.m2\repository\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\Users\joech\.m2\repository\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\Users\joech\.m2\repository\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\Users\joech\.m2\repository\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\Users\joech\.m2\repository\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\Users\joech\.m2\repository\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\Users\joech\.m2\repository\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\Users\joech\.m2\repository\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\Users\joech\.m2\repository\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\Users\joech\.m2\repository\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\Users\joech\.m2\repository\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\Users\joech\.m2\repository\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\Users\joech\.m2\repository\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\Users\joech\.m2\repository\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\Users\joech\.m2\repository\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\Users\joech\.m2\repository\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\Users\joech\.m2\repository\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\Users\joech\.m2\repository\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\Users\joech\.m2\repository\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\Users\joech\.m2\repository\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\Users\joech\.m2\repository\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\Users\joech\.m2\repository\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\Users\joech\.m2\repository\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\Users\joech\.m2\repository\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\Users\joech\.m2\repository\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\Users\joech\.m2\repository\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\Users\joech\.m2\repository\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\Users\joech\.m2\repository\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\Users\joech\.m2\repository\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\Users\joech\.m2\repository\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\Users\joech\.m2\repository\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\Users\joech\.m2\repository\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\Users\joech\.m2\repository\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\Users\joech\.m2\repository\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\Users\joech\.m2\repository\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\Users\joech\.m2\repository\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\Users\joech\.m2\repository\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\Users\joech\.m2\repository\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\Users\joech\.m2\repository\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\Users\joech\.m2\repository\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\Users\joech\.m2\repository\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\Users\joech\.m2\repository\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\Users\joech\.m2\repository\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\Users\joech\.m2\repository\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\Users\joech\.m2\repository\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\Users\joech\.m2\repository\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\Users\joech\.m2\repository\jline\jline\2.14.6\jline-2.14.6.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\Users\joech\.m2\repository\junit\junit\4.13.2\junit-4.13.2.jar;C:\Users\joech\.m2\repository\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\Users\joech\.m2\repository\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\Users\joech\.m2\repository\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\Users\joech\.m2\repository\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\Users\joech\.m2\repository\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\Users\joech\.m2\repository\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\Users\joech\.m2\repository\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\Users\joech\.m2\repository\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\Users\joech\.m2\repository\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\Users\joech\.m2\repository\org\json\json\20220320\json-20220320.jar;C:\Users\joech\.m2\repository\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\Users\joech\.m2\repository\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\Users\joech\.m2\repository\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\Users\joech\.m2\repository\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\Users\joech\.m2\repository\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\Users\joech\.m2\repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\Users\joech\.m2\repository\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\Users\joech\.m2\repository\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\Users\joech\.m2\repository\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\Users\joech\.m2\repository\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\Users\joech\.m2\repository\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\Users\joech\.m2\repository\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\Users\joech\.m2\repository\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\Users\joech\.m2\repository\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\Users\joech\.m2\repository\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\Users\joech\.m2\repository\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\Users\joech\.m2\repository\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\Users\joech\.m2\repository\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\Users\joech\.m2\repository\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar
2025-01-01 00:21:20.065 Server environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:21:20.065 Server environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:21:20.065 Server environment:java.compiler=<NA>
2025-01-01 00:21:20.065 Server environment:os.name=Windows 11
2025-01-01 00:21:20.065 Server environment:os.arch=amd64
2025-01-01 00:21:20.066 Server environment:os.version=10.0
2025-01-01 00:21:20.066 Server environment:user.name=joech
2025-01-01 00:21:20.066 Server environment:user.home=C:\Users\joech
2025-01-01 00:21:20.066 Server environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:21:20.066 Server environment:os.memory.free=83MB
2025-01-01 00:21:20.066 Server environment:os.memory.max=15816MB
2025-01-01 00:21:20.066 Server environment:os.memory.total=136MB
2025-01-01 00:21:20.066 zookeeper.enableEagerACLCheck = false
2025-01-01 00:21:20.067 zookeeper.digest.enabled = true
2025-01-01 00:21:20.067 zookeeper.closeSessionTxn.enabled = true
2025-01-01 00:21:20.067 zookeeper.flushDelay=0
2025-01-01 00:21:20.067 zookeeper.maxWriteQueuePollTime=0
2025-01-01 00:21:20.067 zookeeper.maxBatchSize=1000
2025-01-01 00:21:20.067 zookeeper.intBufferStartingSizeBytes = 1024
2025-01-01 00:21:20.068 zookeeper.snapshot.trust.empty : false
2025-01-01 00:21:20.078 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:21:20.079 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:21:20.079 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:21:20.079 zookeeper.commitLogCount=500
2025-01-01 00:21:20.081 Weighed connection throttling is disabled
2025-01-01 00:21:20.082 minSessionTimeout set to 1600
2025-01-01 00:21:20.082 maxSessionTimeout set to 16000
2025-01-01 00:21:20.082 Response cache size is initialized with value 400.
2025-01-01 00:21:20.082 Response cache size is initialized with value 400.
2025-01-01 00:21:20.083 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:21:20.083 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:21:20.083 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:21:20.083 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:21:20.083 zookeeper.pathStats.delay = 5
2025-01-01 00:21:20.083 zookeeper.pathStats.enabled = false
2025-01-01 00:21:20.084 The max bytes for all large requests are set to 104857600
2025-01-01 00:21:20.084 The large request threshold is set to -1
2025-01-01 00:21:20.084 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-12007815853977718661\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-9392990401778285640\version-2
2025-01-01 00:21:20.107 maxCnxns is not configured, using default value 0.
2025-01-01 00:21:20.108 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:21:20.113 binding to port /127.0.0.1:0
2025-01-01 00:21:20.126 zookeeper.snapshot.compression.method = CHECKED
2025-01-01 00:21:20.126 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-9392990401778285640\version-2\snapshot.0
2025-01-01 00:21:20.130 Snapshot loaded in 14 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:21:20.130 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-9392990401778285640\version-2\snapshot.0
2025-01-01 00:21:20.131 Snapshot taken in 0 ms
2025-01-01 00:21:20.136 zookeeper.request_throttler.shutdownTimeout = 10000
2025-01-01 00:21:20.136 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:21:20.258 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60230
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:21:20.271 Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2025-01-01 00:21:20.311 starting
2025-01-01 00:21:20.311 Connecting to zookeeper on 127.0.0.1:60230
2025-01-01 00:21:20.324 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60230.
2025-01-01 00:21:25.549 Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:21:25.549 Client environment:host.name=Joe
2025-01-01 00:21:25.549 Client environment:java.version=17.0.12
2025-01-01 00:21:25.549 Client environment:java.vendor=Oracle Corporation
2025-01-01 00:21:25.549 Client environment:java.home=C:\work\jdk-17
2025-01-01 00:21:25.549 Client environment:java.class.path=C:\work\ideaIC-2024.2.0.2.win\lib\idea_rt.jar;C:\work\ideaIC-2024.2.0.2.win\plugins\junit\lib\junit5-rt.jar;C:\work\ideaIC-2024.2.0.2.win\plugins\junit\lib\junit-rt.jar;C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\Users\joech\.m2\repository\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\Users\joech\.m2\repository\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\Users\joech\.m2\repository\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\Users\joech\.m2\repository\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\Users\joech\.m2\repository\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\Users\joech\.m2\repository\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\Users\joech\.m2\repository\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\Users\joech\.m2\repository\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\Users\joech\.m2\repository\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\Users\joech\.m2\repository\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\Users\joech\.m2\repository\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\Users\joech\.m2\repository\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\Users\joech\.m2\repository\com\google\guava\guava\19.0\guava-19.0.jar;C:\Users\joech\.m2\repository\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\Users\joech\.m2\repository\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\Users\joech\.m2\repository\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\Users\joech\.m2\repository\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\Users\joech\.m2\repository\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\Users\joech\.m2\repository\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\Users\joech\.m2\repository\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\Users\joech\.m2\repository\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\Users\joech\.m2\repository\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\Users\joech\.m2\repository\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\Users\joech\.m2\repository\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\Users\joech\.m2\repository\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\Users\joech\.m2\repository\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\Users\joech\.m2\repository\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\Users\joech\.m2\repository\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\Users\joech\.m2\repository\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\Users\joech\.m2\repository\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\Users\joech\.m2\repository\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\Users\joech\.m2\repository\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\Users\joech\.m2\repository\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\Users\joech\.m2\repository\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\Users\joech\.m2\repository\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\Users\joech\.m2\repository\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\Users\joech\.m2\repository\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\Users\joech\.m2\repository\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\Users\joech\.m2\repository\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\Users\joech\.m2\repository\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\Users\joech\.m2\repository\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\Users\joech\.m2\repository\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\Users\joech\.m2\repository\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\Users\joech\.m2\repository\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\Users\joech\.m2\repository\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\Users\joech\.m2\repository\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\Users\joech\.m2\repository\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\Users\joech\.m2\repository\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\Users\joech\.m2\repository\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\Users\joech\.m2\repository\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\Users\joech\.m2\repository\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\Users\joech\.m2\repository\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\Users\joech\.m2\repository\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\Users\joech\.m2\repository\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\Users\joech\.m2\repository\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\Users\joech\.m2\repository\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\Users\joech\.m2\repository\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\Users\joech\.m2\repository\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\Users\joech\.m2\repository\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\Users\joech\.m2\repository\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\Users\joech\.m2\repository\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\Users\joech\.m2\repository\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\Users\joech\.m2\repository\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\Users\joech\.m2\repository\jline\jline\2.14.6\jline-2.14.6.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\Users\joech\.m2\repository\junit\junit\4.13.2\junit-4.13.2.jar;C:\Users\joech\.m2\repository\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\Users\joech\.m2\repository\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\Users\joech\.m2\repository\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\Users\joech\.m2\repository\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\Users\joech\.m2\repository\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\Users\joech\.m2\repository\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\Users\joech\.m2\repository\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\Users\joech\.m2\repository\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\Users\joech\.m2\repository\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\Users\joech\.m2\repository\org\json\json\20220320\json-20220320.jar;C:\Users\joech\.m2\repository\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\Users\joech\.m2\repository\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\Users\joech\.m2\repository\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\Users\joech\.m2\repository\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\Users\joech\.m2\repository\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\Users\joech\.m2\repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\Users\joech\.m2\repository\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\Users\joech\.m2\repository\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\Users\joech\.m2\repository\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\Users\joech\.m2\repository\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\Users\joech\.m2\repository\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\Users\joech\.m2\repository\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\Users\joech\.m2\repository\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\Users\joech\.m2\repository\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\Users\joech\.m2\repository\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\Users\joech\.m2\repository\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\Users\joech\.m2\repository\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\Users\joech\.m2\repository\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\Users\joech\.m2\repository\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar
2025-01-01 00:21:25.550 Client environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:21:25.550 Client environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:21:25.550 Client environment:java.compiler=<NA>
2025-01-01 00:21:25.550 Client environment:os.name=Windows 11
2025-01-01 00:21:25.550 Client environment:os.arch=amd64
2025-01-01 00:21:25.550 Client environment:os.version=10.0
2025-01-01 00:21:25.550 Client environment:user.name=joech
2025-01-01 00:21:25.550 Client environment:user.home=C:\Users\joech
2025-01-01 00:21:25.550 Client environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:21:25.550 Client environment:os.memory.free=109MB
2025-01-01 00:21:25.550 Client environment:os.memory.max=15816MB
2025-01-01 00:21:25.550 Client environment:os.memory.total=136MB
2025-01-01 00:21:25.552 Initiating client connection, connectString=127.0.0.1:60230 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@4877919f
2025-01-01 00:21:25.554 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:21:25.560 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:21:25.561 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:21:25.561 Opening socket connection to server /127.0.0.1:60230.
2025-01-01 00:21:25.563 Socket connection established, initiating session, client: /127.0.0.1:60231, server: /127.0.0.1:60230
2025-01-01 00:21:25.569 Creating new log file: log.1
2025-01-01 00:21:25.574 ZooKeeper audit is disabled.
2025-01-01 00:21:25.576 Session establishment complete on server /127.0.0.1:60230, session id = 0x1001820a5440000, negotiated timeout = 16000
2025-01-01 00:21:25.579 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:21:25.722 Cluster ID = TQCi_up5R8uZSu7_zeBHRw
2025-01-01 00:21:25.724 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532\meta.properties
2025-01-01 00:21:25.749 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60230
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:21:25.772 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:21:25.773 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:21:25.773 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:21:25.774 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:21:25.792 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532)
2025-01-01 00:21:25.794 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532 since no clean shutdown file was found
2025-01-01 00:21:25.799 Loaded 0 logs in 8ms.
2025-01-01 00:21:25.800 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:21:25.801 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:21:25.809 Starting the log cleaner
2025-01-01 00:21:25.816 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:21:25.824 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:21:25.830 Feature ZK node at path: /feature does not exist
2025-01-01 00:21:25.845 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:21:26.021 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:21:26.025 Awaiting socket connections on localhost:60232.
2025-01-01 00:21:26.041 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:21:26.045 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:21:26.054 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:21:26.055 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:21:26.055 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:21:26.055 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:21:26.065 [LogDirFailureHandler]: Starting
2025-01-01 00:21:26.076 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:21:26.088 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662086084,1735662086084,1,0,0,72084122528645120,204,0,25

2025-01-01 00:21:26.088 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60232, czxid (broker epoch): 25
2025-01-01 00:21:26.118 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:21:26.120 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:21:26.122 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:21:26.124 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:21:26.129 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:21:26.132 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:21:26.132 [GroupCoordinator 0]: Starting up.
2025-01-01 00:21:26.134 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:21:26.134 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:21:26.136 Feature ZK node created at path: /feature
2025-01-01 00:21:26.149 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:21:26.152 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:21:26.152 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:21:26.156 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:21:26.156 [Controller id=0] Registering handlers
2025-01-01 00:21:26.158 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:21:26.159 [Controller id=0] Deleting isr change notifications
2025-01-01 00:21:26.160 [Controller id=0] Initializing controller context
2025-01-01 00:21:26.169 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:21:26.169 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:21:26.181 [RequestSendThread controllerId=0] Starting
2025-01-01 00:21:26.182 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:21:26.182 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:21:26.183 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:21:26.183 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:21:26.184 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:21:26.184 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:21:26.185 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:21:26.185 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:21:26.186 [Controller id=0] Sending update metadata request
2025-01-01 00:21:26.186 [/config/changes-event-process-thread]: Starting
2025-01-01 00:21:26.189 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:21:26.192 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:21:26.193 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:21:26.194 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:21:26.196 Kafka version: 3.4.1
2025-01-01 00:21:26.196 Kafka commitId: 8a516edc2755df89
2025-01-01 00:21:26.196 Kafka startTimeMs: 1735662086194
2025-01-01 00:21:26.197 [KafkaServer id=0] started
2025-01-01 00:21:26.198 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:21:26.198 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:21:26.199 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:21:26.201 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60232 (id: 0 rack: null) for sending state change requests
2025-01-01 00:21:26.201 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60232]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:21:26.202 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:21:26.206 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:21:26.206 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:21:26.206 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:21:26.206 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:21:26.207 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:21:26.215 [Controller id=0] Starting the controller scheduler
2025-01-01 00:21:26.217 Kafka version: 3.4.1
2025-01-01 00:21:26.219 Kafka commitId: 8a516edc2755df89
2025-01-01 00:21:26.219 Kafka startTimeMs: 1735662086217
2025-01-01 00:21:26.248 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60232 (id: 0 rack: null)
2025-01-01 00:21:26.249 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60232 (id: 0 rack: null)
2025-01-01 00:21:26.275 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0))
2025-01-01 00:21:26.289 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(bVK4NOSBSQOISuK-59pXDQ),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), pong-message-topic-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:21:26.290 [Controller id=0] New partition creation callback for pong-message-topic-0,pong-message-topic-1
2025-01-01 00:21:26.292 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:26.292 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:26.292 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:26.295 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:26.309 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:26.309 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:26.311 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions
2025-01-01 00:21:26.313 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 2 partitions
2025-01-01 00:21:26.314 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:26.317 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:21:26.335 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0, pong-message-topic-1)
2025-01-01 00:21:26.336 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
2025-01-01 00:21:26.379 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:26.392 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532\pong-message-topic-0 with properties {}
2025-01-01 00:21:26.394 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:21:26.394 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:21:26.395 [Broker id=0] Leader pong-message-topic-0 with topic id Some(bVK4NOSBSQOISuK-59pXDQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:26.404 [LogLoader partition=pong-message-topic-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:26.404 Created log for partition pong-message-topic-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532\pong-message-topic-1 with properties {}
2025-01-01 00:21:26.404 [Partition pong-message-topic-1 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-1
2025-01-01 00:21:26.404 [Partition pong-message-topic-1 broker=0] Log loaded for partition pong-message-topic-1 with initial high watermark 0
2025-01-01 00:21:26.404 [Broker id=0] Leader pong-message-topic-1 with topic id Some(bVK4NOSBSQOISuK-59pXDQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:26.410 [Broker id=0] Finished LeaderAndIsr request in 95ms correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:21:26.415 [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:21:26.420 App info kafka.admin.client for adminclient-1 unregistered
2025-01-01 00:21:26.422 Metrics scheduler closed
2025-01-01 00:21:26.422 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:21:26.422 Metrics reporters closed
2025-01-01 00:21:26.591 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/Users/joech/.m2/repository/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/Users/joech/.m2/repository/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:21:26.603 Starting KafkaSpec using Java 17.0.12 with PID 11336 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:21:26.604 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:21:27.002 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:21:27.004 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:21:27.087 Finished Spring Data repository scanning in 78 ms. Found 1 JPA repository interfaces.
2025-01-01 00:21:27.099 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:21:27.100 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:21:27.109 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:21:27.109 Finished Spring Data repository scanning in 3 ms. Found 0 Redis repository interfaces.
2025-01-01 00:21:27.427 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:21:27.483 HHH000412: Hibernate ORM core version 6.2.9.Final
2025-01-01 00:21:27.486 HHH000406: Using bytecode reflection optimizer
2025-01-01 00:21:27.602 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:21:27.685 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:21:27.695 HikariPool-1 - Starting...
2025-01-01 00:21:27.794 HikariPool-1 - Added connection conn0: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:21:27.795 HikariPool-1 - Start completed.
2025-01-01 00:21:27.919 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:21:28.274 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:21:28.317 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:21:29.326 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60232]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:21:29.355 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:21:29.355 Kafka version: 3.4.1
2025-01-01 00:21:29.355 Kafka commitId: 8a516edc2755df89
2025-01-01 00:21:29.355 Kafka startTimeMs: 1735662089355
2025-01-01 00:21:29.355 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:21:29.366 Started KafkaSpec in 2.912 seconds (process running for 16.127)
2025-01-01 00:21:29.373 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to bVK4NOSBSQOISuK-59pXDQ
2025-01-01 00:21:29.373 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to bVK4NOSBSQOISuK-59pXDQ
2025-01-01 00:21:29.374 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cluster ID: TQCi_up5R8uZSu7_zeBHRw
2025-01-01 00:21:29.376 Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-01-01 00:21:29.382 [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(ntMy9IYDSTWRPKfh_YyuXw),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:21:29.382 [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-01-01 00:21:29.382 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:29.383 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:29.383 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:29.383 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:29.383 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:29.383 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:29.383 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:29.388 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:29.388 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:29.388 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:29.388 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:29.388 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:29.389 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-01-01 00:21:29.389 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-01-01 00:21:29.389 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:29.389 [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:21:29.391 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-01-01 00:21:29.391 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-01-01 00:21:29.399 [LogLoader partition=__consumer_offsets-3, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:29.400 Created log for partition __consumer_offsets-3 in C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532\__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:21:29.401 [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-01-01 00:21:29.401 [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-01-01 00:21:29.401 [Broker id=0] Leader __consumer_offsets-3 with topic id Some(ntMy9IYDSTWRPKfh_YyuXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:29.408 [LogLoader partition=__consumer_offsets-2, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:29.408 Created log for partition __consumer_offsets-2 in C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532\__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:21:29.408 [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-01-01 00:21:29.408 [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-01-01 00:21:29.408 [Broker id=0] Leader __consumer_offsets-2 with topic id Some(ntMy9IYDSTWRPKfh_YyuXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:29.417 [LogLoader partition=__consumer_offsets-4, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:29.418 Created log for partition __consumer_offsets-4 in C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532\__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:21:29.418 [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-01-01 00:21:29.418 [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-01-01 00:21:29.418 [Broker id=0] Leader __consumer_offsets-4 with topic id Some(ntMy9IYDSTWRPKfh_YyuXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:29.426 [LogLoader partition=__consumer_offsets-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:29.426 Created log for partition __consumer_offsets-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532\__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:21:29.426 [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-01-01 00:21:29.426 [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-01-01 00:21:29.427 [Broker id=0] Leader __consumer_offsets-1 with topic id Some(ntMy9IYDSTWRPKfh_YyuXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:29.436 [LogLoader partition=__consumer_offsets-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:29.437 Created log for partition __consumer_offsets-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.63d5de74-d80f-463e-a33c-13e29b6e06d617110143821538813532\__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:21:29.437 [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-01-01 00:21:29.437 [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-01-01 00:21:29.437 [Broker id=0] Leader __consumer_offsets-0 with topic id Some(ntMy9IYDSTWRPKfh_YyuXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:29.440 [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-01-01 00:21:29.440 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-01-01 00:21:29.442 [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-01-01 00:21:29.442 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-01-01 00:21:29.442 [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-01-01 00:21:29.442 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-01-01 00:21:29.442 [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-01-01 00:21:29.442 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-01-01 00:21:29.442 [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-01-01 00:21:29.442 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-01-01 00:21:29.442 [Broker id=0] Finished LeaderAndIsr request in 53ms correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:21:29.443 [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-01-01 00:21:29.445 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 4 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler.
2025-01-01 00:21:29.445 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
2025-01-01 00:21:29.445 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
2025-01-01 00:21:29.445 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
2025-01-01 00:21:29.445 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
2025-01-01 00:21:29.476 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Discovered group coordinator localhost:60232 (id: 2147483647 rack: null)
2025-01-01 00:21:29.477 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:21:29.491 [GroupCoordinator 0]: Dynamic member with unknown member id joins group pong-service in Empty state. Created a new member id consumer-pong-service-1-43352028-5496-4acc-b299-20ac4855d356 and request the member to rejoin with this id.
2025-01-01 00:21:29.495 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: need to re-join with the given member-id: consumer-pong-service-1-43352028-5496-4acc-b299-20ac4855d356
2025-01-01 00:21:29.496 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2025-01-01 00:21:29.496 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:21:29.500 [GroupCoordinator 0]: Preparing to rebalance group pong-service in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-pong-service-1-43352028-5496-4acc-b299-20ac4855d356 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)
2025-01-01 00:21:29.504 [GroupCoordinator 0]: Stabilized group pong-service generation 1 (__consumer_offsets-2) with 1 members
2025-01-01 00:21:29.505 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully joined group with generation Generation{generationId=1, memberId='consumer-pong-service-1-43352028-5496-4acc-b299-20ac4855d356', protocol='range'}
2025-01-01 00:21:29.506 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Finished assignment for group at generation 1: {consumer-pong-service-1-43352028-5496-4acc-b299-20ac4855d356=Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])}
2025-01-01 00:21:29.512 [GroupCoordinator 0]: Assignment received from leader consumer-pong-service-1-43352028-5496-4acc-b299-20ac4855d356 for group pong-service for generation 1. The group has 1 members, 0 of which are static.
2025-01-01 00:21:29.548 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully synced group in generation Generation{generationId=1, memberId='consumer-pong-service-1-43352028-5496-4acc-b299-20ac4855d356', protocol='range'}
2025-01-01 00:21:29.548 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Notifying assignor about the new Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])
2025-01-01 00:21:29.550 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Adding newly assigned partitions: pong-message-topic-0, pong-message-topic-1
2025-01-01 00:21:29.556 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:21:29.556 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:21:29.560 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:21:29.560 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:21:29.568 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60232 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:21:29.569 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60232 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:21:29.581 pong-service: partitions assigned: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:21:29.840 Idempotence will be disabled because acks is set to 0, not set to 'all'.
2025-01-01 00:21:29.840 ProducerConfig values: 
	acks = 0
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:60232]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-01-01 00:21:29.853 Kafka version: 3.4.1
2025-01-01 00:21:29.853 Kafka commitId: 8a516edc2755df89
2025-01-01 00:21:29.853 Kafka startTimeMs: 1735662089853
2025-01-01 00:21:29.857 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to bVK4NOSBSQOISuK-59pXDQ
2025-01-01 00:21:29.857 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to bVK4NOSBSQOISuK-59pXDQ
2025-01-01 00:21:29.857 [Producer clientId=producer-1] Cluster ID: TQCi_up5R8uZSu7_zeBHRw
2025-01-01 00:21:30.007 [KafkaServer id=0] shutting down
2025-01-01 00:21:30.007 [KafkaServer id=0] Starting controlled shutdown
2025-01-01 00:21:30.013 [Controller id=0] Shutting down broker 0
2025-01-01 00:21:30.015 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:30.016 [KafkaServer id=0] Controlled shutdown request returned successfully after 6ms
2025-01-01 00:21:30.018 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:21:30.018 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:21:30.018 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:21:30.018 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:21:30.019 [Producer clientId=producer-1] Node -1 disconnected.
2025-01-01 00:21:30.019 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:21:30.019 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cancelled in-flight FETCH request with correlation id 15 due to node 0 being disconnected (elapsed time since creation: 130ms, elapsed time since send: 130ms, request timeout: 30000ms)
2025-01-01 00:21:30.020 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 2147483647 disconnected.
2025-01-01 00:21:30.020 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:21:30.020 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:21:30.020 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Group coordinator localhost:60232 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2025-01-01 00:21:30.020 [Producer clientId=producer-1] Cancelled in-flight METADATA request with correlation id 4 due to node 0 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 30000ms)
2025-01-01 00:21:30.020 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:21:30.021 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:21:30.020 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Error sending fetch request (sessionId=1638170267, epoch=1) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
2025-01-01 00:21:30.021 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:21:30.022 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:21:30.024 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:21:30.024 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:21:30.024 [KafkaApi-0] Shutdown complete.
2025-01-01 00:21:30.025 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:21:30.025 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:21:30.025 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:21:30.026 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:21:30.026 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:21:30.026 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:21:30.026 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:21:30.026 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:21:30.027 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:21:30.027 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:21:30.027 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:21:30.027 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:21:30.027 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:21:30.027 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:21:30.028 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:21:30.028 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:21:30.028 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:21:30.028 [ReplicaManager broker=0] Shutting down
2025-01-01 00:21:30.028 [LogDirFailureHandler]: Shutting down
2025-01-01 00:21:30.028 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:21:30.028 [LogDirFailureHandler]: Stopped
2025-01-01 00:21:30.028 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:21:30.030 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:21:30.030 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:21:30.030 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:21:30.030 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:21:30.030 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:21:30.030 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:21:30.030 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:21:30.031 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:21:30.031 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:21:30.031 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:21:30.031 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:21:30.031 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:21:30.031 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:21:30.031 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:21:30.031 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:21:30.035 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:21:30.035 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:21:30.035 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:21:30.035 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:21:30.036 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:21:30.036 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:21:30.036 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:21:30.036 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:21:30.036 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:21:30.037 Shutting down.
2025-01-01 00:21:30.037 Shutting down the log cleaner.
2025-01-01 00:21:30.037 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:21:30.037 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:21:30.037 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:21:30.057 [ProducerStateManager partition=pong-message-topic-0] Wrote producer snapshot at offset 1 with 0 producer ids in 2 ms.
2025-01-01 00:21:30.066 [ProducerStateManager partition=__consumer_offsets-2] Wrote producer snapshot at offset 4 with 0 producer ids in 2 ms.
2025-01-01 00:21:30.076 Shutdown complete.
2025-01-01 00:21:30.076 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:21:30.076 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:21:30.076 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:21:30.078 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:21:30.079 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:21:30.079 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:21:30.079 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:21:30.079 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:21:30.080 [Controller id=0] Resigned
2025-01-01 00:21:30.080 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:21:30.080 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:21:30.080 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:21:30.080 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:21:30.122 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:21:30.122 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:21:30.122 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60232) could not be established. Broker may not be available.
2025-01-01 00:21:30.122 [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:60232) could not be established. Broker may not be available.
2025-01-01 00:21:30.184 EventThread shut down for session: 0x1001820a5440000
2025-01-01 00:21:30.184 Session: 0x1001820a5440000 closed
2025-01-01 00:21:30.184 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:21:30.184 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:21:30.185 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:21:30.185 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:21:30.186 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:21:30.186 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:21:30.186 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:21:30.186 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:21:30.186 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:21:30.186 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:21:30.186 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:21:30.186 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:21:30.186 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:21:30.186 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:21:30.193 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:21:30.193 Metrics scheduler closed
2025-01-01 00:21:30.193 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:21:30.193 Metrics reporters closed
2025-01-01 00:21:30.194 Broker and topic stats closed
2025-01-01 00:21:30.194 App info kafka.server for 0 unregistered
2025-01-01 00:21:30.194 [KafkaServer id=0] shut down completed
2025-01-01 00:21:30.201 ConnnectionExpirerThread interrupted
2025-01-01 00:21:30.201 selector thread exitted run method
2025-01-01 00:21:30.201 selector thread exitted run method
2025-01-01 00:21:30.201 accept thread exitted run method
2025-01-01 00:21:30.201 shutting down
2025-01-01 00:21:30.201 Shutting down
2025-01-01 00:21:30.201 Draining request throttler queue
2025-01-01 00:21:30.201 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:21:30.202 Shutting down
2025-01-01 00:21:30.202 Shutting down
2025-01-01 00:21:30.202 Shutting down
2025-01-01 00:21:30.202 PrepRequestProcessor exited loop!
2025-01-01 00:21:30.202 SyncRequestProcessor exited!
2025-01-01 00:21:30.202 shutdown of request processor complete
2025-01-01 00:21:30.217 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Revoke previously assigned partitions pong-message-topic-0, pong-message-topic-1
2025-01-01 00:21:30.217 pong-service: partitions revoked: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:21:30.218 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:21:30.218 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:21:30.218 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:21:30.218 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:21:30.218 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:21:30.218 Metrics scheduler closed
2025-01-01 00:21:30.218 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:21:30.218 Metrics reporters closed
2025-01-01 00:21:30.219 App info kafka.consumer for consumer-pong-service-1 unregistered
2025-01-01 00:21:30.219 pong-service: Consumer stopped
2025-01-01 00:21:30.220 [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-01 00:21:30.222 Metrics scheduler closed
2025-01-01 00:21:30.222 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:21:30.222 Metrics reporters closed
2025-01-01 00:21:30.222 App info kafka.producer for producer-1 unregistered
2025-01-01 00:21:30.237 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:21:30.238 HikariPool-1 - Shutdown initiated...
2025-01-01 00:21:30.239 HikariPool-1 - Shutdown completed.
2025-01-01 00:21:40.654 Could not detect default configuration classes for test class [com.demo.controller.PongControllerSpec]: PongControllerSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:21:40.791 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.controller.PongControllerSpec
2025-01-01 00:21:40.943 Could not detect default configuration classes for test class [com.demo.dao.MessageRepositorySpec]: MessageRepositorySpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:21:40.949 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.dao.MessageRepositorySpec
2025-01-01 00:21:40.975 Could not detect default configuration classes for test class [com.demo.kafka.KafkaSpec]: KafkaSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:21:41.008 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.kafka.KafkaSpec
2025-01-01 00:21:41.040 Could not detect default configuration classes for test class [com.demo.controller.PongControllerSpec]: PongControllerSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:21:41.042 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.controller.PongControllerSpec
2025-01-01 00:21:41.045 Could not detect default configuration classes for test class [com.demo.dao.MessageRepositorySpec]: MessageRepositorySpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:21:41.045 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.dao.MessageRepositorySpec
2025-01-01 00:21:41.045 Could not detect default configuration classes for test class [com.demo.kafka.KafkaSpec]: KafkaSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:21:41.045 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.kafka.KafkaSpec
2025-01-01 00:21:41.317 Registered kafka:type=kafka.Log4jController MBean
2025-01-01 00:21:41.386 
2025-01-01 00:21:41.386   ______                  _                                          
2025-01-01 00:21:41.386  |___  /                 | |                                         
2025-01-01 00:21:41.386     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __   
2025-01-01 00:21:41.386    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__|
2025-01-01 00:21:41.386   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |    
2025-01-01 00:21:41.386  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_|
2025-01-01 00:21:41.386                                               | |                     
2025-01-01 00:21:41.386                                               |_|                     
2025-01-01 00:21:41.386 
2025-01-01 00:21:46.528 Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:21:46.528 Server environment:host.name=Joe
2025-01-01 00:21:46.528 Server environment:java.version=17.0.12
2025-01-01 00:21:46.528 Server environment:java.vendor=Oracle Corporation
2025-01-01 00:21:46.528 Server environment:java.home=C:\work\jdk-17
2025-01-01 00:21:46.528 Server environment:java.class.path=C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\work\apache-maven-3.8.8\repo\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\work\apache-maven-3.8.8\repo\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\guava\guava\19.0\guava-19.0.jar;C:\work\apache-maven-3.8.8\repo\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\work\apache-maven-3.8.8\repo\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\work\apache-maven-3.8.8\repo\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\work\apache-maven-3.8.8\repo\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\work\apache-maven-3.8.8\repo\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\work\apache-maven-3.8.8\repo\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\work\apache-maven-3.8.8\repo\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\work\apache-maven-3.8.8\repo\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\work\apache-maven-3.8.8\repo\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\jline\jline\2.14.6\jline-2.14.6.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\junit\junit\4.13.2\junit-4.13.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\work\apache-maven-3.8.8\repo\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\work\apache-maven-3.8.8\repo\org\json\json\20220320\json-20220320.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\work\apache-maven-3.8.8\repo\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\work\apache-maven-3.8.8\repo\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\work\apache-maven-3.8.8\repo\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar;
2025-01-01 00:21:46.528 Server environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:21:46.528 Server environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:21:46.528 Server environment:java.compiler=<NA>
2025-01-01 00:21:46.528 Server environment:os.name=Windows 11
2025-01-01 00:21:46.528 Server environment:os.arch=amd64
2025-01-01 00:21:46.528 Server environment:os.version=10.0
2025-01-01 00:21:46.528 Server environment:user.name=joech
2025-01-01 00:21:46.528 Server environment:user.home=C:\Users\joech
2025-01-01 00:21:46.528 Server environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:21:46.528 Server environment:os.memory.free=96MB
2025-01-01 00:21:46.528 Server environment:os.memory.max=15816MB
2025-01-01 00:21:46.528 Server environment:os.memory.total=136MB
2025-01-01 00:21:46.528 zookeeper.enableEagerACLCheck = false
2025-01-01 00:21:46.528 zookeeper.digest.enabled = true
2025-01-01 00:21:46.528 zookeeper.closeSessionTxn.enabled = true
2025-01-01 00:21:46.528 zookeeper.flushDelay=0
2025-01-01 00:21:46.528 zookeeper.maxWriteQueuePollTime=0
2025-01-01 00:21:46.528 zookeeper.maxBatchSize=1000
2025-01-01 00:21:46.528 zookeeper.intBufferStartingSizeBytes = 1024
2025-01-01 00:21:46.528 zookeeper.snapshot.trust.empty : false
2025-01-01 00:21:46.549 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:21:46.549 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:21:46.549 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:21:46.549 zookeeper.commitLogCount=500
2025-01-01 00:21:46.549 Weighed connection throttling is disabled
2025-01-01 00:21:46.549 minSessionTimeout set to 1600
2025-01-01 00:21:46.549 maxSessionTimeout set to 16000
2025-01-01 00:21:46.556 Response cache size is initialized with value 400.
2025-01-01 00:21:46.556 Response cache size is initialized with value 400.
2025-01-01 00:21:46.556 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:21:46.556 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:21:46.556 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:21:46.556 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:21:46.556 zookeeper.pathStats.delay = 5
2025-01-01 00:21:46.556 zookeeper.pathStats.enabled = false
2025-01-01 00:21:46.556 The max bytes for all large requests are set to 104857600
2025-01-01 00:21:46.556 The large request threshold is set to -1
2025-01-01 00:21:46.556 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-1959135030690634954\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-4827136188816474987\version-2
2025-01-01 00:21:46.598 maxCnxns is not configured, using default value 0.
2025-01-01 00:21:46.598 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:21:46.607 binding to port /127.0.0.1:0
2025-01-01 00:21:46.622 zookeeper.snapshot.compression.method = CHECKED
2025-01-01 00:21:46.622 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-4827136188816474987\version-2\snapshot.0
2025-01-01 00:21:46.626 Snapshot loaded in 15 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:21:46.626 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-4827136188816474987\version-2\snapshot.0
2025-01-01 00:21:46.627 Snapshot taken in 1 ms
2025-01-01 00:21:46.636 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:21:46.636 zookeeper.request_throttler.shutdownTimeout = 10000
2025-01-01 00:21:46.853 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60254
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:21:46.880 Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2025-01-01 00:21:46.950 starting
2025-01-01 00:21:46.950 Connecting to zookeeper on 127.0.0.1:60254
2025-01-01 00:21:46.969 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60254.
2025-01-01 00:21:52.132 Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:21:52.133 Client environment:host.name=Joe
2025-01-01 00:21:52.133 Client environment:java.version=17.0.12
2025-01-01 00:21:52.133 Client environment:java.vendor=Oracle Corporation
2025-01-01 00:21:52.133 Client environment:java.home=C:\work\jdk-17
2025-01-01 00:21:52.133 Client environment:java.class.path=C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\work\apache-maven-3.8.8\repo\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\work\apache-maven-3.8.8\repo\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\guava\guava\19.0\guava-19.0.jar;C:\work\apache-maven-3.8.8\repo\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\work\apache-maven-3.8.8\repo\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\work\apache-maven-3.8.8\repo\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\work\apache-maven-3.8.8\repo\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\work\apache-maven-3.8.8\repo\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\work\apache-maven-3.8.8\repo\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\work\apache-maven-3.8.8\repo\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\work\apache-maven-3.8.8\repo\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\work\apache-maven-3.8.8\repo\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\jline\jline\2.14.6\jline-2.14.6.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\junit\junit\4.13.2\junit-4.13.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\work\apache-maven-3.8.8\repo\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\work\apache-maven-3.8.8\repo\org\json\json\20220320\json-20220320.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\work\apache-maven-3.8.8\repo\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\work\apache-maven-3.8.8\repo\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\work\apache-maven-3.8.8\repo\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar;
2025-01-01 00:21:52.134 Client environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:21:52.135 Client environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:21:52.135 Client environment:java.compiler=<NA>
2025-01-01 00:21:52.135 Client environment:os.name=Windows 11
2025-01-01 00:21:52.136 Client environment:os.arch=amd64
2025-01-01 00:21:52.136 Client environment:os.version=10.0
2025-01-01 00:21:52.136 Client environment:user.name=joech
2025-01-01 00:21:52.136 Client environment:user.home=C:\Users\joech
2025-01-01 00:21:52.136 Client environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:21:52.136 Client environment:os.memory.free=100MB
2025-01-01 00:21:52.136 Client environment:os.memory.max=15816MB
2025-01-01 00:21:52.136 Client environment:os.memory.total=136MB
2025-01-01 00:21:52.141 Initiating client connection, connectString=127.0.0.1:60254 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@5323b690
2025-01-01 00:21:52.144 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:21:52.150 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:21:52.151 Opening socket connection to server /127.0.0.1:60254.
2025-01-01 00:21:52.151 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:21:52.153 Socket connection established, initiating session, client: /127.0.0.1:60255, server: /127.0.0.1:60254
2025-01-01 00:21:52.158 Creating new log file: log.1
2025-01-01 00:21:52.165 ZooKeeper audit is disabled.
2025-01-01 00:21:52.168 Session establishment complete on server /127.0.0.1:60254, session id = 0x10018210cc50000, negotiated timeout = 16000
2025-01-01 00:21:52.170 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:21:52.414 Cluster ID = wmIbM78JTMSe44m7wgZbVA
2025-01-01 00:21:52.414 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932\meta.properties
2025-01-01 00:21:52.446 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60254
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:21:52.477 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:21:52.477 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:21:52.477 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:21:52.477 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:21:52.508 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932)
2025-01-01 00:21:52.508 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932 since no clean shutdown file was found
2025-01-01 00:21:52.508 Loaded 0 logs in 9ms.
2025-01-01 00:21:52.508 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:21:52.524 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:21:52.524 Starting the log cleaner
2025-01-01 00:21:52.540 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:21:52.556 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:21:52.563 Feature ZK node at path: /feature does not exist
2025-01-01 00:21:52.572 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:21:52.919 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:21:52.926 Awaiting socket connections on localhost:60256.
2025-01-01 00:21:52.959 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:21:52.967 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:21:52.993 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:21:52.993 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:21:52.993 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:21:52.993 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:21:53.018 [LogDirFailureHandler]: Starting
2025-01-01 00:21:53.037 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:21:53.056 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662113047,1735662113047,1,0,0,72084124265152512,204,0,25

2025-01-01 00:21:53.056 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60256, czxid (broker epoch): 25
2025-01-01 00:21:53.118 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:21:53.124 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:21:53.132 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:21:53.133 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:21:53.138 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:21:53.144 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:21:53.149 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:21:53.153 Feature ZK node created at path: /feature
2025-01-01 00:21:53.155 [GroupCoordinator 0]: Starting up.
2025-01-01 00:21:53.161 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:21:53.189 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:21:53.193 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:21:53.193 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:21:53.200 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:21:53.200 [Controller id=0] Registering handlers
2025-01-01 00:21:53.205 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:21:53.208 [Controller id=0] Deleting isr change notifications
2025-01-01 00:21:53.210 [Controller id=0] Initializing controller context
2025-01-01 00:21:53.230 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:21:53.239 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:21:53.252 [RequestSendThread controllerId=0] Starting
2025-01-01 00:21:53.255 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:21:53.255 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:21:53.255 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:21:53.256 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:21:53.259 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:21:53.259 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:21:53.260 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:21:53.260 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:21:53.262 [Controller id=0] Sending update metadata request
2025-01-01 00:21:53.265 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:21:53.278 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:21:53.279 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:21:53.283 [/config/changes-event-process-thread]: Starting
2025-01-01 00:21:53.284 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:21:53.285 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:21:53.287 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:21:53.291 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60256 (id: 0 rack: null) for sending state change requests
2025-01-01 00:21:53.292 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:21:53.292 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:21:53.303 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:21:53.303 Kafka version: 3.4.1
2025-01-01 00:21:53.303 Kafka commitId: 8a516edc2755df89
2025-01-01 00:21:53.304 Kafka startTimeMs: 1735662113301
2025-01-01 00:21:53.304 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:21:53.304 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:21:53.304 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:21:53.305 [KafkaServer id=0] started
2025-01-01 00:21:53.305 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:21:53.313 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60256]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:21:53.321 [Controller id=0] Starting the controller scheduler
2025-01-01 00:21:53.364 Kafka version: 3.4.1
2025-01-01 00:21:53.364 Kafka commitId: 8a516edc2755df89
2025-01-01 00:21:53.365 Kafka startTimeMs: 1735662113364
2025-01-01 00:21:53.392 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60256 (id: 0 rack: null)
2025-01-01 00:21:53.443 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60256 (id: 0 rack: null)
2025-01-01 00:21:53.484 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0))
2025-01-01 00:21:53.504 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(R500_WMXQcOQMVdHK2MfHA),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), pong-message-topic-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:21:53.505 [Controller id=0] New partition creation callback for pong-message-topic-0,pong-message-topic-1
2025-01-01 00:21:53.507 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:53.508 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:53.508 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:53.513 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:53.536 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:53.537 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:53.539 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions
2025-01-01 00:21:53.540 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 2 partitions
2025-01-01 00:21:53.541 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:53.547 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:21:53.580 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0, pong-message-topic-1)
2025-01-01 00:21:53.581 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
2025-01-01 00:21:53.665 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:53.682 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932\pong-message-topic-0 with properties {}
2025-01-01 00:21:53.684 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:21:53.684 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:21:53.686 [Broker id=0] Leader pong-message-topic-0 with topic id Some(R500_WMXQcOQMVdHK2MfHA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:53.699 [LogLoader partition=pong-message-topic-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:53.700 Created log for partition pong-message-topic-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932\pong-message-topic-1 with properties {}
2025-01-01 00:21:53.700 [Partition pong-message-topic-1 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-1
2025-01-01 00:21:53.700 [Partition pong-message-topic-1 broker=0] Log loaded for partition pong-message-topic-1 with initial high watermark 0
2025-01-01 00:21:53.700 [Broker id=0] Leader pong-message-topic-1 with topic id Some(R500_WMXQcOQMVdHK2MfHA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:53.710 [Broker id=0] Finished LeaderAndIsr request in 166ms correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:21:53.717 [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:21:53.724 App info kafka.admin.client for adminclient-1 unregistered
2025-01-01 00:21:53.727 Metrics scheduler closed
2025-01-01 00:21:53.728 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:21:53.728 Metrics reporters closed
2025-01-01 00:21:54.140 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/work/apache-maven-3.8.8/repo/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/work/apache-maven-3.8.8/repo/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:21:54.169 zookeeper.snapshot.trust.empty : false
2025-01-01 00:21:54.171 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:21:54.171 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:21:54.171 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:21:54.171 zookeeper.commitLogCount=500
2025-01-01 00:21:54.171 minSessionTimeout set to 1600
2025-01-01 00:21:54.171 maxSessionTimeout set to 16000
2025-01-01 00:21:54.171 Response cache size is initialized with value 400.
2025-01-01 00:21:54.171 Response cache size is initialized with value 400.
2025-01-01 00:21:54.171 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:21:54.171 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:21:54.171 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:21:54.171 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:21:54.171 zookeeper.pathStats.delay = 5
2025-01-01 00:21:54.171 zookeeper.pathStats.enabled = false
2025-01-01 00:21:54.171 The max bytes for all large requests are set to 104857600
2025-01-01 00:21:54.172 The large request threshold is set to -1
2025-01-01 00:21:54.172 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-17455830488118687668\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-16098320537955026388\version-2
2025-01-01 00:21:54.172 maxCnxns is not configured, using default value 0.
2025-01-01 00:21:54.172 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:21:54.173 binding to port /127.0.0.1:0
2025-01-01 00:21:54.173 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-16098320537955026388\version-2\snapshot.0
2025-01-01 00:21:54.173 Snapshot loaded in 1 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:21:54.173 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-16098320537955026388\version-2\snapshot.0
2025-01-01 00:21:54.173 Snapshot taken in 1 ms
2025-01-01 00:21:54.181 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:21:54.183 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.309b16b9-3099-4e74-af62-f47404ee9297847082726029798917
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60260
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:21:54.187 starting
2025-01-01 00:21:54.187 Connecting to zookeeper on 127.0.0.1:60260
2025-01-01 00:21:54.187 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60260.
2025-01-01 00:21:54.187 Initiating client connection, connectString=127.0.0.1:60260 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@35145f52
2025-01-01 00:21:54.190 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:21:54.190 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:21:54.191 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:21:54.191 Opening socket connection to server /127.0.0.1:60260.
2025-01-01 00:21:54.191 Socket connection established, initiating session, client: /127.0.0.1:60261, server: /127.0.0.1:60260
2025-01-01 00:21:54.192 Creating new log file: log.1
2025-01-01 00:21:54.197 Session establishment complete on server /127.0.0.1:60260, session id = 0x10018212a440000, negotiated timeout = 16000
2025-01-01 00:21:54.197 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:21:54.247 Cluster ID = 1BBXGVqtSlOTBD_ezdxW1Q
2025-01-01 00:21:54.248 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.309b16b9-3099-4e74-af62-f47404ee9297847082726029798917\meta.properties
2025-01-01 00:21:54.258 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.309b16b9-3099-4e74-af62-f47404ee9297847082726029798917
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60260
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:21:54.272 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:21:54.272 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:21:54.272 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:21:54.272 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:21:54.278 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.309b16b9-3099-4e74-af62-f47404ee9297847082726029798917)
2025-01-01 00:21:54.278 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.309b16b9-3099-4e74-af62-f47404ee9297847082726029798917 since no clean shutdown file was found
2025-01-01 00:21:54.280 Loaded 0 logs in 1ms.
2025-01-01 00:21:54.280 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:21:54.280 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:21:54.282 Starting the log cleaner
2025-01-01 00:21:54.282 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:21:54.283 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:21:54.285 Feature ZK node at path: /feature does not exist
2025-01-01 00:21:54.287 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:21:54.311 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:21:54.314 Awaiting socket connections on localhost:60262.
2025-01-01 00:21:54.322 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:21:54.325 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:21:54.326 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:21:54.327 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:21:54.327 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:21:54.328 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:21:54.330 [LogDirFailureHandler]: Starting
2025-01-01 00:21:54.333 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:21:54.334 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662114333,1735662114333,1,0,0,72084124760014848,204,0,25

2025-01-01 00:21:54.336 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60262, czxid (broker epoch): 25
2025-01-01 00:21:54.341 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:21:54.342 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:21:54.342 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:21:54.344 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:21:54.344 [GroupCoordinator 0]: Starting up.
2025-01-01 00:21:54.346 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:21:54.346 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:21:54.349 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:21:54.350 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:21:54.351 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:21:54.351 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:21:54.351 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:21:54.352 Feature ZK node created at path: /feature
2025-01-01 00:21:54.352 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:21:54.354 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:21:54.354 [Controller id=0] Registering handlers
2025-01-01 00:21:54.356 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:21:54.357 [Controller id=0] Deleting isr change notifications
2025-01-01 00:21:54.359 [Controller id=0] Initializing controller context
2025-01-01 00:21:54.360 [/config/changes-event-process-thread]: Starting
2025-01-01 00:21:54.361 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:21:54.364 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:21:54.366 Kafka version: 3.4.1
2025-01-01 00:21:54.366 Kafka commitId: 8a516edc2755df89
2025-01-01 00:21:54.366 Kafka startTimeMs: 1735662114366
2025-01-01 00:21:54.369 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:21:54.369 [RequestSendThread controllerId=0] Starting
2025-01-01 00:21:54.369 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:21:54.369 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:21:54.369 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:21:54.371 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:21:54.371 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:21:54.371 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:21:54.371 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:21:54.371 [Controller id=0] Sending update metadata request
2025-01-01 00:21:54.371 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:21:54.372 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:21:54.372 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:21:54.372 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:21:54.372 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:21:54.372 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:21:54.372 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:21:54.373 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60262 (id: 0 rack: null) for sending state change requests
2025-01-01 00:21:54.374 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:21:54.374 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:21:54.374 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:21:54.374 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:21:54.374 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:21:54.376 [Controller id=0] Starting the controller scheduler
2025-01-01 00:21:54.367 Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.server:type=app-info,id=0
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1865)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:960)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:895)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:523)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:563)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:188)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:182)
	at kafka.utils.TestUtils.createServer(TestUtils.scala)
	at org.springframework.kafka.test.EmbeddedKafkaBroker.afterPropertiesSet(EmbeddedKafkaBroker.java:369)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1817)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1766)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:410)
	at org.springframework.kafka.test.context.EmbeddedKafkaContextCustomizer.customizeContext(EmbeddedKafkaContextCustomizer.java:116)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextCustomizerAdapter.initialize(SpringBootContextLoader.java:435)
	at org.springframework.boot.SpringApplication.applyInitializers(SpringApplication.java:610)
	at org.springframework.boot.SpringApplication.prepareContext(SpringApplication.java:390)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:314)
	at org.springframework.boot.test.context.SpringBootContextLoader.lambda$loadContext$3(SpringBootContextLoader.java:137)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:58)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:46)
	at org.springframework.boot.SpringApplication.withHook(SpringApplication.java:1409)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextLoaderHook.run(SpringBootContextLoader.java:545)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:137)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:108)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContextInternal(DefaultCacheAwareContextLoaderDelegate.java:187)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:119)
	at org.springframework.test.context.support.DefaultTestContext.getApplicationContext(DefaultTestContext.java:127)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:141)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:97)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:241)
	at org.spockframework.spring.SpringTestContextManager.prepareTestInstance(SpringTestContextManager.java:56)
	at org.spockframework.spring.SpringInterceptor.interceptInitializerMethod(SpringInterceptor.java:46)
	at org.spockframework.runtime.extension.AbstractMethodInterceptor.intercept(AbstractMethodInterceptor.java:24)
	at org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)
	at org.spockframework.runtime.PlatformSpecRunner.runInitializer(PlatformSpecRunner.java:259)
	at org.spockframework.runtime.PlatformSpecRunner.runInitializer(PlatformSpecRunner.java:254)
	at org.spockframework.runtime.IterationNode.prepare(IterationNode.java:31)
	at org.spockframework.runtime.IterationNode.prepare(IterationNode.java:13)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$prepare$2(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.prepare(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:90)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
	at org.spockframework.runtime.ParameterizedFeatureChildExecutor.execute(ParameterizedFeatureChildExecutor.java:104)
	at org.spockframework.runtime.PlatformParameterizedSpecRunner$1.runIteration(PlatformParameterizedSpecRunner.java:72)
	at org.spockframework.runtime.extension.IDataDriver.lambda$static$0(IDataDriver.java:37)
	at org.spockframework.runtime.PlatformParameterizedSpecRunner.runParameterizedFeature(PlatformParameterizedSpecRunner.java:47)
	at org.spockframework.runtime.ParameterizedFeatureNode.execute(ParameterizedFeatureNode.java:40)
	at org.spockframework.runtime.ParameterizedFeatureNode.execute(ParameterizedFeatureNode.java:16)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)
	at org.spockframework.runtime.FeatureNode.lambda$around$0(FeatureNode.java:41)
	at org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunFeature$4(PlatformSpecRunner.java:199)
	at org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)
	at org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)
	at org.spockframework.runtime.PlatformSpecRunner.runFeature(PlatformSpecRunner.java:192)
	at org.spockframework.runtime.FeatureNode.around(FeatureNode.java:41)
	at org.spockframework.runtime.FeatureNode.around(FeatureNode.java:12)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)
	at org.spockframework.runtime.SpecNode.lambda$around$0(SpecNode.java:63)
	at org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunSpec$0(PlatformSpecRunner.java:61)
	at org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)
	at org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)
	at org.spockframework.runtime.PlatformSpecRunner.runSpec(PlatformSpecRunner.java:55)
	at org.spockframework.runtime.SpecNode.around(SpecNode.java:63)
	at org.spockframework.runtime.SpecNode.around(SpecNode.java:11)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2025-01-01 00:21:54.376 [KafkaServer id=0] started
2025-01-01 00:21:54.379 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60262]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:21:54.383 Kafka version: 3.4.1
2025-01-01 00:21:54.383 Kafka commitId: 8a516edc2755df89
2025-01-01 00:21:54.383 Kafka startTimeMs: 1735662114383
2025-01-01 00:21:54.387 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60262 (id: 0 rack: null)
2025-01-01 00:21:54.402 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-01-01 00:21:54.411 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(fGSBxxBSTSWd9g7ucFaP6g),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:21:54.412 [Controller id=0] New partition creation callback for pong-message-topic-0
2025-01-01 00:21:54.412 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:54.412 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:54.413 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:54.419 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:54.419 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
2025-01-01 00:21:54.419 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions
2025-01-01 00:21:54.421 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:54.421 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 1 partitions
2025-01-01 00:21:54.422 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0)
2025-01-01 00:21:54.422 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
2025-01-01 00:21:54.426 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60262 (id: 0 rack: null)
2025-01-01 00:21:54.430 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.309b16b9-3099-4e74-af62-f47404ee9297847082726029798917] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:54.431 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.309b16b9-3099-4e74-af62-f47404ee9297847082726029798917\pong-message-topic-0 with properties {}
2025-01-01 00:21:54.432 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:21:54.432 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:21:54.432 [Broker id=0] Leader pong-message-topic-0 with topic id Some(fGSBxxBSTSWd9g7ucFaP6g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:54.435 [Broker id=0] Finished LeaderAndIsr request in 14ms correlationId 1 from controller 0 for 1 partitions
2025-01-01 00:21:54.437 [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:21:54.441 App info kafka.admin.client for adminclient-2 unregistered
2025-01-01 00:21:54.442 Metrics scheduler closed
2025-01-01 00:21:54.442 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:21:54.442 Metrics reporters closed
2025-01-01 00:21:54.459 Starting PongControllerSpec using Java 17.0.12 with PID 28120 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:21:54.460 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:21:55.022 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:21:55.027 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:21:55.148 Finished Spring Data repository scanning in 113 ms. Found 1 JPA repository interfaces.
2025-01-01 00:21:55.166 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:21:55.167 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:21:55.184 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:21:55.184 Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2025-01-01 00:21:55.806 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:21:55.882 HHH000412: Hibernate ORM core version 6.2.9.Final
2025-01-01 00:21:55.886 HHH000406: Using bytecode reflection optimizer
2025-01-01 00:21:56.028 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:21:56.204 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:21:56.231 HikariPool-1 - Starting...
2025-01-01 00:21:56.501 HikariPool-1 - Added connection conn0: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:21:56.504 HikariPool-1 - Start completed.
2025-01-01 00:21:56.814 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:21:57.468 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:21:57.564 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:21:58.324 [Controller id=0] Processing automatic preferred replica leader election
2025-01-01 00:21:58.851 Netty started on port 60267
2025-01-01 00:21:58.882 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60256]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:21:58.920 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:21:58.920 Kafka version: 3.4.1
2025-01-01 00:21:58.920 Kafka commitId: 8a516edc2755df89
2025-01-01 00:21:58.920 Kafka startTimeMs: 1735662118920
2025-01-01 00:21:58.921 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:21:58.938 Started PongControllerSpec in 5.121 seconds (process running for 20.058)
2025-01-01 00:21:58.947 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to R500_WMXQcOQMVdHK2MfHA
2025-01-01 00:21:58.947 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to R500_WMXQcOQMVdHK2MfHA
2025-01-01 00:21:58.948 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cluster ID: wmIbM78JTMSe44m7wgZbVA
2025-01-01 00:21:58.950 Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-01-01 00:21:58.955 [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(pdt8DKJYQoyxS76V_0FtgQ),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:21:58.955 [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-01-01 00:21:58.955 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:58.955 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:58.955 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:58.955 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:58.955 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:21:58.955 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:58.957 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:58.964 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:58.964 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:58.964 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:58.964 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:58.964 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:21:58.964 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-01-01 00:21:58.964 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-01-01 00:21:58.965 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:21:58.965 [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:21:58.968 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-01-01 00:21:58.968 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-01-01 00:21:58.977 [LogLoader partition=__consumer_offsets-3, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:58.978 Created log for partition __consumer_offsets-3 in C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932\__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:21:58.979 [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-01-01 00:21:58.979 [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-01-01 00:21:58.979 [Broker id=0] Leader __consumer_offsets-3 with topic id Some(pdt8DKJYQoyxS76V_0FtgQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:58.990 [LogLoader partition=__consumer_offsets-2, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:58.992 Created log for partition __consumer_offsets-2 in C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932\__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:21:58.992 [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-01-01 00:21:58.992 [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-01-01 00:21:58.992 [Broker id=0] Leader __consumer_offsets-2 with topic id Some(pdt8DKJYQoyxS76V_0FtgQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:58.999 [LogLoader partition=__consumer_offsets-4, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:59.000 Created log for partition __consumer_offsets-4 in C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932\__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:21:59.000 [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-01-01 00:21:59.000 [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-01-01 00:21:59.000 [Broker id=0] Leader __consumer_offsets-4 with topic id Some(pdt8DKJYQoyxS76V_0FtgQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:59.008 [LogLoader partition=__consumer_offsets-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:59.008 Created log for partition __consumer_offsets-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932\__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:21:59.008 [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-01-01 00:21:59.008 [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-01-01 00:21:59.009 [Broker id=0] Leader __consumer_offsets-1 with topic id Some(pdt8DKJYQoyxS76V_0FtgQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:59.017 [LogLoader partition=__consumer_offsets-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932] Loading producer state till offset 0 with message format version 2
2025-01-01 00:21:59.018 Created log for partition __consumer_offsets-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.74610d17-86e9-46a2-81c5-9435eb845e1f11776718125467765932\__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:21:59.018 [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-01-01 00:21:59.018 [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-01-01 00:21:59.018 [Broker id=0] Leader __consumer_offsets-0 with topic id Some(pdt8DKJYQoyxS76V_0FtgQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:21:59.021 [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-01-01 00:21:59.022 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-01-01 00:21:59.022 [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-01-01 00:21:59.022 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-01-01 00:21:59.022 [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-01-01 00:21:59.022 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-01-01 00:21:59.023 [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-01-01 00:21:59.023 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-01-01 00:21:59.023 [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-01-01 00:21:59.023 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-01-01 00:21:59.023 [Broker id=0] Finished LeaderAndIsr request in 58ms correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:21:59.024 [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-01-01 00:21:59.027 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 4 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler.
2025-01-01 00:21:59.028 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler.
2025-01-01 00:21:59.028 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler.
2025-01-01 00:21:59.028 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler.
2025-01-01 00:21:59.028 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler.
2025-01-01 00:21:59.050 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Discovered group coordinator localhost:60256 (id: 2147483647 rack: null)
2025-01-01 00:21:59.052 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:21:59.068 [GroupCoordinator 0]: Dynamic member with unknown member id joins group pong-service in Empty state. Created a new member id consumer-pong-service-1-38f46706-5d82-4a0a-93a4-26e7aebc6b95 and request the member to rejoin with this id.
2025-01-01 00:21:59.071 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: need to re-join with the given member-id: consumer-pong-service-1-38f46706-5d82-4a0a-93a4-26e7aebc6b95
2025-01-01 00:21:59.071 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2025-01-01 00:21:59.071 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:21:59.076 [GroupCoordinator 0]: Preparing to rebalance group pong-service in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-pong-service-1-38f46706-5d82-4a0a-93a4-26e7aebc6b95 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)
2025-01-01 00:21:59.080 [GroupCoordinator 0]: Stabilized group pong-service generation 1 (__consumer_offsets-2) with 1 members
2025-01-01 00:21:59.081 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully joined group with generation Generation{generationId=1, memberId='consumer-pong-service-1-38f46706-5d82-4a0a-93a4-26e7aebc6b95', protocol='range'}
2025-01-01 00:21:59.083 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Finished assignment for group at generation 1: {consumer-pong-service-1-38f46706-5d82-4a0a-93a4-26e7aebc6b95=Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])}
2025-01-01 00:21:59.088 [GroupCoordinator 0]: Assignment received from leader consumer-pong-service-1-38f46706-5d82-4a0a-93a4-26e7aebc6b95 for group pong-service for generation 1. The group has 1 members, 0 of which are static.
2025-01-01 00:21:59.127 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully synced group in generation Generation{generationId=1, memberId='consumer-pong-service-1-38f46706-5d82-4a0a-93a4-26e7aebc6b95', protocol='range'}
2025-01-01 00:21:59.127 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Notifying assignor about the new Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])
2025-01-01 00:21:59.129 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Adding newly assigned partitions: pong-message-topic-0, pong-message-topic-1
2025-01-01 00:21:59.137 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:21:59.137 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:21:59.140 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:21:59.140 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:21:59.148 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60256 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:21:59.148 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60256 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:21:59.164 pong-service: partitions assigned: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:21:59.378 [Controller id=0] Processing automatic preferred replica leader election
2025-01-01 00:21:59.781 Received ping message: Hello
2025-01-01 00:21:59.791 Idempotence will be disabled because acks is set to 0, not set to 'all'.
2025-01-01 00:21:59.791 ProducerConfig values: 
	acks = 0
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:60256]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-01-01 00:21:59.810 Kafka version: 3.4.1
2025-01-01 00:21:59.810 Kafka commitId: 8a516edc2755df89
2025-01-01 00:21:59.810 Kafka startTimeMs: 1735662119809
2025-01-01 00:21:59.814 [Producer clientId=producer-1] Cluster ID: wmIbM78JTMSe44m7wgZbVA
2025-01-01 00:21:59.817 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to R500_WMXQcOQMVdHK2MfHA
2025-01-01 00:21:59.817 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to R500_WMXQcOQMVdHK2MfHA
2025-01-01 00:22:01.079 Received ping message: Hello
2025-01-01 00:22:01.084 Throttling request as too many requests in the same second
2025-01-01 00:22:01.084 Received ping message: Hello
2025-01-01 00:22:01.351 [KafkaServer id=0] shutting down
2025-01-01 00:22:01.352 [KafkaServer id=0] Starting controlled shutdown
2025-01-01 00:22:01.359 [Controller id=0] Shutting down broker 0
2025-01-01 00:22:01.361 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:22:01.364 [KafkaServer id=0] Controlled shutdown request returned successfully after 7ms
2025-01-01 00:22:01.365 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:22:01.365 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:22:01.365 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:22:01.366 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:22:01.368 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:22:01.368 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:22:01.368 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cancelled in-flight FETCH request with correlation id 22 due to node 0 being disconnected (elapsed time since creation: 278ms, elapsed time since send: 278ms, request timeout: 30000ms)
2025-01-01 00:22:01.368 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 2147483647 disconnected.
2025-01-01 00:22:01.369 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:22:01.369 [Producer clientId=producer-1] Node -1 disconnected.
2025-01-01 00:22:01.369 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Group coordinator localhost:60256 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2025-01-01 00:22:01.369 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Error sending fetch request (sessionId=397157831, epoch=6) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
2025-01-01 00:22:01.370 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:22:01.371 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:22:01.371 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:22:01.373 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:22:01.375 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:22:01.375 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:22:01.375 [KafkaApi-0] Shutdown complete.
2025-01-01 00:22:01.377 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:22:01.377 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:22:01.377 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:22:01.378 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:22:01.379 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:22:01.379 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:22:01.379 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:22:01.379 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:22:01.379 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:22:01.380 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:22:01.380 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:22:01.380 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:22:01.380 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:22:01.380 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:22:01.380 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:22:01.380 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:22:01.380 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:22:01.381 [ReplicaManager broker=0] Shutting down
2025-01-01 00:22:01.381 [LogDirFailureHandler]: Shutting down
2025-01-01 00:22:01.381 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:22:01.381 [LogDirFailureHandler]: Stopped
2025-01-01 00:22:01.381 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:22:01.382 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:22:01.382 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:22:01.383 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:22:01.383 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:22:01.383 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:22:01.383 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:22:01.383 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:22:01.383 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:22:01.383 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:22:01.383 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:22:01.383 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:22:01.383 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:22:01.384 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:22:01.384 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:22:01.384 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:22:01.388 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:22:01.388 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:22:01.389 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:22:01.389 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:22:01.390 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:22:01.390 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:22:01.390 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:22:01.390 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:22:01.390 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:22:01.391 Shutting down.
2025-01-01 00:22:01.392 Shutting down the log cleaner.
2025-01-01 00:22:01.392 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:22:01.393 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:22:01.393 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:22:01.419 [ProducerStateManager partition=pong-message-topic-0] Wrote producer snapshot at offset 3 with 0 producer ids in 4 ms.
2025-01-01 00:22:01.430 [ProducerStateManager partition=__consumer_offsets-2] Wrote producer snapshot at offset 6 with 0 producer ids in 1 ms.
2025-01-01 00:22:01.440 Shutdown complete.
2025-01-01 00:22:01.440 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:22:01.441 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:22:01.441 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:22:01.442 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:22:01.442 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:22:01.443 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:22:01.443 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:22:01.443 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:22:01.444 [Controller id=0] Resigned
2025-01-01 00:22:01.444 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:22:01.444 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:22:01.444 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:22:01.445 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:22:01.470 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:22:01.470 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:22:01.470 [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:60256) could not be established. Broker may not be available.
2025-01-01 00:22:01.471 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60256) could not be established. Broker may not be available.
2025-01-01 00:22:01.549 EventThread shut down for session: 0x10018210cc50000
2025-01-01 00:22:01.549 Session: 0x10018210cc50000 closed
2025-01-01 00:22:01.550 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:22:01.550 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:22:01.553 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:22:01.553 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:22:01.553 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:22:01.553 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:22:01.553 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:22:01.553 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:22:01.553 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:22:01.553 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:22:01.553 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:22:01.553 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:22:01.553 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:22:01.553 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:22:01.564 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:22:01.564 Metrics scheduler closed
2025-01-01 00:22:01.564 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:22:01.564 Metrics reporters closed
2025-01-01 00:22:01.565 Broker and topic stats closed
2025-01-01 00:22:01.565 App info kafka.server for 0 unregistered
2025-01-01 00:22:01.566 [KafkaServer id=0] shut down completed
2025-01-01 00:22:01.571 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:22:01.572 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60256) could not be established. Broker may not be available.
2025-01-01 00:22:01.575 ConnnectionExpirerThread interrupted
2025-01-01 00:22:01.575 accept thread exitted run method
2025-01-01 00:22:01.575 selector thread exitted run method
2025-01-01 00:22:01.575 selector thread exitted run method
2025-01-01 00:22:01.576 shutting down
2025-01-01 00:22:01.576 Shutting down
2025-01-01 00:22:01.576 Draining request throttler queue
2025-01-01 00:22:01.576 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:22:01.576 Shutting down
2025-01-01 00:22:01.576 Shutting down
2025-01-01 00:22:01.576 Shutting down
2025-01-01 00:22:01.577 PrepRequestProcessor exited loop!
2025-01-01 00:22:01.577 SyncRequestProcessor exited!
2025-01-01 00:22:01.577 shutdown of request processor complete
2025-01-01 00:22:01.595 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Revoke previously assigned partitions pong-message-topic-0, pong-message-topic-1
2025-01-01 00:22:01.596 pong-service: partitions revoked: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:22:01.596 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:22:01.596 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:22:01.596 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:22:01.597 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:22:01.597 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:22:01.597 Metrics scheduler closed
2025-01-01 00:22:01.597 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:22:01.597 Metrics reporters closed
2025-01-01 00:22:01.597 App info kafka.consumer for consumer-pong-service-1 unregistered
2025-01-01 00:22:01.598 pong-service: Consumer stopped
2025-01-01 00:22:01.601 [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-01 00:22:01.603 Metrics scheduler closed
2025-01-01 00:22:01.603 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:22:01.603 Metrics reporters closed
2025-01-01 00:22:01.603 App info kafka.producer for producer-1 unregistered
2025-01-01 00:22:01.616 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:22:01.618 HikariPool-1 - Shutdown initiated...
2025-01-01 00:22:01.620 HikariPool-1 - Shutdown completed.
2025-01-01 00:22:01.890 SessionTrackerImpl exited loop!
2025-01-01 00:22:03.641 [KafkaServer id=0] shutting down
2025-01-01 00:22:03.641 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:22:03.641 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:22:03.641 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:22:03.642 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:22:03.645 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:22:03.645 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:22:03.645 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:22:03.646 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:22:03.647 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:22:03.647 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:22:03.647 [KafkaApi-0] Shutdown complete.
2025-01-01 00:22:03.647 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:22:03.647 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:22:03.647 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:22:03.647 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:22:03.647 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:22:03.647 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:22:03.647 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:22:03.647 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:22:03.648 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:22:03.648 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:22:03.648 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:22:03.648 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:22:03.648 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:22:03.648 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:22:03.648 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:22:03.648 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:22:03.649 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:22:03.649 [ReplicaManager broker=0] Shutting down
2025-01-01 00:22:03.649 [LogDirFailureHandler]: Shutting down
2025-01-01 00:22:03.649 [LogDirFailureHandler]: Stopped
2025-01-01 00:22:03.649 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:22:03.649 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:22:03.649 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:22:03.649 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:22:03.649 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:22:03.649 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:22:03.649 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:22:03.649 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:22:03.649 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:22:03.649 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:22:03.649 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:22:03.649 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:22:03.651 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:22:03.651 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:22:03.651 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:22:03.651 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:22:03.651 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:22:03.655 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:22:03.655 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:22:03.655 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:22:03.655 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:22:03.656 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:22:03.656 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:22:03.656 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:22:03.656 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:22:03.656 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:22:03.656 Shutting down.
2025-01-01 00:22:03.656 Shutting down the log cleaner.
2025-01-01 00:22:03.656 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:22:03.656 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:22:03.656 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:22:03.664 Shutdown complete.
2025-01-01 00:22:03.664 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:22:03.664 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:22:03.664 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:22:03.665 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:22:03.665 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:22:03.665 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:22:03.665 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:22:03.665 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:22:03.666 [Controller id=0] Resigned
2025-01-01 00:22:03.666 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:22:03.666 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:22:03.666 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:22:03.666 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:22:03.769 EventThread shut down for session: 0x10018212a440000
2025-01-01 00:22:03.769 Session: 0x10018212a440000 closed
2025-01-01 00:22:03.769 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:22:03.769 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:22:03.770 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:22:03.770 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:22:03.770 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:22:03.770 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:22:03.770 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:22:03.770 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:22:03.770 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:22:03.770 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:22:03.770 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:22:03.770 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:22:03.770 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:22:03.770 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:22:03.778 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:22:03.778 Metrics scheduler closed
2025-01-01 00:22:03.778 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:22:03.778 Metrics reporters closed
2025-01-01 00:22:03.779 Broker and topic stats closed
2025-01-01 00:22:03.779 App info kafka.server for 0 unregistered
2025-01-01 00:22:03.779 [KafkaServer id=0] shut down completed
2025-01-01 00:22:03.783 ConnnectionExpirerThread interrupted
2025-01-01 00:22:03.783 accept thread exitted run method
2025-01-01 00:22:03.783 selector thread exitted run method
2025-01-01 00:22:03.783 selector thread exitted run method
2025-01-01 00:22:03.785 shutting down
2025-01-01 00:22:03.785 Shutting down
2025-01-01 00:22:03.785 Draining request throttler queue
2025-01-01 00:22:03.785 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:22:03.785 Shutting down
2025-01-01 00:22:03.785 Shutting down
2025-01-01 00:22:03.785 Shutting down
2025-01-01 00:22:03.785 PrepRequestProcessor exited loop!
2025-01-01 00:22:03.785 SyncRequestProcessor exited!
2025-01-01 00:22:03.785 shutdown of request processor complete
2025-01-01 00:22:04.108 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/work/apache-maven-3.8.8/repo/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/work/apache-maven-3.8.8/repo/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:22:04.110 Starting MessageRepositorySpec using Java 17.0.12 with PID 28120 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:22:04.110 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:22:04.212 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:22:04.212 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:22:04.223 Finished Spring Data repository scanning in 11 ms. Found 1 JPA repository interfaces.
2025-01-01 00:22:04.226 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:22:04.226 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:22:04.228 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:22:04.228 Finished Spring Data repository scanning in 3 ms. Found 0 Redis repository interfaces.
2025-01-01 00:22:04.289 SessionTrackerImpl exited loop!
2025-01-01 00:22:04.289 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:22:04.302 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:22:04.302 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:22:04.302 HikariPool-2 - Starting...
2025-01-01 00:22:04.305 HikariPool-2 - Added connection conn10: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:22:04.305 HikariPool-2 - Start completed.
2025-01-01 00:22:04.309 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:22:04.332 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:22:04.346 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:22:04.483 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60256]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:22:04.497 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:22:04.497 Kafka version: 3.4.1
2025-01-01 00:22:04.497 Kafka commitId: 8a516edc2755df89
2025-01-01 00:22:04.497 Kafka startTimeMs: 1735662124497
2025-01-01 00:22:04.497 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:22:04.497 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:22:04.497 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Connection to node -1 (/127.0.0.1:60256) could not be established. Broker may not be available.
2025-01-01 00:22:04.497 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Bootstrap broker 127.0.0.1:60256 (id: -1 rack: null) disconnected
2025-01-01 00:22:04.497 Started MessageRepositorySpec in 0.415 seconds (process running for 25.62)
2025-01-01 00:22:04.529 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:22:04.529 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:22:04.529 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:22:04.529 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:22:04.529 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:22:04.529 Metrics scheduler closed
2025-01-01 00:22:04.529 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:22:04.529 Metrics reporters closed
2025-01-01 00:22:04.529 App info kafka.consumer for consumer-pong-service-2 unregistered
2025-01-01 00:22:04.529 pong-service: Consumer stopped
2025-01-01 00:22:04.533 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:22:04.533 HikariPool-2 - Shutdown initiated...
2025-01-01 00:22:04.545 HikariPool-2 - Shutdown completed.
2025-01-01 00:22:04.560 zookeeper.snapshot.trust.empty : false
2025-01-01 00:22:04.560 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:22:04.560 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:22:04.561 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:22:04.561 zookeeper.commitLogCount=500
2025-01-01 00:22:04.561 minSessionTimeout set to 1600
2025-01-01 00:22:04.561 maxSessionTimeout set to 16000
2025-01-01 00:22:04.561 Response cache size is initialized with value 400.
2025-01-01 00:22:04.561 Response cache size is initialized with value 400.
2025-01-01 00:22:04.561 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:22:04.561 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:22:04.561 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:22:04.561 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:22:04.561 zookeeper.pathStats.delay = 5
2025-01-01 00:22:04.561 zookeeper.pathStats.enabled = false
2025-01-01 00:22:04.561 The max bytes for all large requests are set to 104857600
2025-01-01 00:22:04.561 The large request threshold is set to -1
2025-01-01 00:22:04.561 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-3191735565880794227\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-17687240207606505289\version-2
2025-01-01 00:22:04.561 maxCnxns is not configured, using default value 0.
2025-01-01 00:22:04.561 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:22:04.562 binding to port /127.0.0.1:0
2025-01-01 00:22:04.566 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-17687240207606505289\version-2\snapshot.0
2025-01-01 00:22:04.566 Snapshot loaded in 1 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:22:04.566 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-17687240207606505289\version-2\snapshot.0
2025-01-01 00:22:04.567 Snapshot taken in 1 ms
2025-01-01 00:22:04.567 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:22:04.569 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60285
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:22:04.572 starting
2025-01-01 00:22:04.572 Connecting to zookeeper on 127.0.0.1:60285
2025-01-01 00:22:04.572 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60285.
2025-01-01 00:22:04.572 Initiating client connection, connectString=127.0.0.1:60285 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@138c4486
2025-01-01 00:22:04.573 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:22:04.573 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:22:04.574 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:22:04.574 Opening socket connection to server /127.0.0.1:60285.
2025-01-01 00:22:04.574 Socket connection established, initiating session, client: /127.0.0.1:60286, server: /127.0.0.1:60285
2025-01-01 00:22:04.575 Creating new log file: log.1
2025-01-01 00:22:04.580 Session establishment complete on server /127.0.0.1:60285, session id = 0x100182152d70000, negotiated timeout = 16000
2025-01-01 00:22:04.580 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:22:04.608 Cluster ID = ECo8lpj_Q2Cs-yMLnZLm2w
2025-01-01 00:22:04.608 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157\meta.properties
2025-01-01 00:22:04.614 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60285
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:22:04.619 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:22:04.619 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:22:04.619 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:22:04.619 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:22:04.622 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157)
2025-01-01 00:22:04.624 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157 since no clean shutdown file was found
2025-01-01 00:22:04.625 Loaded 0 logs in 1ms.
2025-01-01 00:22:04.625 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:22:04.625 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:22:04.626 Starting the log cleaner
2025-01-01 00:22:04.626 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:22:04.626 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:22:04.628 Feature ZK node at path: /feature does not exist
2025-01-01 00:22:04.630 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:22:04.642 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:22:04.643 Awaiting socket connections on localhost:60287.
2025-01-01 00:22:04.649 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:22:04.651 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:22:04.651 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:22:04.652 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:22:04.652 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:22:04.652 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:22:04.653 [LogDirFailureHandler]: Starting
2025-01-01 00:22:04.655 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:22:04.657 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662124656,1735662124656,1,0,0,72084125440737280,204,0,25

2025-01-01 00:22:04.657 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60287, czxid (broker epoch): 25
2025-01-01 00:22:04.661 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:22:04.661 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:22:04.662 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:22:04.662 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:22:04.662 [GroupCoordinator 0]: Starting up.
2025-01-01 00:22:04.662 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:22:04.663 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:22:04.665 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:22:04.665 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:22:04.666 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:22:04.666 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:22:04.666 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:22:04.666 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:22:04.667 Feature ZK node created at path: /feature
2025-01-01 00:22:04.667 [Controller id=0] Registering handlers
2025-01-01 00:22:04.667 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:22:04.669 [/config/changes-event-process-thread]: Starting
2025-01-01 00:22:04.670 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:22:04.670 [Controller id=0] Deleting isr change notifications
2025-01-01 00:22:04.670 [Controller id=0] Initializing controller context
2025-01-01 00:22:04.672 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:22:04.672 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:22:04.672 Kafka version: 3.4.1
2025-01-01 00:22:04.672 Kafka commitId: 8a516edc2755df89
2025-01-01 00:22:04.672 Kafka startTimeMs: 1735662124672
2025-01-01 00:22:04.672 [KafkaServer id=0] started
2025-01-01 00:22:04.672 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60287]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:22:04.675 Kafka version: 3.4.1
2025-01-01 00:22:04.675 Kafka commitId: 8a516edc2755df89
2025-01-01 00:22:04.675 Kafka startTimeMs: 1735662124675
2025-01-01 00:22:04.676 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:22:04.676 [RequestSendThread controllerId=0] Starting
2025-01-01 00:22:04.676 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:22:04.676 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:22:04.676 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:22:04.677 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:22:04.677 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:22:04.677 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:22:04.677 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:22:04.677 [Controller id=0] Sending update metadata request
2025-01-01 00:22:04.677 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:22:04.677 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:22:04.677 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:22:04.677 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:22:04.677 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:22:04.677 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:22:04.677 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:22:04.677 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60287 (id: 0 rack: null) for sending state change requests
2025-01-01 00:22:04.679 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:22:04.679 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:22:04.679 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:22:04.679 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:22:04.679 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:22:04.680 [Controller id=0] Starting the controller scheduler
2025-01-01 00:22:04.731 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60287 (id: 0 rack: null)
2025-01-01 00:22:04.752 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60287 (id: 0 rack: null)
2025-01-01 00:22:04.782 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0))
2025-01-01 00:22:04.788 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(Z4mmfIbDRlS4muAafsvHtg),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), pong-message-topic-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:22:04.788 [Controller id=0] New partition creation callback for pong-message-topic-0,pong-message-topic-1
2025-01-01 00:22:04.789 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:22:04.789 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:22:04.789 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:22:04.789 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:22:04.796 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:22:04.796 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:22:04.796 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions
2025-01-01 00:22:04.796 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 2 partitions
2025-01-01 00:22:04.796 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:22:04.797 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:22:04.797 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0, pong-message-topic-1)
2025-01-01 00:22:04.797 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
2025-01-01 00:22:04.803 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157] Loading producer state till offset 0 with message format version 2
2025-01-01 00:22:04.803 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157\pong-message-topic-0 with properties {}
2025-01-01 00:22:04.804 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:22:04.804 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:22:04.804 [Broker id=0] Leader pong-message-topic-0 with topic id Some(Z4mmfIbDRlS4muAafsvHtg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:22:04.811 [LogLoader partition=pong-message-topic-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157] Loading producer state till offset 0 with message format version 2
2025-01-01 00:22:04.812 Created log for partition pong-message-topic-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157\pong-message-topic-1 with properties {}
2025-01-01 00:22:04.812 [Partition pong-message-topic-1 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-1
2025-01-01 00:22:04.812 [Partition pong-message-topic-1 broker=0] Log loaded for partition pong-message-topic-1 with initial high watermark 0
2025-01-01 00:22:04.812 [Broker id=0] Leader pong-message-topic-1 with topic id Some(Z4mmfIbDRlS4muAafsvHtg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:22:04.816 [Broker id=0] Finished LeaderAndIsr request in 19ms correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:22:04.818 [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:22:04.819 App info kafka.admin.client for adminclient-3 unregistered
2025-01-01 00:22:04.820 Metrics scheduler closed
2025-01-01 00:22:04.820 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:22:04.820 Metrics reporters closed
2025-01-01 00:22:04.853 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/work/apache-maven-3.8.8/repo/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/work/apache-maven-3.8.8/repo/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:22:04.854 Starting KafkaSpec using Java 17.0.12 with PID 28120 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:22:04.855 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:22:04.947 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:22:04.947 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:22:04.958 Finished Spring Data repository scanning in 11 ms. Found 1 JPA repository interfaces.
2025-01-01 00:22:04.961 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:22:04.962 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:22:04.965 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:22:04.965 Finished Spring Data repository scanning in 3 ms. Found 0 Redis repository interfaces.
2025-01-01 00:22:05.018 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:22:05.027 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:22:05.027 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:22:05.029 HikariPool-3 - Starting...
2025-01-01 00:22:05.031 HikariPool-3 - Added connection conn19: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:22:05.031 HikariPool-3 - Start completed.
2025-01-01 00:22:05.038 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:22:05.056 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:22:05.072 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:22:05.197 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60287]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:22:05.200 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:22:05.200 Kafka version: 3.4.1
2025-01-01 00:22:05.200 Kafka commitId: 8a516edc2755df89
2025-01-01 00:22:05.200 Kafka startTimeMs: 1735662125200
2025-01-01 00:22:05.201 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:22:05.204 Started KafkaSpec in 0.381 seconds (process running for 26.324)
2025-01-01 00:22:05.206 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to Z4mmfIbDRlS4muAafsvHtg
2025-01-01 00:22:05.206 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to Z4mmfIbDRlS4muAafsvHtg
2025-01-01 00:22:05.206 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cluster ID: ECo8lpj_Q2Cs-yMLnZLm2w
2025-01-01 00:22:05.208 Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-01-01 00:22:05.215 [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(qroNxHOsRvi-OUvhjJb8cA),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:22:05.215 [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-01-01 00:22:05.215 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:22:05.215 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:22:05.215 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:22:05.215 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:22:05.215 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:22:05.215 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:22:05.216 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:22:05.216 Idempotence will be disabled because acks is set to 0, not set to 'all'.
2025-01-01 00:22:05.216 ProducerConfig values: 
	acks = 0
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:60287]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-01-01 00:22:05.219 Kafka version: 3.4.1
2025-01-01 00:22:05.219 Kafka commitId: 8a516edc2755df89
2025-01-01 00:22:05.220 Kafka startTimeMs: 1735662125219
2025-01-01 00:22:05.222 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:22:05.222 [Producer clientId=producer-2] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to Z4mmfIbDRlS4muAafsvHtg
2025-01-01 00:22:05.222 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:22:05.222 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:22:05.222 [Producer clientId=producer-2] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to Z4mmfIbDRlS4muAafsvHtg
2025-01-01 00:22:05.222 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:22:05.222 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:22:05.222 [Producer clientId=producer-2] Cluster ID: ECo8lpj_Q2Cs-yMLnZLm2w
2025-01-01 00:22:05.223 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-01-01 00:22:05.223 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-01-01 00:22:05.223 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:22:05.223 [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:22:05.225 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-01-01 00:22:05.225 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-01-01 00:22:05.233 [LogLoader partition=__consumer_offsets-3, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157] Loading producer state till offset 0 with message format version 2
2025-01-01 00:22:05.234 Created log for partition __consumer_offsets-3 in C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157\__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:22:05.236 [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-01-01 00:22:05.236 [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-01-01 00:22:05.236 [Broker id=0] Leader __consumer_offsets-3 with topic id Some(qroNxHOsRvi-OUvhjJb8cA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:22:05.245 [LogLoader partition=__consumer_offsets-2, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157] Loading producer state till offset 0 with message format version 2
2025-01-01 00:22:05.245 Created log for partition __consumer_offsets-2 in C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157\__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:22:05.245 [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-01-01 00:22:05.246 [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-01-01 00:22:05.246 [Broker id=0] Leader __consumer_offsets-2 with topic id Some(qroNxHOsRvi-OUvhjJb8cA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:22:05.255 [LogLoader partition=__consumer_offsets-4, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157] Loading producer state till offset 0 with message format version 2
2025-01-01 00:22:05.256 Created log for partition __consumer_offsets-4 in C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157\__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:22:05.256 [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-01-01 00:22:05.256 [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-01-01 00:22:05.256 [Broker id=0] Leader __consumer_offsets-4 with topic id Some(qroNxHOsRvi-OUvhjJb8cA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:22:05.263 [LogLoader partition=__consumer_offsets-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157] Loading producer state till offset 0 with message format version 2
2025-01-01 00:22:05.264 Created log for partition __consumer_offsets-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157\__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:22:05.264 [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-01-01 00:22:05.264 [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-01-01 00:22:05.264 [Broker id=0] Leader __consumer_offsets-1 with topic id Some(qroNxHOsRvi-OUvhjJb8cA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:22:05.273 [LogLoader partition=__consumer_offsets-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157] Loading producer state till offset 0 with message format version 2
2025-01-01 00:22:05.274 Created log for partition __consumer_offsets-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.84e3bb39-bb1d-4f73-8897-8ce50cbc94438496228863615354157\__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:22:05.274 [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-01-01 00:22:05.274 [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-01-01 00:22:05.274 [Broker id=0] Leader __consumer_offsets-0 with topic id Some(qroNxHOsRvi-OUvhjJb8cA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:22:05.277 [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-01-01 00:22:05.277 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-01-01 00:22:05.277 [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-01-01 00:22:05.277 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-01-01 00:22:05.277 [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-01-01 00:22:05.277 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-01-01 00:22:05.277 [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-01-01 00:22:05.277 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-01-01 00:22:05.277 [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-01-01 00:22:05.277 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-01-01 00:22:05.278 [Broker id=0] Finished LeaderAndIsr request in 55ms correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:22:05.278 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:22:05.278 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler.
2025-01-01 00:22:05.278 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler.
2025-01-01 00:22:05.278 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler.
2025-01-01 00:22:05.278 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler.
2025-01-01 00:22:05.279 [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-01-01 00:22:05.312 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Discovered group coordinator localhost:60287 (id: 2147483647 rack: null)
2025-01-01 00:22:05.312 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] (Re-)joining group
2025-01-01 00:22:05.315 [GroupCoordinator 0]: Dynamic member with unknown member id joins group pong-service in Empty state. Created a new member id consumer-pong-service-3-33669fa0-8775-414f-86c4-f902a7d16270 and request the member to rejoin with this id.
2025-01-01 00:22:05.315 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: need to re-join with the given member-id: consumer-pong-service-3-33669fa0-8775-414f-86c4-f902a7d16270
2025-01-01 00:22:05.315 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2025-01-01 00:22:05.315 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] (Re-)joining group
2025-01-01 00:22:05.317 [GroupCoordinator 0]: Preparing to rebalance group pong-service in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-pong-service-3-33669fa0-8775-414f-86c4-f902a7d16270 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)
2025-01-01 00:22:05.317 [GroupCoordinator 0]: Stabilized group pong-service generation 1 (__consumer_offsets-2) with 1 members
2025-01-01 00:22:05.318 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Successfully joined group with generation Generation{generationId=1, memberId='consumer-pong-service-3-33669fa0-8775-414f-86c4-f902a7d16270', protocol='range'}
2025-01-01 00:22:05.318 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Finished assignment for group at generation 1: {consumer-pong-service-3-33669fa0-8775-414f-86c4-f902a7d16270=Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])}
2025-01-01 00:22:05.319 [GroupCoordinator 0]: Assignment received from leader consumer-pong-service-3-33669fa0-8775-414f-86c4-f902a7d16270 for group pong-service for generation 1. The group has 1 members, 0 of which are static.
2025-01-01 00:22:05.320 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Successfully synced group in generation Generation{generationId=1, memberId='consumer-pong-service-3-33669fa0-8775-414f-86c4-f902a7d16270', protocol='range'}
2025-01-01 00:22:05.321 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Notifying assignor about the new Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])
2025-01-01 00:22:05.321 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Adding newly assigned partitions: pong-message-topic-0, pong-message-topic-1
2025-01-01 00:22:05.322 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:22:05.322 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:22:05.323 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:22:05.323 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:22:05.324 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting offset for partition pong-message-topic-0 to position FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60287 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:22:05.324 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting offset for partition pong-message-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60287 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:22:05.327 pong-service: partitions assigned: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:22:09.681 [Controller id=0] Processing automatic preferred replica leader election
2025-01-01 00:22:10.423 [KafkaServer id=0] shutting down
2025-01-01 00:22:10.423 [KafkaServer id=0] Starting controlled shutdown
2025-01-01 00:22:10.427 [Controller id=0] Shutting down broker 0
2025-01-01 00:22:10.428 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:22:10.429 [KafkaServer id=0] Controlled shutdown request returned successfully after 2ms
2025-01-01 00:22:10.429 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:22:10.430 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:22:10.430 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:22:10.430 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:22:10.431 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:22:10.431 [Producer clientId=producer-2] Node 0 disconnected.
2025-01-01 00:22:10.431 [Producer clientId=producer-2] Node -1 disconnected.
2025-01-01 00:22:10.431 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:22:10.431 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:22:10.431 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:22:10.432 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cancelled in-flight FETCH request with correlation id 25 due to node 0 being disconnected (elapsed time since creation: 70ms, elapsed time since send: 70ms, request timeout: 30000ms)
2025-01-01 00:22:10.432 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cancelled in-flight METADATA request with correlation id 26 due to node 0 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 30000ms)
2025-01-01 00:22:10.432 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 2147483647 disconnected.
2025-01-01 00:22:10.432 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:22:10.432 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Error sending fetch request (sessionId=1081976668, epoch=10) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
2025-01-01 00:22:10.432 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Group coordinator localhost:60287 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2025-01-01 00:22:10.432 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:22:10.432 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:22:10.432 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:22:10.433 [KafkaApi-0] Shutdown complete.
2025-01-01 00:22:10.433 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:22:10.433 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:22:10.433 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:22:10.433 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:22:10.433 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:22:10.433 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:22:10.433 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:22:10.433 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:22:10.433 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:22:10.433 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:22:10.433 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:22:10.433 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:22:10.433 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:22:10.433 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:22:10.433 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:22:10.433 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:22:10.434 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:22:10.434 [ReplicaManager broker=0] Shutting down
2025-01-01 00:22:10.434 [LogDirFailureHandler]: Shutting down
2025-01-01 00:22:10.434 [LogDirFailureHandler]: Stopped
2025-01-01 00:22:10.434 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:22:10.434 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:22:10.434 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:22:10.434 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:22:10.434 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:22:10.434 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:22:10.434 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:22:10.434 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:22:10.434 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:22:10.434 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:22:10.434 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:22:10.434 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:22:10.434 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:22:10.434 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:22:10.435 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:22:10.435 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:22:10.435 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:22:10.440 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:22:10.440 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:22:10.440 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:22:10.440 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:22:10.441 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:22:10.441 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:22:10.441 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:22:10.441 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:22:10.441 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:22:10.441 Shutting down.
2025-01-01 00:22:10.441 Shutting down the log cleaner.
2025-01-01 00:22:10.441 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:22:10.441 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:22:10.441 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:22:10.454 [ProducerStateManager partition=pong-message-topic-0] Wrote producer snapshot at offset 1 with 0 producer ids in 2 ms.
2025-01-01 00:22:10.464 [ProducerStateManager partition=__consumer_offsets-2] Wrote producer snapshot at offset 3 with 0 producer ids in 2 ms.
2025-01-01 00:22:10.470 Shutdown complete.
2025-01-01 00:22:10.470 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:22:10.470 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:22:10.470 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:22:10.470 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:22:10.472 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:22:10.472 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:22:10.472 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:22:10.472 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:22:10.472 [Controller id=0] Resigned
2025-01-01 00:22:10.472 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:22:10.472 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:22:10.472 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:22:10.473 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:22:10.531 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:22:10.531 [Producer clientId=producer-2] Node 0 disconnected.
2025-01-01 00:22:10.531 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60287) could not be established. Broker may not be available.
2025-01-01 00:22:10.531 [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:60287) could not be established. Broker may not be available.
2025-01-01 00:22:10.574 EventThread shut down for session: 0x100182152d70000
2025-01-01 00:22:10.574 Session: 0x100182152d70000 closed
2025-01-01 00:22:10.574 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:22:10.574 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:22:10.574 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:22:10.574 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:22:10.574 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:22:10.574 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:22:10.574 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:22:10.576 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:22:10.576 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:22:10.576 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:22:10.576 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:22:10.576 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:22:10.576 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:22:10.576 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:22:10.581 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:22:10.581 Metrics scheduler closed
2025-01-01 00:22:10.581 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:22:10.581 Metrics reporters closed
2025-01-01 00:22:10.582 Broker and topic stats closed
2025-01-01 00:22:10.582 App info kafka.server for 0 unregistered
2025-01-01 00:22:10.582 [KafkaServer id=0] shut down completed
2025-01-01 00:22:10.590 ConnnectionExpirerThread interrupted
2025-01-01 00:22:10.590 selector thread exitted run method
2025-01-01 00:22:10.590 accept thread exitted run method
2025-01-01 00:22:10.590 selector thread exitted run method
2025-01-01 00:22:10.590 shutting down
2025-01-01 00:22:10.590 Shutting down
2025-01-01 00:22:10.590 Draining request throttler queue
2025-01-01 00:22:10.590 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:22:10.592 Shutting down
2025-01-01 00:22:10.592 Shutting down
2025-01-01 00:22:10.592 Shutting down
2025-01-01 00:22:10.592 SyncRequestProcessor exited!
2025-01-01 00:22:10.592 PrepRequestProcessor exited loop!
2025-01-01 00:22:10.592 shutdown of request processor complete
2025-01-01 00:22:10.604 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Revoke previously assigned partitions pong-message-topic-0, pong-message-topic-1
2025-01-01 00:22:10.604 pong-service: partitions revoked: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:22:10.604 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:22:10.604 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:22:10.604 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:22:10.604 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:22:10.604 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:22:10.604 Metrics scheduler closed
2025-01-01 00:22:10.604 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:22:10.605 Metrics reporters closed
2025-01-01 00:22:10.605 App info kafka.consumer for consumer-pong-service-3 unregistered
2025-01-01 00:22:10.605 pong-service: Consumer stopped
2025-01-01 00:22:10.605 [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-01 00:22:10.605 Metrics scheduler closed
2025-01-01 00:22:10.605 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:22:10.605 Metrics reporters closed
2025-01-01 00:22:10.605 App info kafka.producer for producer-2 unregistered
2025-01-01 00:22:10.605 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:22:10.605 HikariPool-3 - Shutdown initiated...
2025-01-01 00:22:10.605 HikariPool-3 - Shutdown completed.
2025-01-01 00:22:10.693 SessionTrackerImpl exited loop!
2025-01-01 00:23:00.931 Could not detect default configuration classes for test class [com.demo.kafka.KafkaSpec]: KafkaSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:23:00.990 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.kafka.KafkaSpec
2025-01-01 00:23:01.059 Could not detect default configuration classes for test class [com.demo.dao.MessageRepositorySpec]: MessageRepositorySpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:23:01.065 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.dao.MessageRepositorySpec
2025-01-01 00:23:01.071 Could not detect default configuration classes for test class [com.demo.controller.PongControllerSpec]: PongControllerSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:23:01.083 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.controller.PongControllerSpec
2025-01-01 00:23:01.296 Registered kafka:type=kafka.Log4jController MBean
2025-01-01 00:23:01.347 
2025-01-01 00:23:01.347   ______                  _                                          
2025-01-01 00:23:01.347  |___  /                 | |                                         
2025-01-01 00:23:01.347     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __   
2025-01-01 00:23:01.347    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__|
2025-01-01 00:23:01.347   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |    
2025-01-01 00:23:01.347  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_|
2025-01-01 00:23:01.347                                               | |                     
2025-01-01 00:23:01.347                                               |_|                     
2025-01-01 00:23:01.347 
2025-01-01 00:23:07.097 Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:23:07.097 Server environment:host.name=Joe
2025-01-01 00:23:07.097 Server environment:java.version=17.0.12
2025-01-01 00:23:07.097 Server environment:java.vendor=Oracle Corporation
2025-01-01 00:23:07.097 Server environment:java.home=C:\work\jdk-17
2025-01-01 00:23:07.097 Server environment:java.class.path=C:\work\ideaIC-2024.2.0.2.win\lib\idea_rt.jar;C:\work\ideaIC-2024.2.0.2.win\plugins\junit\lib\junit5-rt.jar;C:\work\ideaIC-2024.2.0.2.win\plugins\junit\lib\junit-rt.jar;C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\Users\joech\.m2\repository\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\Users\joech\.m2\repository\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\Users\joech\.m2\repository\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\Users\joech\.m2\repository\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\Users\joech\.m2\repository\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\Users\joech\.m2\repository\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\Users\joech\.m2\repository\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\Users\joech\.m2\repository\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\Users\joech\.m2\repository\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\Users\joech\.m2\repository\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\Users\joech\.m2\repository\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\Users\joech\.m2\repository\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\Users\joech\.m2\repository\com\google\guava\guava\19.0\guava-19.0.jar;C:\Users\joech\.m2\repository\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\Users\joech\.m2\repository\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\Users\joech\.m2\repository\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\Users\joech\.m2\repository\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\Users\joech\.m2\repository\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\Users\joech\.m2\repository\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\Users\joech\.m2\repository\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\Users\joech\.m2\repository\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\Users\joech\.m2\repository\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\Users\joech\.m2\repository\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\Users\joech\.m2\repository\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\Users\joech\.m2\repository\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\Users\joech\.m2\repository\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\Users\joech\.m2\repository\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\Users\joech\.m2\repository\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\Users\joech\.m2\repository\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\Users\joech\.m2\repository\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\Users\joech\.m2\repository\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\Users\joech\.m2\repository\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\Users\joech\.m2\repository\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\Users\joech\.m2\repository\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\Users\joech\.m2\repository\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\Users\joech\.m2\repository\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\Users\joech\.m2\repository\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\Users\joech\.m2\repository\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\Users\joech\.m2\repository\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\Users\joech\.m2\repository\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\Users\joech\.m2\repository\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\Users\joech\.m2\repository\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\Users\joech\.m2\repository\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\Users\joech\.m2\repository\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\Users\joech\.m2\repository\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\Users\joech\.m2\repository\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\Users\joech\.m2\repository\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\Users\joech\.m2\repository\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\Users\joech\.m2\repository\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\Users\joech\.m2\repository\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\Users\joech\.m2\repository\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\Users\joech\.m2\repository\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\Users\joech\.m2\repository\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\Users\joech\.m2\repository\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\Users\joech\.m2\repository\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\Users\joech\.m2\repository\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\Users\joech\.m2\repository\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\Users\joech\.m2\repository\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\Users\joech\.m2\repository\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\Users\joech\.m2\repository\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\Users\joech\.m2\repository\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\Users\joech\.m2\repository\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\Users\joech\.m2\repository\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\Users\joech\.m2\repository\jline\jline\2.14.6\jline-2.14.6.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\Users\joech\.m2\repository\junit\junit\4.13.2\junit-4.13.2.jar;C:\Users\joech\.m2\repository\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\Users\joech\.m2\repository\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\Users\joech\.m2\repository\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\Users\joech\.m2\repository\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\Users\joech\.m2\repository\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\Users\joech\.m2\repository\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\Users\joech\.m2\repository\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\Users\joech\.m2\repository\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\Users\joech\.m2\repository\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\Users\joech\.m2\repository\org\json\json\20220320\json-20220320.jar;C:\Users\joech\.m2\repository\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\Users\joech\.m2\repository\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\Users\joech\.m2\repository\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\Users\joech\.m2\repository\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\Users\joech\.m2\repository\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\Users\joech\.m2\repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\Users\joech\.m2\repository\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\Users\joech\.m2\repository\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\Users\joech\.m2\repository\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\Users\joech\.m2\repository\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\Users\joech\.m2\repository\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\Users\joech\.m2\repository\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\Users\joech\.m2\repository\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\Users\joech\.m2\repository\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\Users\joech\.m2\repository\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\Users\joech\.m2\repository\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\Users\joech\.m2\repository\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\Users\joech\.m2\repository\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\Users\joech\.m2\repository\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar
2025-01-01 00:23:07.098 Server environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:23:07.098 Server environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:23:07.098 Server environment:java.compiler=<NA>
2025-01-01 00:23:07.098 Server environment:os.name=Windows 11
2025-01-01 00:23:07.098 Server environment:os.arch=amd64
2025-01-01 00:23:07.098 Server environment:os.version=10.0
2025-01-01 00:23:07.098 Server environment:user.name=joech
2025-01-01 00:23:07.098 Server environment:user.home=C:\Users\joech
2025-01-01 00:23:07.098 Server environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:23:07.098 Server environment:os.memory.free=78MB
2025-01-01 00:23:07.098 Server environment:os.memory.max=15816MB
2025-01-01 00:23:07.098 Server environment:os.memory.total=136MB
2025-01-01 00:23:07.098 zookeeper.enableEagerACLCheck = false
2025-01-01 00:23:07.100 zookeeper.digest.enabled = true
2025-01-01 00:23:07.100 zookeeper.closeSessionTxn.enabled = true
2025-01-01 00:23:07.100 zookeeper.flushDelay=0
2025-01-01 00:23:07.100 zookeeper.maxWriteQueuePollTime=0
2025-01-01 00:23:07.100 zookeeper.maxBatchSize=1000
2025-01-01 00:23:07.100 zookeeper.intBufferStartingSizeBytes = 1024
2025-01-01 00:23:07.101 zookeeper.snapshot.trust.empty : false
2025-01-01 00:23:07.111 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:23:07.111 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:23:07.112 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:23:07.112 zookeeper.commitLogCount=500
2025-01-01 00:23:07.114 Weighed connection throttling is disabled
2025-01-01 00:23:07.115 minSessionTimeout set to 1600
2025-01-01 00:23:07.115 maxSessionTimeout set to 16000
2025-01-01 00:23:07.116 Response cache size is initialized with value 400.
2025-01-01 00:23:07.116 Response cache size is initialized with value 400.
2025-01-01 00:23:07.117 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:23:07.117 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:23:07.117 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:23:07.117 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:23:07.117 zookeeper.pathStats.delay = 5
2025-01-01 00:23:07.117 zookeeper.pathStats.enabled = false
2025-01-01 00:23:07.118 The max bytes for all large requests are set to 104857600
2025-01-01 00:23:07.118 The large request threshold is set to -1
2025-01-01 00:23:07.118 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-547876185135117010\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-14244058682461880203\version-2
2025-01-01 00:23:07.139 maxCnxns is not configured, using default value 0.
2025-01-01 00:23:07.140 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:23:07.143 binding to port /127.0.0.1:0
2025-01-01 00:23:07.154 zookeeper.snapshot.compression.method = CHECKED
2025-01-01 00:23:07.154 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-14244058682461880203\version-2\snapshot.0
2025-01-01 00:23:07.157 Snapshot loaded in 11 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:23:07.157 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-14244058682461880203\version-2\snapshot.0
2025-01-01 00:23:07.157 Snapshot taken in 0 ms
2025-01-01 00:23:07.163 zookeeper.request_throttler.shutdownTimeout = 10000
2025-01-01 00:23:07.163 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:23:07.283 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60313
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:23:07.295 Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2025-01-01 00:23:07.341 starting
2025-01-01 00:23:07.341 Connecting to zookeeper on 127.0.0.1:60313
2025-01-01 00:23:07.349 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60313.
2025-01-01 00:23:14.135 Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:23:14.136 Client environment:host.name=Joe
2025-01-01 00:23:14.136 Client environment:java.version=17.0.12
2025-01-01 00:23:14.136 Client environment:java.vendor=Oracle Corporation
2025-01-01 00:23:14.136 Client environment:java.home=C:\work\jdk-17
2025-01-01 00:23:14.136 Client environment:java.class.path=C:\work\ideaIC-2024.2.0.2.win\lib\idea_rt.jar;C:\work\ideaIC-2024.2.0.2.win\plugins\junit\lib\junit5-rt.jar;C:\work\ideaIC-2024.2.0.2.win\plugins\junit\lib\junit-rt.jar;C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\Users\joech\.m2\repository\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\Users\joech\.m2\repository\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\Users\joech\.m2\repository\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\Users\joech\.m2\repository\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\Users\joech\.m2\repository\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\Users\joech\.m2\repository\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\Users\joech\.m2\repository\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\Users\joech\.m2\repository\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\Users\joech\.m2\repository\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\Users\joech\.m2\repository\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\Users\joech\.m2\repository\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\Users\joech\.m2\repository\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\Users\joech\.m2\repository\com\google\guava\guava\19.0\guava-19.0.jar;C:\Users\joech\.m2\repository\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\Users\joech\.m2\repository\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\Users\joech\.m2\repository\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\Users\joech\.m2\repository\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\Users\joech\.m2\repository\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\Users\joech\.m2\repository\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\Users\joech\.m2\repository\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\Users\joech\.m2\repository\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\Users\joech\.m2\repository\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\Users\joech\.m2\repository\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\Users\joech\.m2\repository\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\Users\joech\.m2\repository\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\Users\joech\.m2\repository\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\Users\joech\.m2\repository\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\Users\joech\.m2\repository\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\Users\joech\.m2\repository\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\Users\joech\.m2\repository\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\Users\joech\.m2\repository\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\Users\joech\.m2\repository\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\Users\joech\.m2\repository\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\Users\joech\.m2\repository\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\Users\joech\.m2\repository\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\Users\joech\.m2\repository\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\Users\joech\.m2\repository\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\Users\joech\.m2\repository\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\Users\joech\.m2\repository\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\Users\joech\.m2\repository\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\Users\joech\.m2\repository\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\Users\joech\.m2\repository\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\Users\joech\.m2\repository\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\Users\joech\.m2\repository\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\Users\joech\.m2\repository\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\Users\joech\.m2\repository\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\Users\joech\.m2\repository\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\Users\joech\.m2\repository\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\Users\joech\.m2\repository\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\Users\joech\.m2\repository\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\Users\joech\.m2\repository\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\Users\joech\.m2\repository\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\Users\joech\.m2\repository\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\Users\joech\.m2\repository\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\Users\joech\.m2\repository\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\Users\joech\.m2\repository\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\Users\joech\.m2\repository\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\Users\joech\.m2\repository\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\Users\joech\.m2\repository\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\Users\joech\.m2\repository\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\Users\joech\.m2\repository\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\Users\joech\.m2\repository\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\Users\joech\.m2\repository\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\Users\joech\.m2\repository\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\Users\joech\.m2\repository\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\Users\joech\.m2\repository\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\Users\joech\.m2\repository\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\Users\joech\.m2\repository\jline\jline\2.14.6\jline-2.14.6.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\Users\joech\.m2\repository\junit\junit\4.13.2\junit-4.13.2.jar;C:\Users\joech\.m2\repository\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\Users\joech\.m2\repository\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\Users\joech\.m2\repository\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\Users\joech\.m2\repository\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\Users\joech\.m2\repository\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\Users\joech\.m2\repository\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\Users\joech\.m2\repository\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\Users\joech\.m2\repository\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\Users\joech\.m2\repository\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\Users\joech\.m2\repository\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\Users\joech\.m2\repository\org\json\json\20220320\json-20220320.jar;C:\Users\joech\.m2\repository\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\Users\joech\.m2\repository\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\Users\joech\.m2\repository\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\Users\joech\.m2\repository\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\Users\joech\.m2\repository\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\Users\joech\.m2\repository\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\Users\joech\.m2\repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\Users\joech\.m2\repository\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\Users\joech\.m2\repository\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\Users\joech\.m2\repository\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\Users\joech\.m2\repository\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\Users\joech\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\Users\joech\.m2\repository\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\Users\joech\.m2\repository\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\Users\joech\.m2\repository\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\Users\joech\.m2\repository\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\Users\joech\.m2\repository\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\Users\joech\.m2\repository\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\Users\joech\.m2\repository\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\Users\joech\.m2\repository\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\Users\joech\.m2\repository\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\Users\joech\.m2\repository\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\Users\joech\.m2\repository\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\Users\joech\.m2\repository\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar
2025-01-01 00:23:14.137 Client environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:23:14.137 Client environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:23:14.137 Client environment:java.compiler=<NA>
2025-01-01 00:23:14.137 Client environment:os.name=Windows 11
2025-01-01 00:23:14.137 Client environment:os.arch=amd64
2025-01-01 00:23:14.137 Client environment:os.version=10.0
2025-01-01 00:23:14.137 Client environment:user.name=joech
2025-01-01 00:23:14.137 Client environment:user.home=C:\Users\joech
2025-01-01 00:23:14.137 Client environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:23:14.137 Client environment:os.memory.free=105MB
2025-01-01 00:23:14.137 Client environment:os.memory.max=15816MB
2025-01-01 00:23:14.137 Client environment:os.memory.total=136MB
2025-01-01 00:23:14.139 Initiating client connection, connectString=127.0.0.1:60313 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@12d40609
2025-01-01 00:23:14.143 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:23:14.147 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:23:14.148 Opening socket connection to server /127.0.0.1:60313.
2025-01-01 00:23:14.148 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:23:14.150 Socket connection established, initiating session, client: /127.0.0.1:60314, server: /127.0.0.1:60313
2025-01-01 00:23:14.155 Creating new log file: log.1
2025-01-01 00:23:14.160 ZooKeeper audit is disabled.
2025-01-01 00:23:14.162 Session establishment complete on server /127.0.0.1:60313, session id = 0x100182247560000, negotiated timeout = 16000
2025-01-01 00:23:14.165 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:23:14.343 Cluster ID = sKrP-CaLTcOum_X2-FZg-g
2025-01-01 00:23:14.345 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603\meta.properties
2025-01-01 00:23:14.375 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60313
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:23:14.399 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:23:14.400 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:23:14.400 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:23:14.402 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:23:14.426 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603)
2025-01-01 00:23:14.428 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603 since no clean shutdown file was found
2025-01-01 00:23:14.435 Loaded 0 logs in 9ms.
2025-01-01 00:23:14.435 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:23:14.437 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:23:14.446 Starting the log cleaner
2025-01-01 00:23:14.452 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:23:14.462 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:23:14.468 Feature ZK node at path: /feature does not exist
2025-01-01 00:23:14.486 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:23:14.695 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:23:14.700 Awaiting socket connections on localhost:60315.
2025-01-01 00:23:14.716 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:23:14.720 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:23:14.730 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:23:14.730 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:23:14.731 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:23:14.731 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:23:14.741 [LogDirFailureHandler]: Starting
2025-01-01 00:23:14.754 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:23:14.766 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662194760,1735662194760,1,0,0,72084129542701056,204,0,25

2025-01-01 00:23:14.766 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60315, czxid (broker epoch): 25
2025-01-01 00:23:14.800 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:23:14.801 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:23:14.808 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:23:14.808 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:23:14.813 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:23:14.816 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:23:14.818 [GroupCoordinator 0]: Starting up.
2025-01-01 00:23:14.818 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:23:14.820 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:23:14.820 Feature ZK node created at path: /feature
2025-01-01 00:23:14.832 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:23:14.834 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:23:14.834 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:23:14.838 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:23:14.838 [Controller id=0] Registering handlers
2025-01-01 00:23:14.840 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:23:14.842 [Controller id=0] Deleting isr change notifications
2025-01-01 00:23:14.844 [Controller id=0] Initializing controller context
2025-01-01 00:23:14.854 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:23:14.855 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:23:14.867 [RequestSendThread controllerId=0] Starting
2025-01-01 00:23:14.868 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:23:14.868 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:23:14.869 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:23:14.869 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:23:14.871 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:23:14.871 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:23:14.871 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:23:14.871 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:23:14.873 [Controller id=0] Sending update metadata request
2025-01-01 00:23:14.874 [/config/changes-event-process-thread]: Starting
2025-01-01 00:23:14.876 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:23:14.879 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:23:14.881 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:23:14.881 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:23:14.882 Kafka version: 3.4.1
2025-01-01 00:23:14.882 Kafka commitId: 8a516edc2755df89
2025-01-01 00:23:14.884 Kafka startTimeMs: 1735662194881
2025-01-01 00:23:14.884 [KafkaServer id=0] started
2025-01-01 00:23:14.886 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:23:14.886 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:23:14.886 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:23:14.888 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60315 (id: 0 rack: null) for sending state change requests
2025-01-01 00:23:14.888 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60315]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:23:14.890 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:23:14.894 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:23:14.895 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:23:14.895 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:23:14.895 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:23:14.895 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:23:14.903 [Controller id=0] Starting the controller scheduler
2025-01-01 00:23:14.908 Kafka version: 3.4.1
2025-01-01 00:23:14.908 Kafka commitId: 8a516edc2755df89
2025-01-01 00:23:14.908 Kafka startTimeMs: 1735662194908
2025-01-01 00:23:14.923 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60315 (id: 0 rack: null)
2025-01-01 00:23:14.969 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0))
2025-01-01 00:23:14.983 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(NslYe9S8Tnm3CqMUOZpxlA),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), pong-message-topic-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:23:14.983 [Controller id=0] New partition creation callback for pong-message-topic-0,pong-message-topic-1
2025-01-01 00:23:14.985 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:14.985 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:14.986 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:14.988 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:14.994 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60315 (id: 0 rack: null)
2025-01-01 00:23:15.002 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:15.002 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:15.003 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions
2025-01-01 00:23:15.004 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 2 partitions
2025-01-01 00:23:15.005 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:15.008 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:23:15.027 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0, pong-message-topic-1)
2025-01-01 00:23:15.028 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
2025-01-01 00:23:15.080 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:15.089 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603\pong-message-topic-0 with properties {}
2025-01-01 00:23:15.090 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:23:15.090 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:23:15.092 [Broker id=0] Leader pong-message-topic-0 with topic id Some(NslYe9S8Tnm3CqMUOZpxlA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:15.101 [LogLoader partition=pong-message-topic-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:15.102 Created log for partition pong-message-topic-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603\pong-message-topic-1 with properties {}
2025-01-01 00:23:15.102 [Partition pong-message-topic-1 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-1
2025-01-01 00:23:15.102 [Partition pong-message-topic-1 broker=0] Log loaded for partition pong-message-topic-1 with initial high watermark 0
2025-01-01 00:23:15.102 [Broker id=0] Leader pong-message-topic-1 with topic id Some(NslYe9S8Tnm3CqMUOZpxlA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:15.109 [Broker id=0] Finished LeaderAndIsr request in 102ms correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:23:15.114 [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:23:15.118 App info kafka.admin.client for adminclient-1 unregistered
2025-01-01 00:23:15.121 Metrics scheduler closed
2025-01-01 00:23:15.121 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:23:15.121 Metrics reporters closed
2025-01-01 00:23:15.279 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/Users/joech/.m2/repository/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/Users/joech/.m2/repository/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:23:15.296 Starting KafkaSpec using Java 17.0.12 with PID 28660 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:23:15.297 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:23:15.700 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:23:15.702 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:23:15.787 Finished Spring Data repository scanning in 79 ms. Found 1 JPA repository interfaces.
2025-01-01 00:23:15.799 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:23:15.800 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:23:15.811 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:23:15.811 Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2025-01-01 00:23:16.142 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:23:16.217 HHH000412: Hibernate ORM core version 6.2.9.Final
2025-01-01 00:23:16.220 HHH000406: Using bytecode reflection optimizer
2025-01-01 00:23:16.349 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:23:16.439 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:23:16.448 HikariPool-1 - Starting...
2025-01-01 00:23:16.547 HikariPool-1 - Added connection conn0: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:23:16.548 HikariPool-1 - Start completed.
2025-01-01 00:23:16.679 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:23:17.009 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:23:17.058 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:23:18.022 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60315]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:23:18.047 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:23:18.047 Kafka version: 3.4.1
2025-01-01 00:23:18.047 Kafka commitId: 8a516edc2755df89
2025-01-01 00:23:18.047 Kafka startTimeMs: 1735662198047
2025-01-01 00:23:18.047 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:23:18.058 Started KafkaSpec in 2.907 seconds (process running for 18.317)
2025-01-01 00:23:18.066 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to NslYe9S8Tnm3CqMUOZpxlA
2025-01-01 00:23:18.066 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to NslYe9S8Tnm3CqMUOZpxlA
2025-01-01 00:23:18.067 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cluster ID: sKrP-CaLTcOum_X2-FZg-g
2025-01-01 00:23:18.068 Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-01-01 00:23:18.073 [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(tN5PpbMRSau06IUddGstEg),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:23:18.073 [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-01-01 00:23:18.073 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:18.073 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:18.073 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:18.073 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:18.073 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:18.073 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:18.073 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:18.081 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:18.081 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:18.081 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:18.081 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:18.081 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:18.081 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-01-01 00:23:18.081 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-01-01 00:23:18.081 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:18.082 [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:23:18.082 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-01-01 00:23:18.082 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-01-01 00:23:18.089 [LogLoader partition=__consumer_offsets-3, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:18.090 Created log for partition __consumer_offsets-3 in C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603\__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:23:18.091 [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-01-01 00:23:18.091 [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-01-01 00:23:18.091 [Broker id=0] Leader __consumer_offsets-3 with topic id Some(tN5PpbMRSau06IUddGstEg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:18.098 [LogLoader partition=__consumer_offsets-2, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:18.098 Created log for partition __consumer_offsets-2 in C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603\__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:23:18.098 [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-01-01 00:23:18.098 [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-01-01 00:23:18.098 [Broker id=0] Leader __consumer_offsets-2 with topic id Some(tN5PpbMRSau06IUddGstEg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:18.106 [LogLoader partition=__consumer_offsets-4, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:18.106 Created log for partition __consumer_offsets-4 in C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603\__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:23:18.106 [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-01-01 00:23:18.106 [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-01-01 00:23:18.106 [Broker id=0] Leader __consumer_offsets-4 with topic id Some(tN5PpbMRSau06IUddGstEg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:18.114 [LogLoader partition=__consumer_offsets-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:18.114 Created log for partition __consumer_offsets-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603\__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:23:18.115 [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-01-01 00:23:18.115 [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-01-01 00:23:18.115 [Broker id=0] Leader __consumer_offsets-1 with topic id Some(tN5PpbMRSau06IUddGstEg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:18.123 [LogLoader partition=__consumer_offsets-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:18.127 Created log for partition __consumer_offsets-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.78cc801b-648d-4bf1-b398-b334aaf1fece11112485223941860603\__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:23:18.127 [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-01-01 00:23:18.127 [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-01-01 00:23:18.127 [Broker id=0] Leader __consumer_offsets-0 with topic id Some(tN5PpbMRSau06IUddGstEg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:18.130 [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-01-01 00:23:18.130 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-01-01 00:23:18.131 [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-01-01 00:23:18.131 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-01-01 00:23:18.131 [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-01-01 00:23:18.131 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-01-01 00:23:18.131 [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-01-01 00:23:18.131 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-01-01 00:23:18.131 [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-01-01 00:23:18.131 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-01-01 00:23:18.131 [Broker id=0] Finished LeaderAndIsr request in 50ms correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:23:18.133 [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-01-01 00:23:18.137 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 6 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
2025-01-01 00:23:18.137 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler.
2025-01-01 00:23:18.137 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler.
2025-01-01 00:23:18.137 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler.
2025-01-01 00:23:18.137 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler.
2025-01-01 00:23:18.168 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Discovered group coordinator localhost:60315 (id: 2147483647 rack: null)
2025-01-01 00:23:18.169 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:23:18.183 [GroupCoordinator 0]: Dynamic member with unknown member id joins group pong-service in Empty state. Created a new member id consumer-pong-service-1-188bb533-2dfe-4197-b60b-85083b4ad1fd and request the member to rejoin with this id.
2025-01-01 00:23:18.185 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: need to re-join with the given member-id: consumer-pong-service-1-188bb533-2dfe-4197-b60b-85083b4ad1fd
2025-01-01 00:23:18.185 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2025-01-01 00:23:18.186 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:23:18.189 [GroupCoordinator 0]: Preparing to rebalance group pong-service in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-pong-service-1-188bb533-2dfe-4197-b60b-85083b4ad1fd with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)
2025-01-01 00:23:18.193 [GroupCoordinator 0]: Stabilized group pong-service generation 1 (__consumer_offsets-2) with 1 members
2025-01-01 00:23:18.194 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully joined group with generation Generation{generationId=1, memberId='consumer-pong-service-1-188bb533-2dfe-4197-b60b-85083b4ad1fd', protocol='range'}
2025-01-01 00:23:18.195 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Finished assignment for group at generation 1: {consumer-pong-service-1-188bb533-2dfe-4197-b60b-85083b4ad1fd=Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])}
2025-01-01 00:23:18.199 [GroupCoordinator 0]: Assignment received from leader consumer-pong-service-1-188bb533-2dfe-4197-b60b-85083b4ad1fd for group pong-service for generation 1. The group has 1 members, 0 of which are static.
2025-01-01 00:23:18.229 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully synced group in generation Generation{generationId=1, memberId='consumer-pong-service-1-188bb533-2dfe-4197-b60b-85083b4ad1fd', protocol='range'}
2025-01-01 00:23:18.229 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Notifying assignor about the new Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])
2025-01-01 00:23:18.231 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Adding newly assigned partitions: pong-message-topic-0, pong-message-topic-1
2025-01-01 00:23:18.238 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:23:18.238 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:23:18.241 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:23:18.241 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:23:18.248 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60315 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:23:18.248 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60315 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:23:18.259 pong-service: partitions assigned: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:23:18.507 Idempotence will be disabled because acks is set to 0, not set to 'all'.
2025-01-01 00:23:18.507 ProducerConfig values: 
	acks = 0
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:60315]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-01-01 00:23:18.517 Kafka version: 3.4.1
2025-01-01 00:23:18.518 Kafka commitId: 8a516edc2755df89
2025-01-01 00:23:18.518 Kafka startTimeMs: 1735662198517
2025-01-01 00:23:18.521 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to NslYe9S8Tnm3CqMUOZpxlA
2025-01-01 00:23:18.521 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to NslYe9S8Tnm3CqMUOZpxlA
2025-01-01 00:23:18.521 [Producer clientId=producer-1] Cluster ID: sKrP-CaLTcOum_X2-FZg-g
2025-01-01 00:23:18.669 [KafkaServer id=0] shutting down
2025-01-01 00:23:18.669 [KafkaServer id=0] Starting controlled shutdown
2025-01-01 00:23:18.674 [Controller id=0] Shutting down broker 0
2025-01-01 00:23:18.680 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:18.683 [KafkaServer id=0] Controlled shutdown request returned successfully after 12ms
2025-01-01 00:23:18.684 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:23:18.684 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:23:18.684 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:23:18.684 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:23:18.686 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 2147483647 disconnected.
2025-01-01 00:23:18.686 [Producer clientId=producer-1] Node -1 disconnected.
2025-01-01 00:23:18.687 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:23:18.687 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:23:18.687 [Producer clientId=producer-1] Cancelled in-flight METADATA request with correlation id 4 due to node 0 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 30000ms)
2025-01-01 00:23:18.687 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cancelled in-flight FETCH request with correlation id 15 due to node 0 being disconnected (elapsed time since creation: 138ms, elapsed time since send: 137ms, request timeout: 30000ms)
2025-01-01 00:23:18.687 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cancelled in-flight METADATA request with correlation id 17 due to node 0 being disconnected (elapsed time since creation: 41ms, elapsed time since send: 41ms, request timeout: 30000ms)
2025-01-01 00:23:18.687 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:23:18.688 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:23:18.688 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:23:18.687 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Error sending fetch request (sessionId=588280299, epoch=1) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
2025-01-01 00:23:18.688 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Group coordinator localhost:60315 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2025-01-01 00:23:18.688 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:23:18.690 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:23:18.692 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:23:18.692 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:23:18.692 [KafkaApi-0] Shutdown complete.
2025-01-01 00:23:18.693 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:23:18.693 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:23:18.693 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:23:18.694 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:23:18.694 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:23:18.694 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:23:18.694 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:23:18.694 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:23:18.694 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:23:18.695 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:23:18.695 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:23:18.695 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:23:18.695 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:23:18.695 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:23:18.695 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:23:18.695 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:23:18.696 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:23:18.696 [ReplicaManager broker=0] Shutting down
2025-01-01 00:23:18.696 [LogDirFailureHandler]: Shutting down
2025-01-01 00:23:18.696 [LogDirFailureHandler]: Stopped
2025-01-01 00:23:18.696 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:23:18.696 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:23:18.697 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:23:18.697 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:23:18.697 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:23:18.697 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:23:18.697 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:23:18.697 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:23:18.697 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:23:18.697 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:23:18.697 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:23:18.698 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:23:18.698 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:23:18.698 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:23:18.698 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:23:18.698 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:23:18.698 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:23:18.703 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:23:18.704 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:23:18.704 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:23:18.704 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:23:18.705 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:23:18.705 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:23:18.705 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:23:18.705 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:23:18.705 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:23:18.706 Shutting down.
2025-01-01 00:23:18.706 Shutting down the log cleaner.
2025-01-01 00:23:18.707 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:23:18.707 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:23:18.707 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:23:18.727 [ProducerStateManager partition=pong-message-topic-0] Wrote producer snapshot at offset 1 with 0 producer ids in 2 ms.
2025-01-01 00:23:18.737 [ProducerStateManager partition=__consumer_offsets-2] Wrote producer snapshot at offset 4 with 0 producer ids in 2 ms.
2025-01-01 00:23:18.747 Shutdown complete.
2025-01-01 00:23:18.747 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:23:18.747 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:23:18.747 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:23:18.748 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:23:18.749 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:23:18.749 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:23:18.749 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:23:18.749 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:23:18.750 [Controller id=0] Resigned
2025-01-01 00:23:18.750 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:23:18.750 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:23:18.750 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:23:18.750 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:23:18.788 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:23:18.788 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:23:18.788 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60315) could not be established. Broker may not be available.
2025-01-01 00:23:18.788 [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:60315) could not be established. Broker may not be available.
2025-01-01 00:23:18.854 Session: 0x100182247560000 closed
2025-01-01 00:23:18.854 EventThread shut down for session: 0x100182247560000
2025-01-01 00:23:18.855 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:23:18.855 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:23:18.857 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:23:18.857 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:23:18.857 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:23:18.857 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:23:18.857 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:23:18.857 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:23:18.858 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:23:18.858 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:23:18.858 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:23:18.858 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:23:18.858 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:23:18.858 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:23:18.864 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:23:18.864 Metrics scheduler closed
2025-01-01 00:23:18.864 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:23:18.864 Metrics reporters closed
2025-01-01 00:23:18.865 Broker and topic stats closed
2025-01-01 00:23:18.865 App info kafka.server for 0 unregistered
2025-01-01 00:23:18.865 [KafkaServer id=0] shut down completed
2025-01-01 00:23:18.873 ConnnectionExpirerThread interrupted
2025-01-01 00:23:18.873 accept thread exitted run method
2025-01-01 00:23:18.873 selector thread exitted run method
2025-01-01 00:23:18.873 selector thread exitted run method
2025-01-01 00:23:18.873 shutting down
2025-01-01 00:23:18.873 Shutting down
2025-01-01 00:23:18.873 Draining request throttler queue
2025-01-01 00:23:18.873 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:23:18.873 Shutting down
2025-01-01 00:23:18.873 Shutting down
2025-01-01 00:23:18.873 Shutting down
2025-01-01 00:23:18.873 PrepRequestProcessor exited loop!
2025-01-01 00:23:18.873 SyncRequestProcessor exited!
2025-01-01 00:23:18.873 shutdown of request processor complete
2025-01-01 00:23:18.887 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Revoke previously assigned partitions pong-message-topic-0, pong-message-topic-1
2025-01-01 00:23:18.888 pong-service: partitions revoked: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:23:18.888 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:23:18.888 [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:60315) could not be established. Broker may not be available.
2025-01-01 00:23:18.888 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:23:18.888 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:23:18.888 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:23:18.888 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:23:18.888 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:23:18.888 Metrics scheduler closed
2025-01-01 00:23:18.888 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:23:18.888 Metrics reporters closed
2025-01-01 00:23:18.890 App info kafka.consumer for consumer-pong-service-1 unregistered
2025-01-01 00:23:18.890 pong-service: Consumer stopped
2025-01-01 00:23:18.890 [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-01 00:23:18.891 Metrics scheduler closed
2025-01-01 00:23:18.891 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:23:18.891 Metrics reporters closed
2025-01-01 00:23:18.891 App info kafka.producer for producer-1 unregistered
2025-01-01 00:23:18.902 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:23:18.903 HikariPool-1 - Shutdown initiated...
2025-01-01 00:23:18.904 HikariPool-1 - Shutdown completed.
2025-01-01 00:23:19.094 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/Users/joech/.m2/repository/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/Users/joech/.m2/repository/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:23:19.096 Starting MessageRepositorySpec using Java 17.0.12 with PID 28660 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:23:19.096 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:23:19.162 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:23:19.162 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:23:19.169 Finished Spring Data repository scanning in 6 ms. Found 1 JPA repository interfaces.
2025-01-01 00:23:19.171 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:23:19.171 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:23:19.173 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:23:19.174 Finished Spring Data repository scanning in 2 ms. Found 0 Redis repository interfaces.
2025-01-01 00:23:19.212 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:23:19.217 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:23:19.217 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:23:19.217 HikariPool-2 - Starting...
2025-01-01 00:23:19.218 HikariPool-2 - Added connection conn10: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:23:19.219 HikariPool-2 - Start completed.
2025-01-01 00:23:19.224 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:23:19.237 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:23:19.247 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:23:19.369 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60315]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:23:19.373 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:23:19.373 Kafka version: 3.4.1
2025-01-01 00:23:19.373 Kafka commitId: 8a516edc2755df89
2025-01-01 00:23:19.374 Kafka startTimeMs: 1735662199373
2025-01-01 00:23:19.374 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:23:19.375 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:23:19.376 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Connection to node -1 (/127.0.0.1:60315) could not be established. Broker may not be available.
2025-01-01 00:23:19.376 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Bootstrap broker 127.0.0.1:60315 (id: -1 rack: null) disconnected
2025-01-01 00:23:19.376 Started MessageRepositorySpec in 0.302 seconds (process running for 19.636)
2025-01-01 00:23:19.404 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:23:19.404 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:23:19.404 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:23:19.404 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:23:19.404 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:23:19.404 Metrics scheduler closed
2025-01-01 00:23:19.404 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:23:19.404 Metrics reporters closed
2025-01-01 00:23:19.405 App info kafka.consumer for consumer-pong-service-2 unregistered
2025-01-01 00:23:19.405 pong-service: Consumer stopped
2025-01-01 00:23:19.407 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:23:19.408 HikariPool-2 - Shutdown initiated...
2025-01-01 00:23:19.412 HikariPool-2 - Shutdown completed.
2025-01-01 00:23:19.423 zookeeper.snapshot.trust.empty : false
2025-01-01 00:23:19.423 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:23:19.423 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:23:19.423 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:23:19.423 zookeeper.commitLogCount=500
2025-01-01 00:23:19.423 minSessionTimeout set to 1600
2025-01-01 00:23:19.423 maxSessionTimeout set to 16000
2025-01-01 00:23:19.423 Response cache size is initialized with value 400.
2025-01-01 00:23:19.423 Response cache size is initialized with value 400.
2025-01-01 00:23:19.423 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:23:19.423 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:23:19.423 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:23:19.423 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:23:19.423 zookeeper.pathStats.delay = 5
2025-01-01 00:23:19.423 zookeeper.pathStats.enabled = false
2025-01-01 00:23:19.423 The max bytes for all large requests are set to 104857600
2025-01-01 00:23:19.423 The large request threshold is set to -1
2025-01-01 00:23:19.423 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-1729644400413079495\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-13891337323027311511\version-2
2025-01-01 00:23:19.423 maxCnxns is not configured, using default value 0.
2025-01-01 00:23:19.424 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:23:19.425 binding to port /127.0.0.1:0
2025-01-01 00:23:19.427 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-13891337323027311511\version-2\snapshot.0
2025-01-01 00:23:19.428 Snapshot loaded in 0 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:23:19.428 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-13891337323027311511\version-2\snapshot.0
2025-01-01 00:23:19.428 Snapshot taken in 1 ms
2025-01-01 00:23:19.428 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:23:19.429 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60330
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:23:19.431 starting
2025-01-01 00:23:19.431 Connecting to zookeeper on 127.0.0.1:60330
2025-01-01 00:23:19.431 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60330.
2025-01-01 00:23:19.431 Initiating client connection, connectString=127.0.0.1:60330 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@627f515c
2025-01-01 00:23:19.432 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:23:19.432 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:23:19.432 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:23:19.432 Opening socket connection to server /127.0.0.1:60330.
2025-01-01 00:23:19.432 Socket connection established, initiating session, client: /127.0.0.1:60331, server: /127.0.0.1:60330
2025-01-01 00:23:19.434 Creating new log file: log.1
2025-01-01 00:23:19.437 Session establishment complete on server /127.0.0.1:60330, session id = 0x100182277440000, negotiated timeout = 16000
2025-01-01 00:23:19.437 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:23:19.458 Cluster ID = MXE1tUq6QN6ue67Js8Ez-Q
2025-01-01 00:23:19.458 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822\meta.properties
2025-01-01 00:23:19.462 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60330
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:23:19.464 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:23:19.464 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:23:19.464 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:23:19.464 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:23:19.468 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822)
2025-01-01 00:23:19.468 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822 since no clean shutdown file was found
2025-01-01 00:23:19.469 Loaded 0 logs in 1ms.
2025-01-01 00:23:19.469 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:23:19.469 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:23:19.470 Starting the log cleaner
2025-01-01 00:23:19.471 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:23:19.471 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:23:19.472 Feature ZK node at path: /feature does not exist
2025-01-01 00:23:19.474 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:23:19.482 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:23:19.483 Awaiting socket connections on localhost:60332.
2025-01-01 00:23:19.489 SessionTrackerImpl exited loop!
2025-01-01 00:23:19.489 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:23:19.490 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:23:19.490 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:23:19.492 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:23:19.492 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:23:19.492 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:23:19.493 [LogDirFailureHandler]: Starting
2025-01-01 00:23:19.494 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:23:19.495 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662199495,1735662199495,1,0,0,72084130346827776,204,0,25

2025-01-01 00:23:19.496 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60332, czxid (broker epoch): 25
2025-01-01 00:23:19.500 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:23:19.500 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:23:19.500 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:23:19.500 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:23:19.501 [GroupCoordinator 0]: Starting up.
2025-01-01 00:23:19.501 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:23:19.502 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:23:19.502 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:23:19.502 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:23:19.504 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:23:19.504 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:23:19.504 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:23:19.504 Feature ZK node created at path: /feature
2025-01-01 00:23:19.505 [Controller id=0] Registering handlers
2025-01-01 00:23:19.505 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:23:19.505 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:23:19.505 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:23:19.507 [Controller id=0] Deleting isr change notifications
2025-01-01 00:23:19.507 [Controller id=0] Initializing controller context
2025-01-01 00:23:19.507 [/config/changes-event-process-thread]: Starting
2025-01-01 00:23:19.508 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:23:19.509 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:23:19.509 Kafka version: 3.4.1
2025-01-01 00:23:19.509 Kafka commitId: 8a516edc2755df89
2025-01-01 00:23:19.509 Kafka startTimeMs: 1735662199509
2025-01-01 00:23:19.509 [KafkaServer id=0] started
2025-01-01 00:23:19.509 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60332]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:23:19.511 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:23:19.511 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:23:19.511 [RequestSendThread controllerId=0] Starting
2025-01-01 00:23:19.511 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:23:19.511 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:23:19.511 Kafka version: 3.4.1
2025-01-01 00:23:19.511 Kafka commitId: 8a516edc2755df89
2025-01-01 00:23:19.511 Kafka startTimeMs: 1735662199511
2025-01-01 00:23:19.511 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:23:19.512 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:23:19.512 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:23:19.512 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:23:19.512 [Controller id=0] Sending update metadata request
2025-01-01 00:23:19.512 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:23:19.512 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:23:19.512 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:23:19.512 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:23:19.512 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:23:19.512 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:23:19.512 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:23:19.512 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60332 (id: 0 rack: null) for sending state change requests
2025-01-01 00:23:19.513 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:23:19.513 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:23:19.513 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:23:19.513 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:23:19.513 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:23:19.515 [Controller id=0] Starting the controller scheduler
2025-01-01 00:23:19.517 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0))
2025-01-01 00:23:19.521 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(yeHjjw77TeWN9z0Y9HwZLQ),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), pong-message-topic-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:23:19.522 [Controller id=0] New partition creation callback for pong-message-topic-0,pong-message-topic-1
2025-01-01 00:23:19.522 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:19.522 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:19.522 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:19.522 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:19.525 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:19.525 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:19.525 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions
2025-01-01 00:23:19.525 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 2 partitions
2025-01-01 00:23:19.525 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:19.525 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:23:19.526 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0, pong-message-topic-1)
2025-01-01 00:23:19.526 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
2025-01-01 00:23:19.532 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:19.532 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822\pong-message-topic-0 with properties {}
2025-01-01 00:23:19.533 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:23:19.533 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:23:19.533 [Broker id=0] Leader pong-message-topic-0 with topic id Some(yeHjjw77TeWN9z0Y9HwZLQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:19.538 [LogLoader partition=pong-message-topic-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:19.539 Created log for partition pong-message-topic-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822\pong-message-topic-1 with properties {}
2025-01-01 00:23:19.539 [Partition pong-message-topic-1 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-1
2025-01-01 00:23:19.539 [Partition pong-message-topic-1 broker=0] Log loaded for partition pong-message-topic-1 with initial high watermark 0
2025-01-01 00:23:19.539 [Broker id=0] Leader pong-message-topic-1 with topic id Some(yeHjjw77TeWN9z0Y9HwZLQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:19.542 [Broker id=0] Finished LeaderAndIsr request in 17ms correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:23:19.543 [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:23:19.543 App info kafka.admin.client for adminclient-2 unregistered
2025-01-01 00:23:19.545 Metrics scheduler closed
2025-01-01 00:23:19.545 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:23:19.545 Metrics reporters closed
2025-01-01 00:23:19.572 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/Users/joech/.m2/repository/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/Users/joech/.m2/repository/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:23:19.575 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60332 (id: 0 rack: null)
2025-01-01 00:23:19.576 zookeeper.snapshot.trust.empty : false
2025-01-01 00:23:19.577 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:23:19.577 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:23:19.577 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:23:19.577 zookeeper.commitLogCount=500
2025-01-01 00:23:19.577 minSessionTimeout set to 1600
2025-01-01 00:23:19.577 maxSessionTimeout set to 16000
2025-01-01 00:23:19.577 Response cache size is initialized with value 400.
2025-01-01 00:23:19.577 Response cache size is initialized with value 400.
2025-01-01 00:23:19.577 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:23:19.577 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:23:19.577 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:23:19.577 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:23:19.577 zookeeper.pathStats.delay = 5
2025-01-01 00:23:19.577 zookeeper.pathStats.enabled = false
2025-01-01 00:23:19.577 The max bytes for all large requests are set to 104857600
2025-01-01 00:23:19.577 The large request threshold is set to -1
2025-01-01 00:23:19.577 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-11484308614285734117\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-12509187194762343566\version-2
2025-01-01 00:23:19.577 maxCnxns is not configured, using default value 0.
2025-01-01 00:23:19.577 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:23:19.579 binding to port /127.0.0.1:0
2025-01-01 00:23:19.580 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-12509187194762343566\version-2\snapshot.0
2025-01-01 00:23:19.580 Snapshot loaded in 1 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:23:19.581 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-12509187194762343566\version-2\snapshot.0
2025-01-01 00:23:19.581 Snapshot taken in 0 ms
2025-01-01 00:23:19.581 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:23:19.583 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.26dc2aea-b897-4d67-9526-a68a21f3d20218194040099261854472
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60336
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:23:19.584 starting
2025-01-01 00:23:19.584 Connecting to zookeeper on 127.0.0.1:60336
2025-01-01 00:23:19.585 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60336.
2025-01-01 00:23:19.585 Initiating client connection, connectString=127.0.0.1:60336 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@60386e94
2025-01-01 00:23:19.586 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:23:19.586 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:23:19.586 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:23:19.586 Opening socket connection to server /127.0.0.1:60336.
2025-01-01 00:23:19.587 Socket connection established, initiating session, client: /127.0.0.1:60337, server: /127.0.0.1:60336
2025-01-01 00:23:19.587 Creating new log file: log.1
2025-01-01 00:23:19.588 Session establishment complete on server /127.0.0.1:60336, session id = 0x100182277dd0000, negotiated timeout = 16000
2025-01-01 00:23:19.588 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:23:19.592 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60332 (id: 0 rack: null)
2025-01-01 00:23:19.613 Cluster ID = dAYkLe4zSd20bIFepPrXKw
2025-01-01 00:23:19.613 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.26dc2aea-b897-4d67-9526-a68a21f3d20218194040099261854472\meta.properties
2025-01-01 00:23:19.618 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.26dc2aea-b897-4d67-9526-a68a21f3d20218194040099261854472
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60336
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:23:19.624 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:23:19.624 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:23:19.624 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:23:19.624 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:23:19.627 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.26dc2aea-b897-4d67-9526-a68a21f3d20218194040099261854472)
2025-01-01 00:23:19.628 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.26dc2aea-b897-4d67-9526-a68a21f3d20218194040099261854472 since no clean shutdown file was found
2025-01-01 00:23:19.628 Loaded 0 logs in 0ms.
2025-01-01 00:23:19.628 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:23:19.628 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:23:19.630 Starting the log cleaner
2025-01-01 00:23:19.630 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:23:19.631 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:23:19.632 Feature ZK node at path: /feature does not exist
2025-01-01 00:23:19.633 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:23:19.638 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:23:19.639 Awaiting socket connections on localhost:60338.
2025-01-01 00:23:19.644 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:23:19.646 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:23:19.646 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:23:19.647 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:23:19.647 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:23:19.647 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:23:19.648 [LogDirFailureHandler]: Starting
2025-01-01 00:23:19.650 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:23:19.651 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662199650,1735662199650,1,0,0,72084130356854784,204,0,25

2025-01-01 00:23:19.651 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60338, czxid (broker epoch): 25
2025-01-01 00:23:19.654 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:23:19.654 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:23:19.654 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:23:19.654 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:23:19.654 [GroupCoordinator 0]: Starting up.
2025-01-01 00:23:19.656 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:23:19.657 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:23:19.657 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:23:19.658 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:23:19.658 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:23:19.658 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:23:19.658 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:23:19.659 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:23:19.659 Feature ZK node created at path: /feature
2025-01-01 00:23:19.659 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:23:19.659 [Controller id=0] Registering handlers
2025-01-01 00:23:19.661 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:23:19.661 [/config/changes-event-process-thread]: Starting
2025-01-01 00:23:19.661 [Controller id=0] Deleting isr change notifications
2025-01-01 00:23:19.661 [Controller id=0] Initializing controller context
2025-01-01 00:23:19.662 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:23:19.662 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:23:19.664 Kafka version: 3.4.1
2025-01-01 00:23:19.664 Kafka commitId: 8a516edc2755df89
2025-01-01 00:23:19.664 Kafka startTimeMs: 1735662199664
2025-01-01 00:23:19.664 Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.server:type=app-info,id=0
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1865)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:960)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:895)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:523)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:563)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:188)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:182)
	at kafka.utils.TestUtils.createServer(TestUtils.scala)
	at org.springframework.kafka.test.EmbeddedKafkaBroker.afterPropertiesSet(EmbeddedKafkaBroker.java:369)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1817)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1766)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:410)
	at org.springframework.kafka.test.context.EmbeddedKafkaContextCustomizer.customizeContext(EmbeddedKafkaContextCustomizer.java:116)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextCustomizerAdapter.initialize(SpringBootContextLoader.java:435)
	at org.springframework.boot.SpringApplication.applyInitializers(SpringApplication.java:610)
	at org.springframework.boot.SpringApplication.prepareContext(SpringApplication.java:390)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:314)
	at org.springframework.boot.test.context.SpringBootContextLoader.lambda$loadContext$3(SpringBootContextLoader.java:137)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:58)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:46)
	at org.springframework.boot.SpringApplication.withHook(SpringApplication.java:1409)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextLoaderHook.run(SpringBootContextLoader.java:545)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:137)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:108)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContextInternal(DefaultCacheAwareContextLoaderDelegate.java:187)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:119)
	at org.springframework.test.context.support.DefaultTestContext.getApplicationContext(DefaultTestContext.java:127)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:141)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:97)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:241)
	at org.spockframework.spring.SpringTestContextManager.prepareTestInstance(SpringTestContextManager.java:56)
	at org.spockframework.spring.SpringInterceptor.interceptInitializerMethod(SpringInterceptor.java:46)
	at org.spockframework.runtime.extension.AbstractMethodInterceptor.intercept(AbstractMethodInterceptor.java:24)
	at org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)
	at org.spockframework.runtime.PlatformSpecRunner.runInitializer(PlatformSpecRunner.java:259)
	at org.spockframework.runtime.PlatformSpecRunner.runInitializer(PlatformSpecRunner.java:254)
	at org.spockframework.runtime.IterationNode.prepare(IterationNode.java:31)
	at org.spockframework.runtime.IterationNode.prepare(IterationNode.java:13)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$prepare$2(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.prepare(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:90)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
	at org.spockframework.runtime.ParameterizedFeatureChildExecutor.execute(ParameterizedFeatureChildExecutor.java:104)
	at org.spockframework.runtime.PlatformParameterizedSpecRunner$1.runIteration(PlatformParameterizedSpecRunner.java:72)
	at org.spockframework.runtime.extension.IDataDriver.lambda$static$0(IDataDriver.java:37)
	at org.spockframework.runtime.PlatformParameterizedSpecRunner.runParameterizedFeature(PlatformParameterizedSpecRunner.java:47)
	at org.spockframework.runtime.ParameterizedFeatureNode.execute(ParameterizedFeatureNode.java:40)
	at org.spockframework.runtime.ParameterizedFeatureNode.execute(ParameterizedFeatureNode.java:16)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)
	at org.spockframework.runtime.FeatureNode.lambda$around$0(FeatureNode.java:41)
	at org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunFeature$4(PlatformSpecRunner.java:199)
	at org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)
	at org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)
	at org.spockframework.runtime.PlatformSpecRunner.runFeature(PlatformSpecRunner.java:192)
	at org.spockframework.runtime.FeatureNode.around(FeatureNode.java:41)
	at org.spockframework.runtime.FeatureNode.around(FeatureNode.java:12)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)
	at org.spockframework.runtime.SpecNode.lambda$around$0(SpecNode.java:63)
	at org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunSpec$0(PlatformSpecRunner.java:61)
	at org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)
	at org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)
	at org.spockframework.runtime.PlatformSpecRunner.runSpec(PlatformSpecRunner.java:55)
	at org.spockframework.runtime.SpecNode.around(SpecNode.java:63)
	at org.spockframework.runtime.SpecNode.around(SpecNode.java:11)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
2025-01-01 00:23:19.665 [KafkaServer id=0] started
2025-01-01 00:23:19.665 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60338]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:23:19.666 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:23:19.666 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:23:19.666 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:23:19.666 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:23:19.666 [RequestSendThread controllerId=0] Starting
2025-01-01 00:23:19.666 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:23:19.666 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:23:19.666 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:23:19.666 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:23:19.666 [Controller id=0] Sending update metadata request
2025-01-01 00:23:19.666 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:23:19.666 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:23:19.666 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:23:19.668 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:23:19.668 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:23:19.668 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:23:19.668 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:23:19.668 Kafka version: 3.4.1
2025-01-01 00:23:19.668 Kafka commitId: 8a516edc2755df89
2025-01-01 00:23:19.668 Kafka startTimeMs: 1735662199668
2025-01-01 00:23:19.668 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60338 (id: 0 rack: null) for sending state change requests
2025-01-01 00:23:19.668 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:23:19.669 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:23:19.669 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:23:19.669 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:23:19.669 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:23:19.669 [Controller id=0] Starting the controller scheduler
2025-01-01 00:23:19.674 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-01-01 00:23:19.678 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(LCF0Buy1QLyYpMhKoUSdxw),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:23:19.678 [Controller id=0] New partition creation callback for pong-message-topic-0
2025-01-01 00:23:19.678 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:19.678 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:19.679 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:19.682 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:19.682 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
2025-01-01 00:23:19.682 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions
2025-01-01 00:23:19.683 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:19.683 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 1 partitions
2025-01-01 00:23:19.683 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0)
2025-01-01 00:23:19.683 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
2025-01-01 00:23:19.688 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.26dc2aea-b897-4d67-9526-a68a21f3d20218194040099261854472] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:19.689 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.26dc2aea-b897-4d67-9526-a68a21f3d20218194040099261854472\pong-message-topic-0 with properties {}
2025-01-01 00:23:19.689 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:23:19.689 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:23:19.689 [Broker id=0] Leader pong-message-topic-0 with topic id Some(LCF0Buy1QLyYpMhKoUSdxw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:19.691 [Broker id=0] Finished LeaderAndIsr request in 8ms correlationId 1 from controller 0 for 1 partitions
2025-01-01 00:23:19.693 [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:23:19.693 App info kafka.admin.client for adminclient-3 unregistered
2025-01-01 00:23:19.694 Metrics scheduler closed
2025-01-01 00:23:19.694 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:23:19.694 Metrics reporters closed
2025-01-01 00:23:19.696 Starting PongControllerSpec using Java 17.0.12 with PID 28660 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:23:19.696 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:23:19.733 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60338 (id: 0 rack: null)
2025-01-01 00:23:19.746 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60338 (id: 0 rack: null)
2025-01-01 00:23:19.765 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:23:19.765 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:23:19.772 Finished Spring Data repository scanning in 7 ms. Found 1 JPA repository interfaces.
2025-01-01 00:23:19.775 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:23:19.775 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:23:19.781 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:23:19.781 Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2025-01-01 00:23:19.903 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:23:19.908 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:23:19.908 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:23:19.909 HikariPool-3 - Starting...
2025-01-01 00:23:19.910 HikariPool-3 - Added connection conn19: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:23:19.911 HikariPool-3 - Start completed.
2025-01-01 00:23:19.917 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:23:19.930 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:23:19.939 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:23:20.142 Netty started on port 60342
2025-01-01 00:23:20.143 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60332]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:23:20.146 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:23:20.146 Kafka version: 3.4.1
2025-01-01 00:23:20.146 Kafka commitId: 8a516edc2755df89
2025-01-01 00:23:20.146 Kafka startTimeMs: 1735662200146
2025-01-01 00:23:20.146 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:23:20.149 Started PongControllerSpec in 0.596 seconds (process running for 20.408)
2025-01-01 00:23:20.150 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to yeHjjw77TeWN9z0Y9HwZLQ
2025-01-01 00:23:20.150 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to yeHjjw77TeWN9z0Y9HwZLQ
2025-01-01 00:23:20.150 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cluster ID: MXE1tUq6QN6ue67Js8Ez-Q
2025-01-01 00:23:20.151 Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-01-01 00:23:20.155 [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(7OWm5_wEQgu08JdHLM4wFw),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:23:20.157 [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-01-01 00:23:20.157 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:20.157 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:20.157 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:20.157 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:20.157 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:23:20.157 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:20.157 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:20.161 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:20.162 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:20.162 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:20.162 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:20.162 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:23:20.162 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-01-01 00:23:20.162 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-01-01 00:23:20.162 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:20.162 [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:23:20.164 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-01-01 00:23:20.164 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-01-01 00:23:20.171 [LogLoader partition=__consumer_offsets-3, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:20.172 Created log for partition __consumer_offsets-3 in C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822\__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:23:20.173 [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-01-01 00:23:20.173 [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-01-01 00:23:20.173 [Broker id=0] Leader __consumer_offsets-3 with topic id Some(7OWm5_wEQgu08JdHLM4wFw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:20.181 [LogLoader partition=__consumer_offsets-2, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:20.181 Created log for partition __consumer_offsets-2 in C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822\__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:23:20.182 [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-01-01 00:23:20.182 [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-01-01 00:23:20.182 [Broker id=0] Leader __consumer_offsets-2 with topic id Some(7OWm5_wEQgu08JdHLM4wFw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:20.189 [LogLoader partition=__consumer_offsets-4, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:20.189 Created log for partition __consumer_offsets-4 in C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822\__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:23:20.189 [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-01-01 00:23:20.189 [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-01-01 00:23:20.190 [Broker id=0] Leader __consumer_offsets-4 with topic id Some(7OWm5_wEQgu08JdHLM4wFw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:20.197 [LogLoader partition=__consumer_offsets-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:20.197 Created log for partition __consumer_offsets-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822\__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:23:20.197 [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-01-01 00:23:20.197 [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-01-01 00:23:20.197 [Broker id=0] Leader __consumer_offsets-1 with topic id Some(7OWm5_wEQgu08JdHLM4wFw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:20.205 [LogLoader partition=__consumer_offsets-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822] Loading producer state till offset 0 with message format version 2
2025-01-01 00:23:20.205 Created log for partition __consumer_offsets-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.92590c14-1377-4f6d-a4c6-2698a06fdbed10881364481290867822\__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:23:20.206 [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-01-01 00:23:20.206 [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-01-01 00:23:20.206 [Broker id=0] Leader __consumer_offsets-0 with topic id Some(7OWm5_wEQgu08JdHLM4wFw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:23:20.208 [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-01-01 00:23:20.209 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-01-01 00:23:20.209 [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-01-01 00:23:20.209 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-01-01 00:23:20.209 [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-01-01 00:23:20.209 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-01-01 00:23:20.209 [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-01-01 00:23:20.209 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-01-01 00:23:20.209 [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-01-01 00:23:20.209 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-01-01 00:23:20.209 [Broker id=0] Finished LeaderAndIsr request in 47ms correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:23:20.209 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:23:20.209 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:23:20.209 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:23:20.209 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:23:20.209 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:23:20.210 [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-01-01 00:23:20.254 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Discovered group coordinator localhost:60332 (id: 2147483647 rack: null)
2025-01-01 00:23:20.254 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] (Re-)joining group
2025-01-01 00:23:20.256 [GroupCoordinator 0]: Dynamic member with unknown member id joins group pong-service in Empty state. Created a new member id consumer-pong-service-3-9b8bdc0a-f71c-4c4f-baea-bd1f9a615e52 and request the member to rejoin with this id.
2025-01-01 00:23:20.257 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: need to re-join with the given member-id: consumer-pong-service-3-9b8bdc0a-f71c-4c4f-baea-bd1f9a615e52
2025-01-01 00:23:20.257 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2025-01-01 00:23:20.257 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] (Re-)joining group
2025-01-01 00:23:20.257 [GroupCoordinator 0]: Preparing to rebalance group pong-service in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-pong-service-3-9b8bdc0a-f71c-4c4f-baea-bd1f9a615e52 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)
2025-01-01 00:23:20.258 [GroupCoordinator 0]: Stabilized group pong-service generation 1 (__consumer_offsets-2) with 1 members
2025-01-01 00:23:20.258 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Successfully joined group with generation Generation{generationId=1, memberId='consumer-pong-service-3-9b8bdc0a-f71c-4c4f-baea-bd1f9a615e52', protocol='range'}
2025-01-01 00:23:20.258 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Finished assignment for group at generation 1: {consumer-pong-service-3-9b8bdc0a-f71c-4c4f-baea-bd1f9a615e52=Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])}
2025-01-01 00:23:20.259 [GroupCoordinator 0]: Assignment received from leader consumer-pong-service-3-9b8bdc0a-f71c-4c4f-baea-bd1f9a615e52 for group pong-service for generation 1. The group has 1 members, 0 of which are static.
2025-01-01 00:23:20.260 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Successfully synced group in generation Generation{generationId=1, memberId='consumer-pong-service-3-9b8bdc0a-f71c-4c4f-baea-bd1f9a615e52', protocol='range'}
2025-01-01 00:23:20.260 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Notifying assignor about the new Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])
2025-01-01 00:23:20.260 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Adding newly assigned partitions: pong-message-topic-0, pong-message-topic-1
2025-01-01 00:23:20.261 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:23:20.261 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:23:20.261 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:23:20.261 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:23:20.262 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting offset for partition pong-message-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60332 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:23:20.262 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting offset for partition pong-message-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60332 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:23:20.264 pong-service: partitions assigned: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:23:20.383 Received ping message: Hello
2025-01-01 00:23:20.383 Idempotence will be disabled because acks is set to 0, not set to 'all'.
2025-01-01 00:23:20.383 ProducerConfig values: 
	acks = 0
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:60332]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-01-01 00:23:20.386 Kafka version: 3.4.1
2025-01-01 00:23:20.386 Kafka commitId: 8a516edc2755df89
2025-01-01 00:23:20.386 Kafka startTimeMs: 1735662200386
2025-01-01 00:23:20.388 [Producer clientId=producer-2] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to yeHjjw77TeWN9z0Y9HwZLQ
2025-01-01 00:23:20.388 [Producer clientId=producer-2] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to yeHjjw77TeWN9z0Y9HwZLQ
2025-01-01 00:23:20.388 [Producer clientId=producer-2] Cluster ID: MXE1tUq6QN6ue67Js8Ez-Q
2025-01-01 00:23:21.553 Received ping message: Hello
2025-01-01 00:23:21.555 Throttling request as too many requests in the same second
2025-01-01 00:23:21.555 Throttling request as too many requests in the same second
2025-01-01 00:23:21.575 [KafkaServer id=0] shutting down
2025-01-01 00:23:21.575 [KafkaServer id=0] Starting controlled shutdown
2025-01-01 00:23:21.577 [Controller id=0] Shutting down broker 0
2025-01-01 00:23:21.577 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:23:21.578 [KafkaServer id=0] Controlled shutdown request returned successfully after 1ms
2025-01-01 00:23:21.578 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:23:21.578 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:23:21.578 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:23:21.578 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:23:21.578 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 2147483647 disconnected.
2025-01-01 00:23:21.578 [Producer clientId=producer-2] Node 0 disconnected.
2025-01-01 00:23:21.579 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Group coordinator localhost:60332 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2025-01-01 00:23:21.579 [Producer clientId=producer-2] Node -1 disconnected.
2025-01-01 00:23:21.579 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:23:21.579 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cancelled in-flight FETCH request with correlation id 19 due to node 0 being disconnected (elapsed time since creation: 23ms, elapsed time since send: 23ms, request timeout: 30000ms)
2025-01-01 00:23:21.579 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cancelled in-flight METADATA request with correlation id 22 due to node 0 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, request timeout: 30000ms)
2025-01-01 00:23:21.579 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:23:21.579 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Error sending fetch request (sessionId=1140025030, epoch=4) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
2025-01-01 00:23:21.579 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:23:21.579 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:23:21.580 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:23:21.580 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:23:21.580 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:23:21.580 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:23:21.580 [KafkaApi-0] Shutdown complete.
2025-01-01 00:23:21.580 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:23:21.581 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:23:21.581 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:23:21.581 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:23:21.581 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:23:21.581 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:23:21.581 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:23:21.581 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:23:21.581 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:23:21.581 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:23:21.581 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:23:21.581 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:23:21.581 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:23:21.581 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:23:21.581 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:23:21.581 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:23:21.581 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:23:21.581 [ReplicaManager broker=0] Shutting down
2025-01-01 00:23:21.581 [LogDirFailureHandler]: Shutting down
2025-01-01 00:23:21.581 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:23:21.581 [LogDirFailureHandler]: Stopped
2025-01-01 00:23:21.581 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:23:21.581 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:23:21.581 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:23:21.581 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:23:21.581 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:23:21.581 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:23:21.581 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:23:21.581 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:23:21.582 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:23:21.582 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:23:21.582 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:23:21.582 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:23:21.582 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:23:21.582 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:23:21.582 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:23:21.582 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:23:21.586 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:23:21.587 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:23:21.587 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:23:21.587 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:23:21.587 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:23:21.587 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:23:21.587 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:23:21.587 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:23:21.587 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:23:21.587 Shutting down.
2025-01-01 00:23:21.588 Shutting down the log cleaner.
2025-01-01 00:23:21.588 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:23:21.588 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:23:21.588 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:23:21.599 [ProducerStateManager partition=pong-message-topic-0] Wrote producer snapshot at offset 2 with 0 producer ids in 1 ms.
2025-01-01 00:23:21.608 [ProducerStateManager partition=__consumer_offsets-2] Wrote producer snapshot at offset 5 with 0 producer ids in 2 ms.
2025-01-01 00:23:21.614 Shutdown complete.
2025-01-01 00:23:21.614 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:23:21.614 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:23:21.614 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:23:21.614 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:23:21.614 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:23:21.614 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:23:21.614 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:23:21.614 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:23:21.615 [Controller id=0] Resigned
2025-01-01 00:23:21.615 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:23:21.615 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:23:21.615 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:23:21.615 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:23:21.680 [Producer clientId=producer-2] Node 0 disconnected.
2025-01-01 00:23:21.680 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:23:21.680 [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:60332) could not be established. Broker may not be available.
2025-01-01 00:23:21.680 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60332) could not be established. Broker may not be available.
2025-01-01 00:23:21.718 EventThread shut down for session: 0x100182277440000
2025-01-01 00:23:21.718 Session: 0x100182277440000 closed
2025-01-01 00:23:21.719 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:23:21.719 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:23:21.719 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:23:21.719 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:23:21.719 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:23:21.719 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:23:21.719 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:23:21.719 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:23:21.719 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:23:21.719 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:23:21.719 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:23:21.719 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:23:21.719 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:23:21.719 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:23:21.723 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:23:21.723 Metrics scheduler closed
2025-01-01 00:23:21.723 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:23:21.723 Metrics reporters closed
2025-01-01 00:23:21.723 Broker and topic stats closed
2025-01-01 00:23:21.723 App info kafka.server for 0 unregistered
2025-01-01 00:23:21.723 [KafkaServer id=0] shut down completed
2025-01-01 00:23:21.729 ConnnectionExpirerThread interrupted
2025-01-01 00:23:21.729 selector thread exitted run method
2025-01-01 00:23:21.729 accept thread exitted run method
2025-01-01 00:23:21.729 selector thread exitted run method
2025-01-01 00:23:21.729 shutting down
2025-01-01 00:23:21.729 Shutting down
2025-01-01 00:23:21.729 Draining request throttler queue
2025-01-01 00:23:21.729 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:23:21.729 Shutting down
2025-01-01 00:23:21.729 Shutting down
2025-01-01 00:23:21.729 Shutting down
2025-01-01 00:23:21.729 SyncRequestProcessor exited!
2025-01-01 00:23:21.729 PrepRequestProcessor exited loop!
2025-01-01 00:23:21.729 shutdown of request processor complete
2025-01-01 00:23:21.742 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Revoke previously assigned partitions pong-message-topic-0, pong-message-topic-1
2025-01-01 00:23:21.742 pong-service: partitions revoked: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:23:21.742 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:23:21.742 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:23:21.742 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:23:21.742 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:23:21.742 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:23:21.742 Metrics scheduler closed
2025-01-01 00:23:21.742 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:23:21.742 Metrics reporters closed
2025-01-01 00:23:21.742 App info kafka.consumer for consumer-pong-service-3 unregistered
2025-01-01 00:23:21.742 pong-service: Consumer stopped
2025-01-01 00:23:21.745 [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-01 00:23:21.745 Metrics scheduler closed
2025-01-01 00:23:21.745 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:23:21.745 Metrics reporters closed
2025-01-01 00:23:21.745 App info kafka.producer for producer-2 unregistered
2025-01-01 00:23:21.747 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:23:21.748 HikariPool-3 - Shutdown initiated...
2025-01-01 00:23:21.748 HikariPool-3 - Shutdown completed.
2025-01-01 00:23:21.891 SessionTrackerImpl exited loop!
2025-01-01 00:23:23.765 [KafkaServer id=0] shutting down
2025-01-01 00:23:23.765 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:23:23.765 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:23:23.765 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:23:23.765 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:23:23.766 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:23:23.766 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:23:23.766 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:23:23.766 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:23:23.767 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:23:23.767 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:23:23.767 [KafkaApi-0] Shutdown complete.
2025-01-01 00:23:23.767 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:23:23.767 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:23:23.767 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:23:23.767 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:23:23.767 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:23:23.767 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:23:23.767 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:23:23.767 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:23:23.768 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:23:23.768 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:23:23.768 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:23:23.768 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:23:23.768 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:23:23.768 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:23:23.768 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:23:23.768 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:23:23.768 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:23:23.768 [ReplicaManager broker=0] Shutting down
2025-01-01 00:23:23.768 [LogDirFailureHandler]: Shutting down
2025-01-01 00:23:23.768 [LogDirFailureHandler]: Stopped
2025-01-01 00:23:23.768 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:23:23.768 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:23:23.768 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:23:23.768 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:23:23.768 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:23:23.768 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:23:23.768 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:23:23.768 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:23:23.768 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:23:23.768 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:23:23.768 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:23:23.768 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:23:23.768 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:23:23.768 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:23:23.768 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:23:23.770 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:23:23.770 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:23:23.774 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:23:23.774 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:23:23.774 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:23:23.774 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:23:23.775 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:23:23.775 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:23:23.775 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:23:23.775 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:23:23.775 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:23:23.775 Shutting down.
2025-01-01 00:23:23.775 Shutting down the log cleaner.
2025-01-01 00:23:23.775 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:23:23.775 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:23:23.775 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:23:23.781 Shutdown complete.
2025-01-01 00:23:23.781 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:23:23.781 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:23:23.781 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:23:23.781 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:23:23.781 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:23:23.781 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:23:23.781 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:23:23.781 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:23:23.781 [Controller id=0] Resigned
2025-01-01 00:23:23.781 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:23:23.781 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:23:23.781 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:23:23.781 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:23:23.884 Session: 0x100182277dd0000 closed
2025-01-01 00:23:23.884 EventThread shut down for session: 0x100182277dd0000
2025-01-01 00:23:23.884 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:23:23.884 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:23:23.884 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:23:23.884 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:23:23.884 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:23:23.884 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:23:23.884 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:23:23.884 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:23:23.884 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:23:23.884 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:23:23.884 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:23:23.884 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:23:23.884 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:23:23.884 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:23:23.886 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:23:23.886 Metrics scheduler closed
2025-01-01 00:23:23.886 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:23:23.886 Metrics reporters closed
2025-01-01 00:23:23.888 Broker and topic stats closed
2025-01-01 00:23:23.888 App info kafka.server for 0 unregistered
2025-01-01 00:23:23.888 [KafkaServer id=0] shut down completed
2025-01-01 00:23:23.890 ConnnectionExpirerThread interrupted
2025-01-01 00:23:23.890 selector thread exitted run method
2025-01-01 00:23:23.890 accept thread exitted run method
2025-01-01 00:23:23.890 selector thread exitted run method
2025-01-01 00:23:23.890 shutting down
2025-01-01 00:23:23.890 Shutting down
2025-01-01 00:23:23.890 Draining request throttler queue
2025-01-01 00:23:23.890 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:23:23.890 Shutting down
2025-01-01 00:23:23.890 Shutting down
2025-01-01 00:23:23.890 Shutting down
2025-01-01 00:23:23.890 SyncRequestProcessor exited!
2025-01-01 00:23:23.890 PrepRequestProcessor exited loop!
2025-01-01 00:23:23.890 shutdown of request processor complete
2025-01-01 00:25:30.723 Could not detect default configuration classes for test class [com.demo.controller.PongControllerSpec]: PongControllerSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:25:30.848 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.controller.PongControllerSpec
2025-01-01 00:25:31.006 Could not detect default configuration classes for test class [com.demo.dao.MessageRepositorySpec]: MessageRepositorySpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:25:31.006 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.dao.MessageRepositorySpec
2025-01-01 00:25:31.025 Could not detect default configuration classes for test class [com.demo.kafka.KafkaSpec]: KafkaSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:25:31.044 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.kafka.KafkaSpec
2025-01-01 00:25:31.070 Could not detect default configuration classes for test class [com.demo.controller.PongControllerSpec]: PongControllerSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:25:31.070 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.controller.PongControllerSpec
2025-01-01 00:25:31.077 Could not detect default configuration classes for test class [com.demo.dao.MessageRepositorySpec]: MessageRepositorySpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:25:31.077 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.dao.MessageRepositorySpec
2025-01-01 00:25:31.077 Could not detect default configuration classes for test class [com.demo.kafka.KafkaSpec]: KafkaSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:25:31.084 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.kafka.KafkaSpec
2025-01-01 00:25:31.359 Registered kafka:type=kafka.Log4jController MBean
2025-01-01 00:25:31.419 
2025-01-01 00:25:31.419   ______                  _                                          
2025-01-01 00:25:31.419  |___  /                 | |                                         
2025-01-01 00:25:31.419     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __   
2025-01-01 00:25:31.419    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__|
2025-01-01 00:25:31.419   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |    
2025-01-01 00:25:31.419  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_|
2025-01-01 00:25:31.419                                               | |                     
2025-01-01 00:25:31.419                                               |_|                     
2025-01-01 00:25:31.419 
2025-01-01 00:25:36.673 Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:25:36.673 Server environment:host.name=Joe
2025-01-01 00:25:36.673 Server environment:java.version=17.0.12
2025-01-01 00:25:36.673 Server environment:java.vendor=Oracle Corporation
2025-01-01 00:25:36.673 Server environment:java.home=C:\work\jdk-17
2025-01-01 00:25:36.673 Server environment:java.class.path=C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\work\apache-maven-3.8.8\repo\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\work\apache-maven-3.8.8\repo\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\guava\guava\19.0\guava-19.0.jar;C:\work\apache-maven-3.8.8\repo\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\work\apache-maven-3.8.8\repo\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\work\apache-maven-3.8.8\repo\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\work\apache-maven-3.8.8\repo\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\work\apache-maven-3.8.8\repo\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\work\apache-maven-3.8.8\repo\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\work\apache-maven-3.8.8\repo\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\work\apache-maven-3.8.8\repo\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\work\apache-maven-3.8.8\repo\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\jline\jline\2.14.6\jline-2.14.6.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\junit\junit\4.13.2\junit-4.13.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\work\apache-maven-3.8.8\repo\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\work\apache-maven-3.8.8\repo\org\json\json\20220320\json-20220320.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\work\apache-maven-3.8.8\repo\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\work\apache-maven-3.8.8\repo\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\work\apache-maven-3.8.8\repo\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar;
2025-01-01 00:25:36.675 Server environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:25:36.675 Server environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:25:36.675 Server environment:java.compiler=<NA>
2025-01-01 00:25:36.675 Server environment:os.name=Windows 11
2025-01-01 00:25:36.675 Server environment:os.arch=amd64
2025-01-01 00:25:36.675 Server environment:os.version=10.0
2025-01-01 00:25:36.675 Server environment:user.name=joech
2025-01-01 00:25:36.675 Server environment:user.home=C:\Users\joech
2025-01-01 00:25:36.675 Server environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:25:36.675 Server environment:os.memory.free=453MB
2025-01-01 00:25:36.675 Server environment:os.memory.max=15816MB
2025-01-01 00:25:36.675 Server environment:os.memory.total=568MB
2025-01-01 00:25:36.675 zookeeper.enableEagerACLCheck = false
2025-01-01 00:25:36.675 zookeeper.digest.enabled = true
2025-01-01 00:25:36.675 zookeeper.closeSessionTxn.enabled = true
2025-01-01 00:25:36.675 zookeeper.flushDelay=0
2025-01-01 00:25:36.675 zookeeper.maxWriteQueuePollTime=0
2025-01-01 00:25:36.675 zookeeper.maxBatchSize=1000
2025-01-01 00:25:36.675 zookeeper.intBufferStartingSizeBytes = 1024
2025-01-01 00:25:36.679 zookeeper.snapshot.trust.empty : false
2025-01-01 00:25:36.688 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:25:36.688 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:25:36.688 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:25:36.688 zookeeper.commitLogCount=500
2025-01-01 00:25:36.688 Weighed connection throttling is disabled
2025-01-01 00:25:36.688 minSessionTimeout set to 1600
2025-01-01 00:25:36.688 maxSessionTimeout set to 16000
2025-01-01 00:25:36.688 Response cache size is initialized with value 400.
2025-01-01 00:25:36.688 Response cache size is initialized with value 400.
2025-01-01 00:25:36.705 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:25:36.705 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:25:36.705 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:25:36.705 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:25:36.705 zookeeper.pathStats.delay = 5
2025-01-01 00:25:36.705 zookeeper.pathStats.enabled = false
2025-01-01 00:25:36.705 The max bytes for all large requests are set to 104857600
2025-01-01 00:25:36.705 The large request threshold is set to -1
2025-01-01 00:25:36.705 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-8668157556877916606\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-9194841038971812338\version-2
2025-01-01 00:25:36.739 maxCnxns is not configured, using default value 0.
2025-01-01 00:25:36.739 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:25:36.739 binding to port /127.0.0.1:0
2025-01-01 00:25:36.762 zookeeper.snapshot.compression.method = CHECKED
2025-01-01 00:25:36.762 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-9194841038971812338\version-2\snapshot.0
2025-01-01 00:25:36.765 Snapshot loaded in 15 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:25:36.766 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-9194841038971812338\version-2\snapshot.0
2025-01-01 00:25:36.766 Snapshot taken in 1 ms
2025-01-01 00:25:36.776 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:25:36.776 zookeeper.request_throttler.shutdownTimeout = 10000
2025-01-01 00:25:36.989 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60377
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:25:37.016 Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2025-01-01 00:25:37.086 starting
2025-01-01 00:25:37.087 Connecting to zookeeper on 127.0.0.1:60377
2025-01-01 00:25:37.103 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60377.
2025-01-01 00:25:42.317 Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:25:42.317 Client environment:host.name=Joe
2025-01-01 00:25:42.317 Client environment:java.version=17.0.12
2025-01-01 00:25:42.318 Client environment:java.vendor=Oracle Corporation
2025-01-01 00:25:42.318 Client environment:java.home=C:\work\jdk-17
2025-01-01 00:25:42.318 Client environment:java.class.path=C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\work\apache-maven-3.8.8\repo\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\work\apache-maven-3.8.8\repo\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\guava\guava\19.0\guava-19.0.jar;C:\work\apache-maven-3.8.8\repo\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\work\apache-maven-3.8.8\repo\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\work\apache-maven-3.8.8\repo\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\work\apache-maven-3.8.8\repo\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\work\apache-maven-3.8.8\repo\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\work\apache-maven-3.8.8\repo\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\work\apache-maven-3.8.8\repo\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\work\apache-maven-3.8.8\repo\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\work\apache-maven-3.8.8\repo\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\jline\jline\2.14.6\jline-2.14.6.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\junit\junit\4.13.2\junit-4.13.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\work\apache-maven-3.8.8\repo\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\work\apache-maven-3.8.8\repo\org\json\json\20220320\json-20220320.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\work\apache-maven-3.8.8\repo\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\work\apache-maven-3.8.8\repo\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\work\apache-maven-3.8.8\repo\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar;
2025-01-01 00:25:42.319 Client environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:25:42.319 Client environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:25:42.319 Client environment:java.compiler=<NA>
2025-01-01 00:25:42.319 Client environment:os.name=Windows 11
2025-01-01 00:25:42.319 Client environment:os.arch=amd64
2025-01-01 00:25:42.319 Client environment:os.version=10.0
2025-01-01 00:25:42.319 Client environment:user.name=joech
2025-01-01 00:25:42.319 Client environment:user.home=C:\Users\joech
2025-01-01 00:25:42.319 Client environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:25:42.319 Client environment:os.memory.free=95MB
2025-01-01 00:25:42.319 Client environment:os.memory.max=15816MB
2025-01-01 00:25:42.319 Client environment:os.memory.total=136MB
2025-01-01 00:25:42.323 Initiating client connection, connectString=127.0.0.1:60377 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@7625bfbd
2025-01-01 00:25:42.327 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:25:42.333 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:25:42.334 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:25:42.334 Opening socket connection to server /127.0.0.1:60377.
2025-01-01 00:25:42.336 Socket connection established, initiating session, client: /127.0.0.1:60378, server: /127.0.0.1:60377
2025-01-01 00:25:42.343 Creating new log file: log.1
2025-01-01 00:25:42.353 ZooKeeper audit is disabled.
2025-01-01 00:25:42.357 Session establishment complete on server /127.0.0.1:60377, session id = 0x10018248fbf0000, negotiated timeout = 16000
2025-01-01 00:25:42.360 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:25:42.677 Cluster ID = 9uLZitUZT2-0E5-V5TR5kQ
2025-01-01 00:25:42.681 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127\meta.properties
2025-01-01 00:25:42.725 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60377
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:25:42.753 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:25:42.754 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:25:42.755 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:25:42.757 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:25:42.787 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127)
2025-01-01 00:25:42.790 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127 since no clean shutdown file was found
2025-01-01 00:25:42.798 Loaded 0 logs in 11ms.
2025-01-01 00:25:42.799 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:25:42.801 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:25:42.815 Starting the log cleaner
2025-01-01 00:25:42.826 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:25:42.841 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:25:42.853 Feature ZK node at path: /feature does not exist
2025-01-01 00:25:42.888 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:25:43.266 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:25:43.274 Awaiting socket connections on localhost:60383.
2025-01-01 00:25:43.303 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:25:43.312 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:25:43.329 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:25:43.330 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:25:43.331 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:25:43.331 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:25:43.353 [LogDirFailureHandler]: Starting
2025-01-01 00:25:43.375 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:25:43.396 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662343388,1735662343388,1,0,0,72084139347476480,204,0,25

2025-01-01 00:25:43.397 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60383, czxid (broker epoch): 25
2025-01-01 00:25:43.462 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:25:43.467 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:25:43.474 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:25:43.475 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:25:43.482 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:25:43.487 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:25:43.489 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:25:43.489 Feature ZK node created at path: /feature
2025-01-01 00:25:43.495 [GroupCoordinator 0]: Starting up.
2025-01-01 00:25:43.498 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:25:43.521 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:25:43.521 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:25:43.521 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:25:43.538 [Controller id=0] Registering handlers
2025-01-01 00:25:43.538 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:25:43.543 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:25:43.547 [Controller id=0] Deleting isr change notifications
2025-01-01 00:25:43.549 [Controller id=0] Initializing controller context
2025-01-01 00:25:43.569 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:25:43.575 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:25:43.597 [RequestSendThread controllerId=0] Starting
2025-01-01 00:25:43.601 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:25:43.601 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:25:43.601 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:25:43.601 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:25:43.603 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:25:43.603 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:25:43.603 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:25:43.603 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:25:43.608 [Controller id=0] Sending update metadata request
2025-01-01 00:25:43.616 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:25:43.626 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:25:43.626 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:25:43.626 [/config/changes-event-process-thread]: Starting
2025-01-01 00:25:43.633 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:25:43.633 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:25:43.633 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:25:43.640 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:25:43.640 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60383 (id: 0 rack: null) for sending state change requests
2025-01-01 00:25:43.645 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:25:43.645 Kafka version: 3.4.1
2025-01-01 00:25:43.645 Kafka commitId: 8a516edc2755df89
2025-01-01 00:25:43.645 Kafka startTimeMs: 1735662343640
2025-01-01 00:25:43.649 [KafkaServer id=0] started
2025-01-01 00:25:43.651 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:25:43.651 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:25:43.651 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:25:43.651 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:25:43.651 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:25:43.657 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60383]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:25:43.672 [Controller id=0] Starting the controller scheduler
2025-01-01 00:25:43.704 Kafka version: 3.4.1
2025-01-01 00:25:43.704 Kafka commitId: 8a516edc2755df89
2025-01-01 00:25:43.704 Kafka startTimeMs: 1735662343704
2025-01-01 00:25:43.740 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60383 (id: 0 rack: null)
2025-01-01 00:25:43.832 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60383 (id: 0 rack: null)
2025-01-01 00:25:43.834 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0))
2025-01-01 00:25:43.862 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(BoYV-0FgQB-HDZiM5UpC6Q),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), pong-message-topic-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:25:43.863 [Controller id=0] New partition creation callback for pong-message-topic-0,pong-message-topic-1
2025-01-01 00:25:43.866 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:43.866 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:43.866 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:43.866 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:43.895 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:43.895 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:43.895 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions
2025-01-01 00:25:43.900 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 2 partitions
2025-01-01 00:25:43.900 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:43.906 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:25:43.948 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0, pong-message-topic-1)
2025-01-01 00:25:43.948 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
2025-01-01 00:25:44.048 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:44.064 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127\pong-message-topic-0 with properties {}
2025-01-01 00:25:44.064 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:25:44.064 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:25:44.064 [Broker id=0] Leader pong-message-topic-0 with topic id Some(BoYV-0FgQB-HDZiM5UpC6Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:44.093 [LogLoader partition=pong-message-topic-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:44.096 Created log for partition pong-message-topic-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127\pong-message-topic-1 with properties {}
2025-01-01 00:25:44.096 [Partition pong-message-topic-1 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-1
2025-01-01 00:25:44.096 [Partition pong-message-topic-1 broker=0] Log loaded for partition pong-message-topic-1 with initial high watermark 0
2025-01-01 00:25:44.096 [Broker id=0] Leader pong-message-topic-1 with topic id Some(BoYV-0FgQB-HDZiM5UpC6Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:44.104 [Broker id=0] Finished LeaderAndIsr request in 201ms correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:25:44.111 [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:25:44.121 App info kafka.admin.client for adminclient-1 unregistered
2025-01-01 00:25:44.124 Metrics scheduler closed
2025-01-01 00:25:44.124 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:25:44.124 Metrics reporters closed
2025-01-01 00:25:44.476 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/work/apache-maven-3.8.8/repo/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/work/apache-maven-3.8.8/repo/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:25:44.497 zookeeper.snapshot.trust.empty : false
2025-01-01 00:25:44.498 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:25:44.498 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:25:44.498 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:25:44.498 zookeeper.commitLogCount=500
2025-01-01 00:25:44.498 minSessionTimeout set to 1600
2025-01-01 00:25:44.498 maxSessionTimeout set to 16000
2025-01-01 00:25:44.498 Response cache size is initialized with value 400.
2025-01-01 00:25:44.498 Response cache size is initialized with value 400.
2025-01-01 00:25:44.498 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:25:44.498 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:25:44.498 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:25:44.498 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:25:44.498 zookeeper.pathStats.delay = 5
2025-01-01 00:25:44.498 zookeeper.pathStats.enabled = false
2025-01-01 00:25:44.498 The max bytes for all large requests are set to 104857600
2025-01-01 00:25:44.498 The large request threshold is set to -1
2025-01-01 00:25:44.498 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-12290615184600310703\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-10222558806073198576\version-2
2025-01-01 00:25:44.498 maxCnxns is not configured, using default value 0.
2025-01-01 00:25:44.498 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:25:44.501 binding to port /127.0.0.1:0
2025-01-01 00:25:44.504 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-10222558806073198576\version-2\snapshot.0
2025-01-01 00:25:44.505 Snapshot loaded in 2 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:25:44.505 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-10222558806073198576\version-2\snapshot.0
2025-01-01 00:25:44.506 Snapshot taken in 1 ms
2025-01-01 00:25:44.507 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:25:44.509 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.ffb3cee2-6549-4478-a364-3a031fbda0b417760730022697697920
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60387
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:25:44.513 starting
2025-01-01 00:25:44.513 Connecting to zookeeper on 127.0.0.1:60387
2025-01-01 00:25:44.513 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60387.
2025-01-01 00:25:44.513 Initiating client connection, connectString=127.0.0.1:60387 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@637ae337
2025-01-01 00:25:44.515 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:25:44.515 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:25:44.515 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:25:44.515 Opening socket connection to server /127.0.0.1:60387.
2025-01-01 00:25:44.516 Socket connection established, initiating session, client: /127.0.0.1:60388, server: /127.0.0.1:60387
2025-01-01 00:25:44.517 Creating new log file: log.1
2025-01-01 00:25:44.521 Session establishment complete on server /127.0.0.1:60387, session id = 0x1001824adfa0000, negotiated timeout = 16000
2025-01-01 00:25:44.521 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:25:44.553 Cluster ID = p_vOjsynR2WQOddnTYw1qg
2025-01-01 00:25:44.553 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.ffb3cee2-6549-4478-a364-3a031fbda0b417760730022697697920\meta.properties
2025-01-01 00:25:44.563 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.ffb3cee2-6549-4478-a364-3a031fbda0b417760730022697697920
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60387
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:25:44.576 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:25:44.576 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:25:44.576 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:25:44.576 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:25:44.581 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.ffb3cee2-6549-4478-a364-3a031fbda0b417760730022697697920)
2025-01-01 00:25:44.581 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.ffb3cee2-6549-4478-a364-3a031fbda0b417760730022697697920 since no clean shutdown file was found
2025-01-01 00:25:44.582 Loaded 0 logs in 1ms.
2025-01-01 00:25:44.583 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:25:44.583 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:25:44.584 Starting the log cleaner
2025-01-01 00:25:44.585 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:25:44.587 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:25:44.588 Feature ZK node at path: /feature does not exist
2025-01-01 00:25:44.591 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:25:44.610 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:25:44.611 Awaiting socket connections on localhost:60389.
2025-01-01 00:25:44.619 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:25:44.624 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:25:44.624 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:25:44.625 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:25:44.625 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:25:44.626 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:25:44.628 [LogDirFailureHandler]: Starting
2025-01-01 00:25:44.631 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:25:44.633 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662344632,1735662344632,1,0,0,72084139854659584,204,0,25

2025-01-01 00:25:44.633 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60389, czxid (broker epoch): 25
2025-01-01 00:25:44.639 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:25:44.639 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:25:44.640 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:25:44.641 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:25:44.641 [GroupCoordinator 0]: Starting up.
2025-01-01 00:25:44.643 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:25:44.644 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:25:44.646 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:25:44.646 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:25:44.648 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:25:44.648 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:25:44.649 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:25:44.649 Feature ZK node created at path: /feature
2025-01-01 00:25:44.651 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:25:44.652 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:25:44.652 [Controller id=0] Registering handlers
2025-01-01 00:25:44.653 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:25:44.655 [Controller id=0] Deleting isr change notifications
2025-01-01 00:25:44.656 [/config/changes-event-process-thread]: Starting
2025-01-01 00:25:44.656 [Controller id=0] Initializing controller context
2025-01-01 00:25:44.660 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:25:44.661 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:25:44.663 Kafka version: 3.4.1
2025-01-01 00:25:44.663 Kafka commitId: 8a516edc2755df89
2025-01-01 00:25:44.663 Kafka startTimeMs: 1735662344663
2025-01-01 00:25:44.668 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:25:44.668 [RequestSendThread controllerId=0] Starting
2025-01-01 00:25:44.668 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:25:44.668 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:25:44.668 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:25:44.670 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:25:44.670 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:25:44.670 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:25:44.664 Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.server:type=app-info,id=0
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1865)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:960)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:895)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:523)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:563)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:188)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:182)
	at kafka.utils.TestUtils.createServer(TestUtils.scala)
	at org.springframework.kafka.test.EmbeddedKafkaBroker.afterPropertiesSet(EmbeddedKafkaBroker.java:369)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1817)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1766)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:410)
	at org.springframework.kafka.test.context.EmbeddedKafkaContextCustomizer.customizeContext(EmbeddedKafkaContextCustomizer.java:116)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextCustomizerAdapter.initialize(SpringBootContextLoader.java:435)
	at org.springframework.boot.SpringApplication.applyInitializers(SpringApplication.java:610)
	at org.springframework.boot.SpringApplication.prepareContext(SpringApplication.java:390)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:314)
	at org.springframework.boot.test.context.SpringBootContextLoader.lambda$loadContext$3(SpringBootContextLoader.java:137)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:58)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:46)
	at org.springframework.boot.SpringApplication.withHook(SpringApplication.java:1409)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextLoaderHook.run(SpringBootContextLoader.java:545)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:137)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:108)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContextInternal(DefaultCacheAwareContextLoaderDelegate.java:187)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:119)
	at org.springframework.test.context.support.DefaultTestContext.getApplicationContext(DefaultTestContext.java:127)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:141)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:97)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:241)
	at org.spockframework.spring.SpringTestContextManager.prepareTestInstance(SpringTestContextManager.java:56)
	at org.spockframework.spring.SpringInterceptor.interceptInitializerMethod(SpringInterceptor.java:46)
	at org.spockframework.runtime.extension.AbstractMethodInterceptor.intercept(AbstractMethodInterceptor.java:24)
	at org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)
	at org.spockframework.runtime.PlatformSpecRunner.runInitializer(PlatformSpecRunner.java:259)
	at org.spockframework.runtime.PlatformSpecRunner.runInitializer(PlatformSpecRunner.java:254)
	at org.spockframework.runtime.IterationNode.prepare(IterationNode.java:31)
	at org.spockframework.runtime.IterationNode.prepare(IterationNode.java:13)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$prepare$2(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.prepare(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:90)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
	at org.spockframework.runtime.ParameterizedFeatureChildExecutor.execute(ParameterizedFeatureChildExecutor.java:104)
	at org.spockframework.runtime.PlatformParameterizedSpecRunner$1.runIteration(PlatformParameterizedSpecRunner.java:72)
	at org.spockframework.runtime.extension.IDataDriver.lambda$static$0(IDataDriver.java:37)
	at org.spockframework.runtime.PlatformParameterizedSpecRunner.runParameterizedFeature(PlatformParameterizedSpecRunner.java:47)
	at org.spockframework.runtime.ParameterizedFeatureNode.execute(ParameterizedFeatureNode.java:40)
	at org.spockframework.runtime.ParameterizedFeatureNode.execute(ParameterizedFeatureNode.java:16)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)
	at org.spockframework.runtime.FeatureNode.lambda$around$0(FeatureNode.java:41)
	at org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunFeature$4(PlatformSpecRunner.java:199)
	at org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)
	at org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)
	at org.spockframework.runtime.PlatformSpecRunner.runFeature(PlatformSpecRunner.java:192)
	at org.spockframework.runtime.FeatureNode.around(FeatureNode.java:41)
	at org.spockframework.runtime.FeatureNode.around(FeatureNode.java:12)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)
	at org.spockframework.runtime.SpecNode.lambda$around$0(SpecNode.java:63)
	at org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunSpec$0(PlatformSpecRunner.java:61)
	at org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)
	at org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)
	at org.spockframework.runtime.PlatformSpecRunner.runSpec(PlatformSpecRunner.java:55)
	at org.spockframework.runtime.SpecNode.around(SpecNode.java:63)
	at org.spockframework.runtime.SpecNode.around(SpecNode.java:11)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2025-01-01 00:25:44.670 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:25:44.670 [KafkaServer id=0] started
2025-01-01 00:25:44.670 [Controller id=0] Sending update metadata request
2025-01-01 00:25:44.670 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:25:44.670 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:25:44.670 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:25:44.671 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:25:44.671 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:25:44.671 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:25:44.671 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:25:44.671 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60389]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:25:44.673 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60389 (id: 0 rack: null) for sending state change requests
2025-01-01 00:25:44.676 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:25:44.676 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:25:44.676 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:25:44.676 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:25:44.676 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:25:44.678 [Controller id=0] Starting the controller scheduler
2025-01-01 00:25:44.678 Kafka version: 3.4.1
2025-01-01 00:25:44.678 Kafka commitId: 8a516edc2755df89
2025-01-01 00:25:44.678 Kafka startTimeMs: 1735662344678
2025-01-01 00:25:44.694 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60389 (id: 0 rack: null)
2025-01-01 00:25:44.698 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-01-01 00:25:44.709 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(UNTIG-uzRgOJf5q8SjMc4A),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:25:44.709 [Controller id=0] New partition creation callback for pong-message-topic-0
2025-01-01 00:25:44.709 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:44.709 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:44.709 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:44.716 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:44.716 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
2025-01-01 00:25:44.718 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions
2025-01-01 00:25:44.718 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:44.719 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 1 partitions
2025-01-01 00:25:44.720 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0)
2025-01-01 00:25:44.720 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
2025-01-01 00:25:44.724 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60389 (id: 0 rack: null)
2025-01-01 00:25:44.730 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.ffb3cee2-6549-4478-a364-3a031fbda0b417760730022697697920] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:44.731 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.ffb3cee2-6549-4478-a364-3a031fbda0b417760730022697697920\pong-message-topic-0 with properties {}
2025-01-01 00:25:44.731 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:25:44.731 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:25:44.731 [Broker id=0] Leader pong-message-topic-0 with topic id Some(UNTIG-uzRgOJf5q8SjMc4A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:44.736 [Broker id=0] Finished LeaderAndIsr request in 17ms correlationId 1 from controller 0 for 1 partitions
2025-01-01 00:25:44.739 [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:25:44.741 App info kafka.admin.client for adminclient-2 unregistered
2025-01-01 00:25:44.741 Metrics scheduler closed
2025-01-01 00:25:44.743 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:25:44.743 Metrics reporters closed
2025-01-01 00:25:44.753 Starting PongControllerSpec using Java 17.0.12 with PID 24192 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:25:44.753 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:25:45.348 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:25:45.349 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:25:45.497 Finished Spring Data repository scanning in 138 ms. Found 1 JPA repository interfaces.
2025-01-01 00:25:45.518 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:25:45.521 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:25:45.541 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:25:45.541 Finished Spring Data repository scanning in 6 ms. Found 0 Redis repository interfaces.
2025-01-01 00:25:46.181 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:25:46.259 HHH000412: Hibernate ORM core version 6.2.9.Final
2025-01-01 00:25:46.263 HHH000406: Using bytecode reflection optimizer
2025-01-01 00:25:46.430 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:25:46.609 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:25:46.636 HikariPool-1 - Starting...
2025-01-01 00:25:46.902 HikariPool-1 - Added connection conn0: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:25:46.907 HikariPool-1 - Start completed.
2025-01-01 00:25:47.207 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:25:47.843 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:25:47.934 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:25:48.677 [Controller id=0] Processing automatic preferred replica leader election
2025-01-01 00:25:49.166 Netty started on port 60393
2025-01-01 00:25:49.198 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60383]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:25:49.236 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:25:49.236 Kafka version: 3.4.1
2025-01-01 00:25:49.236 Kafka commitId: 8a516edc2755df89
2025-01-01 00:25:49.236 Kafka startTimeMs: 1735662349236
2025-01-01 00:25:49.237 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:25:49.253 Started PongControllerSpec in 5.048 seconds (process running for 20.277)
2025-01-01 00:25:49.263 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to BoYV-0FgQB-HDZiM5UpC6Q
2025-01-01 00:25:49.264 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to BoYV-0FgQB-HDZiM5UpC6Q
2025-01-01 00:25:49.265 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cluster ID: 9uLZitUZT2-0E5-V5TR5kQ
2025-01-01 00:25:49.267 Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-01-01 00:25:49.273 [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(wbchp_6vSoa9jNhkXJ5Rvg),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:25:49.274 [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-01-01 00:25:49.274 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:49.274 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:49.274 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:49.274 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:49.274 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:49.274 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:49.274 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:49.281 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:49.281 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:49.281 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:49.281 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:49.281 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:49.281 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-01-01 00:25:49.282 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-01-01 00:25:49.282 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:49.283 [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:25:49.285 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-01-01 00:25:49.285 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-01-01 00:25:49.293 [LogLoader partition=__consumer_offsets-3, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:49.295 Created log for partition __consumer_offsets-3 in C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127\__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:25:49.296 [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-01-01 00:25:49.296 [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-01-01 00:25:49.296 [Broker id=0] Leader __consumer_offsets-3 with topic id Some(wbchp_6vSoa9jNhkXJ5Rvg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:49.303 [LogLoader partition=__consumer_offsets-2, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:49.304 Created log for partition __consumer_offsets-2 in C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127\__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:25:49.304 [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-01-01 00:25:49.304 [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-01-01 00:25:49.304 [Broker id=0] Leader __consumer_offsets-2 with topic id Some(wbchp_6vSoa9jNhkXJ5Rvg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:49.313 [LogLoader partition=__consumer_offsets-4, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:49.314 Created log for partition __consumer_offsets-4 in C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127\__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:25:49.314 [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-01-01 00:25:49.314 [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-01-01 00:25:49.314 [Broker id=0] Leader __consumer_offsets-4 with topic id Some(wbchp_6vSoa9jNhkXJ5Rvg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:49.322 [LogLoader partition=__consumer_offsets-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:49.323 Created log for partition __consumer_offsets-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127\__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:25:49.323 [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-01-01 00:25:49.323 [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-01-01 00:25:49.323 [Broker id=0] Leader __consumer_offsets-1 with topic id Some(wbchp_6vSoa9jNhkXJ5Rvg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:49.331 [LogLoader partition=__consumer_offsets-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:49.332 Created log for partition __consumer_offsets-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.b86650b6-7568-42dd-968a-fa74e80d78675718258239440450127\__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:25:49.332 [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-01-01 00:25:49.332 [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-01-01 00:25:49.332 [Broker id=0] Leader __consumer_offsets-0 with topic id Some(wbchp_6vSoa9jNhkXJ5Rvg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:49.335 [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-01-01 00:25:49.336 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-01-01 00:25:49.337 [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-01-01 00:25:49.337 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-01-01 00:25:49.337 [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-01-01 00:25:49.337 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-01-01 00:25:49.337 [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-01-01 00:25:49.337 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-01-01 00:25:49.337 [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-01-01 00:25:49.337 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-01-01 00:25:49.337 [Broker id=0] Finished LeaderAndIsr request in 55ms correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:25:49.339 [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-01-01 00:25:49.340 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 3 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler.
2025-01-01 00:25:49.341 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler.
2025-01-01 00:25:49.341 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler.
2025-01-01 00:25:49.341 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler.
2025-01-01 00:25:49.341 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler.
2025-01-01 00:25:49.369 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Discovered group coordinator localhost:60383 (id: 2147483647 rack: null)
2025-01-01 00:25:49.370 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:25:49.392 [GroupCoordinator 0]: Dynamic member with unknown member id joins group pong-service in Empty state. Created a new member id consumer-pong-service-1-3a097a4e-d169-45db-8096-47ad2ce7ccc5 and request the member to rejoin with this id.
2025-01-01 00:25:49.396 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: need to re-join with the given member-id: consumer-pong-service-1-3a097a4e-d169-45db-8096-47ad2ce7ccc5
2025-01-01 00:25:49.397 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2025-01-01 00:25:49.397 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:25:49.403 [GroupCoordinator 0]: Preparing to rebalance group pong-service in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-pong-service-1-3a097a4e-d169-45db-8096-47ad2ce7ccc5 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)
2025-01-01 00:25:49.409 [GroupCoordinator 0]: Stabilized group pong-service generation 1 (__consumer_offsets-2) with 1 members
2025-01-01 00:25:49.410 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully joined group with generation Generation{generationId=1, memberId='consumer-pong-service-1-3a097a4e-d169-45db-8096-47ad2ce7ccc5', protocol='range'}
2025-01-01 00:25:49.412 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Finished assignment for group at generation 1: {consumer-pong-service-1-3a097a4e-d169-45db-8096-47ad2ce7ccc5=Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])}
2025-01-01 00:25:49.419 [GroupCoordinator 0]: Assignment received from leader consumer-pong-service-1-3a097a4e-d169-45db-8096-47ad2ce7ccc5 for group pong-service for generation 1. The group has 1 members, 0 of which are static.
2025-01-01 00:25:49.461 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully synced group in generation Generation{generationId=1, memberId='consumer-pong-service-1-3a097a4e-d169-45db-8096-47ad2ce7ccc5', protocol='range'}
2025-01-01 00:25:49.461 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Notifying assignor about the new Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])
2025-01-01 00:25:49.463 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Adding newly assigned partitions: pong-message-topic-0, pong-message-topic-1
2025-01-01 00:25:49.472 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:25:49.472 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:25:49.476 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:25:49.476 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:25:49.486 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60383 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:25:49.486 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60383 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:25:49.502 pong-service: partitions assigned: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:25:49.680 [Controller id=0] Processing automatic preferred replica leader election
2025-01-01 00:25:50.167 Received ping message: Hello
2025-01-01 00:25:50.176 Idempotence will be disabled because acks is set to 0, not set to 'all'.
2025-01-01 00:25:50.176 ProducerConfig values: 
	acks = 0
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:60383]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-01-01 00:25:50.196 Kafka version: 3.4.1
2025-01-01 00:25:50.196 Kafka commitId: 8a516edc2755df89
2025-01-01 00:25:50.196 Kafka startTimeMs: 1735662350196
2025-01-01 00:25:50.200 [Producer clientId=producer-1] Cluster ID: 9uLZitUZT2-0E5-V5TR5kQ
2025-01-01 00:25:50.204 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to BoYV-0FgQB-HDZiM5UpC6Q
2025-01-01 00:25:50.204 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to BoYV-0FgQB-HDZiM5UpC6Q
2025-01-01 00:25:51.471 Received ping message: Hello
2025-01-01 00:25:51.476 Throttling request as too many requests in the same second
2025-01-01 00:25:51.476 Received ping message: Hello
2025-01-01 00:25:51.742 [KafkaServer id=0] shutting down
2025-01-01 00:25:51.742 [KafkaServer id=0] Starting controlled shutdown
2025-01-01 00:25:51.749 [Controller id=0] Shutting down broker 0
2025-01-01 00:25:51.752 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:51.756 [KafkaServer id=0] Controlled shutdown request returned successfully after 9ms
2025-01-01 00:25:51.758 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:25:51.759 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:25:51.759 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:25:51.759 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:25:51.761 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:25:51.761 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:25:51.762 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cancelled in-flight FETCH request with correlation id 22 due to node 0 being disconnected (elapsed time since creation: 279ms, elapsed time since send: 279ms, request timeout: 30000ms)
2025-01-01 00:25:51.762 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Error sending fetch request (sessionId=1848900257, epoch=6) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
2025-01-01 00:25:51.762 [Producer clientId=producer-1] Node -1 disconnected.
2025-01-01 00:25:51.762 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 2147483647 disconnected.
2025-01-01 00:25:51.762 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:25:51.762 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Group coordinator localhost:60383 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2025-01-01 00:25:51.764 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:25:51.764 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:25:51.765 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:25:51.766 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:25:51.768 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:25:51.768 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:25:51.770 [KafkaApi-0] Shutdown complete.
2025-01-01 00:25:51.770 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:25:51.770 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:25:51.770 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:25:51.771 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:25:51.772 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:25:51.772 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:25:51.772 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:25:51.772 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:25:51.773 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:25:51.773 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:25:51.773 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:25:51.774 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:25:51.774 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:25:51.774 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:25:51.774 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:25:51.774 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:25:51.775 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:25:51.775 [ReplicaManager broker=0] Shutting down
2025-01-01 00:25:51.775 [LogDirFailureHandler]: Shutting down
2025-01-01 00:25:51.775 [LogDirFailureHandler]: Stopped
2025-01-01 00:25:51.775 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:25:51.776 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:25:51.776 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:25:51.776 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:25:51.776 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:25:51.776 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:25:51.776 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:25:51.776 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:25:51.777 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:25:51.777 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:25:51.777 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:25:51.777 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:25:51.779 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:25:51.779 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:25:51.779 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:25:51.779 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:25:51.779 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:25:51.785 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:25:51.785 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:25:51.787 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:25:51.787 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:25:51.788 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:25:51.788 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:25:51.789 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:25:51.789 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:25:51.789 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:25:51.789 Shutting down.
2025-01-01 00:25:51.790 Shutting down the log cleaner.
2025-01-01 00:25:51.791 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:25:51.792 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:25:51.792 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:25:51.820 [ProducerStateManager partition=pong-message-topic-0] Wrote producer snapshot at offset 3 with 0 producer ids in 3 ms.
2025-01-01 00:25:51.831 [ProducerStateManager partition=__consumer_offsets-2] Wrote producer snapshot at offset 6 with 0 producer ids in 1 ms.
2025-01-01 00:25:51.844 Shutdown complete.
2025-01-01 00:25:51.844 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:25:51.844 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:25:51.844 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:25:51.851 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:25:51.851 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:25:51.851 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:25:51.853 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:25:51.853 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:25:51.853 [Controller id=0] Resigned
2025-01-01 00:25:51.854 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:25:51.854 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:25:51.854 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:25:51.854 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:25:51.863 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:25:51.863 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60383) could not be established. Broker may not be available.
2025-01-01 00:25:51.863 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:25:51.863 [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:60383) could not be established. Broker may not be available.
2025-01-01 00:25:51.960 Session: 0x10018248fbf0000 closed
2025-01-01 00:25:51.960 EventThread shut down for session: 0x10018248fbf0000
2025-01-01 00:25:51.960 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:25:51.960 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:25:51.962 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:25:51.962 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:25:51.962 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:25:51.962 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:25:51.962 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:25:51.962 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:25:51.962 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:25:51.962 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:25:51.962 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:25:51.962 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:25:51.962 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:25:51.963 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:25:51.963 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:25:51.963 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:25:51.963 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60383) could not be established. Broker may not be available.
2025-01-01 00:25:51.963 [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:60383) could not be established. Broker may not be available.
2025-01-01 00:25:51.977 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:25:51.977 Metrics scheduler closed
2025-01-01 00:25:51.977 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:25:51.977 Metrics reporters closed
2025-01-01 00:25:51.978 Broker and topic stats closed
2025-01-01 00:25:51.978 App info kafka.server for 0 unregistered
2025-01-01 00:25:51.978 [KafkaServer id=0] shut down completed
2025-01-01 00:25:51.991 ConnnectionExpirerThread interrupted
2025-01-01 00:25:51.991 selector thread exitted run method
2025-01-01 00:25:51.991 selector thread exitted run method
2025-01-01 00:25:51.993 accept thread exitted run method
2025-01-01 00:25:51.993 shutting down
2025-01-01 00:25:51.993 Shutting down
2025-01-01 00:25:51.993 Draining request throttler queue
2025-01-01 00:25:51.993 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:25:51.993 Shutting down
2025-01-01 00:25:51.993 Shutting down
2025-01-01 00:25:51.993 Shutting down
2025-01-01 00:25:51.993 PrepRequestProcessor exited loop!
2025-01-01 00:25:51.993 SyncRequestProcessor exited!
2025-01-01 00:25:51.993 shutdown of request processor complete
2025-01-01 00:25:52.014 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Revoke previously assigned partitions pong-message-topic-0, pong-message-topic-1
2025-01-01 00:25:52.014 pong-service: partitions revoked: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:25:52.014 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:25:52.014 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:25:52.014 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:25:52.014 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:25:52.014 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:25:52.016 Metrics scheduler closed
2025-01-01 00:25:52.016 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:25:52.016 Metrics reporters closed
2025-01-01 00:25:52.016 App info kafka.consumer for consumer-pong-service-1 unregistered
2025-01-01 00:25:52.016 pong-service: Consumer stopped
2025-01-01 00:25:52.022 [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-01 00:25:52.022 Metrics scheduler closed
2025-01-01 00:25:52.022 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:25:52.022 Metrics reporters closed
2025-01-01 00:25:52.022 App info kafka.producer for producer-1 unregistered
2025-01-01 00:25:52.042 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:25:52.045 HikariPool-1 - Shutdown initiated...
2025-01-01 00:25:52.045 HikariPool-1 - Shutdown completed.
2025-01-01 00:25:52.299 SessionTrackerImpl exited loop!
2025-01-01 00:25:54.065 [KafkaServer id=0] shutting down
2025-01-01 00:25:54.065 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:25:54.065 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:25:54.065 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:25:54.065 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:25:54.068 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:25:54.069 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:25:54.069 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:25:54.069 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:25:54.070 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:25:54.070 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:25:54.070 [KafkaApi-0] Shutdown complete.
2025-01-01 00:25:54.070 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:25:54.070 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:25:54.070 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:25:54.070 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:25:54.070 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:25:54.070 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:25:54.070 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:25:54.070 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:25:54.071 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:25:54.071 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:25:54.071 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:25:54.071 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:25:54.071 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:25:54.071 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:25:54.072 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:25:54.072 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:25:54.072 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:25:54.072 [ReplicaManager broker=0] Shutting down
2025-01-01 00:25:54.072 [LogDirFailureHandler]: Shutting down
2025-01-01 00:25:54.072 [LogDirFailureHandler]: Stopped
2025-01-01 00:25:54.072 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:25:54.072 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:25:54.072 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:25:54.072 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:25:54.072 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:25:54.072 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:25:54.072 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:25:54.072 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:25:54.072 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:25:54.073 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:25:54.073 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:25:54.073 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:25:54.073 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:25:54.073 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:25:54.073 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:25:54.073 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:25:54.073 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:25:54.079 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:25:54.079 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:25:54.079 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:25:54.079 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:25:54.079 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:25:54.079 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:25:54.081 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:25:54.081 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:25:54.081 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:25:54.081 Shutting down.
2025-01-01 00:25:54.081 Shutting down the log cleaner.
2025-01-01 00:25:54.081 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:25:54.081 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:25:54.081 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:25:54.088 Shutdown complete.
2025-01-01 00:25:54.088 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:25:54.088 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:25:54.088 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:25:54.088 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:25:54.088 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:25:54.088 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:25:54.089 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:25:54.089 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:25:54.089 [Controller id=0] Resigned
2025-01-01 00:25:54.089 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:25:54.089 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:25:54.089 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:25:54.090 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:25:54.193 Session: 0x1001824adfa0000 closed
2025-01-01 00:25:54.193 EventThread shut down for session: 0x1001824adfa0000
2025-01-01 00:25:54.193 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:25:54.193 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:25:54.193 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:25:54.193 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:25:54.193 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:25:54.193 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:25:54.193 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:25:54.193 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:25:54.193 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:25:54.193 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:25:54.193 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:25:54.194 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:25:54.194 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:25:54.194 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:25:54.201 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:25:54.201 Metrics scheduler closed
2025-01-01 00:25:54.201 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:25:54.201 Metrics reporters closed
2025-01-01 00:25:54.201 Broker and topic stats closed
2025-01-01 00:25:54.201 App info kafka.server for 0 unregistered
2025-01-01 00:25:54.201 [KafkaServer id=0] shut down completed
2025-01-01 00:25:54.205 ConnnectionExpirerThread interrupted
2025-01-01 00:25:54.206 accept thread exitted run method
2025-01-01 00:25:54.206 selector thread exitted run method
2025-01-01 00:25:54.206 selector thread exitted run method
2025-01-01 00:25:54.207 shutting down
2025-01-01 00:25:54.207 Shutting down
2025-01-01 00:25:54.207 Draining request throttler queue
2025-01-01 00:25:54.207 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:25:54.207 Shutting down
2025-01-01 00:25:54.207 Shutting down
2025-01-01 00:25:54.207 Shutting down
2025-01-01 00:25:54.207 PrepRequestProcessor exited loop!
2025-01-01 00:25:54.207 SyncRequestProcessor exited!
2025-01-01 00:25:54.207 shutdown of request processor complete
2025-01-01 00:25:54.479 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/work/apache-maven-3.8.8/repo/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/work/apache-maven-3.8.8/repo/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:25:54.480 Starting MessageRepositorySpec using Java 17.0.12 with PID 24192 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:25:54.481 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:25:54.575 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:25:54.575 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:25:54.586 Finished Spring Data repository scanning in 10 ms. Found 1 JPA repository interfaces.
2025-01-01 00:25:54.588 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:25:54.588 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:25:54.592 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:25:54.592 Finished Spring Data repository scanning in 3 ms. Found 0 Redis repository interfaces.
2025-01-01 00:25:54.656 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:25:54.662 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:25:54.662 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:25:54.662 HikariPool-2 - Starting...
2025-01-01 00:25:54.664 HikariPool-2 - Added connection conn10: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:25:54.664 HikariPool-2 - Start completed.
2025-01-01 00:25:54.670 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:25:54.689 SessionTrackerImpl exited loop!
2025-01-01 00:25:54.689 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:25:54.705 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:25:54.866 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60383]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:25:54.866 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:25:54.866 Kafka version: 3.4.1
2025-01-01 00:25:54.866 Kafka commitId: 8a516edc2755df89
2025-01-01 00:25:54.866 Kafka startTimeMs: 1735662354866
2025-01-01 00:25:54.866 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:25:54.872 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:25:54.872 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Connection to node -1 (/127.0.0.1:60383) could not be established. Broker may not be available.
2025-01-01 00:25:54.872 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Bootstrap broker 127.0.0.1:60383 (id: -1 rack: null) disconnected
2025-01-01 00:25:54.872 Started MessageRepositorySpec in 0.419 seconds (process running for 25.898)
2025-01-01 00:25:54.895 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:25:54.895 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:25:54.895 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:25:54.895 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:25:54.895 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:25:54.895 Metrics scheduler closed
2025-01-01 00:25:54.895 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:25:54.895 Metrics reporters closed
2025-01-01 00:25:54.895 App info kafka.consumer for consumer-pong-service-2 unregistered
2025-01-01 00:25:54.895 pong-service: Consumer stopped
2025-01-01 00:25:54.904 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:25:54.907 HikariPool-2 - Shutdown initiated...
2025-01-01 00:25:54.920 HikariPool-2 - Shutdown completed.
2025-01-01 00:25:54.934 zookeeper.snapshot.trust.empty : false
2025-01-01 00:25:54.936 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:25:54.936 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:25:54.936 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:25:54.936 zookeeper.commitLogCount=500
2025-01-01 00:25:54.936 minSessionTimeout set to 1600
2025-01-01 00:25:54.936 maxSessionTimeout set to 16000
2025-01-01 00:25:54.936 Response cache size is initialized with value 400.
2025-01-01 00:25:54.936 Response cache size is initialized with value 400.
2025-01-01 00:25:54.936 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:25:54.936 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:25:54.936 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:25:54.936 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:25:54.936 zookeeper.pathStats.delay = 5
2025-01-01 00:25:54.936 zookeeper.pathStats.enabled = false
2025-01-01 00:25:54.936 The max bytes for all large requests are set to 104857600
2025-01-01 00:25:54.936 The large request threshold is set to -1
2025-01-01 00:25:54.936 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-18374300978517809364\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-8218654999436376451\version-2
2025-01-01 00:25:54.936 maxCnxns is not configured, using default value 0.
2025-01-01 00:25:54.936 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:25:54.936 binding to port /127.0.0.1:0
2025-01-01 00:25:54.941 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-8218654999436376451\version-2\snapshot.0
2025-01-01 00:25:54.941 Snapshot loaded in 1 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:25:54.941 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-8218654999436376451\version-2\snapshot.0
2025-01-01 00:25:54.942 Snapshot taken in 1 ms
2025-01-01 00:25:54.942 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:25:54.944 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60410
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:25:54.947 starting
2025-01-01 00:25:54.947 Connecting to zookeeper on 127.0.0.1:60410
2025-01-01 00:25:54.947 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60410.
2025-01-01 00:25:54.947 Initiating client connection, connectString=127.0.0.1:60410 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@3d4ff6a5
2025-01-01 00:25:54.949 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:25:54.949 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:25:54.949 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:25:54.949 Opening socket connection to server /127.0.0.1:60410.
2025-01-01 00:25:54.950 Socket connection established, initiating session, client: /127.0.0.1:60411, server: /127.0.0.1:60410
2025-01-01 00:25:54.950 Creating new log file: log.1
2025-01-01 00:25:54.955 Session establishment complete on server /127.0.0.1:60410, session id = 0x1001824d6be0000, negotiated timeout = 16000
2025-01-01 00:25:54.955 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:25:54.983 Cluster ID = wpEzaX0aS7mjEsbm9SuUlA
2025-01-01 00:25:54.983 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711\meta.properties
2025-01-01 00:25:54.991 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60410
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:25:54.994 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:25:54.994 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:25:54.994 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:25:54.994 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:25:54.998 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711)
2025-01-01 00:25:54.998 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711 since no clean shutdown file was found
2025-01-01 00:25:54.999 Loaded 0 logs in 1ms.
2025-01-01 00:25:54.999 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:25:54.999 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:25:55.000 Starting the log cleaner
2025-01-01 00:25:55.001 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:25:55.001 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:25:55.002 Feature ZK node at path: /feature does not exist
2025-01-01 00:25:55.005 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:25:55.013 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:25:55.016 Awaiting socket connections on localhost:60412.
2025-01-01 00:25:55.022 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:25:55.023 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:25:55.024 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:25:55.024 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:25:55.024 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:25:55.025 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:25:55.026 [LogDirFailureHandler]: Starting
2025-01-01 00:25:55.028 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:25:55.029 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662355029,1735662355029,1,0,0,72084140538593280,204,0,25

2025-01-01 00:25:55.029 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60412, czxid (broker epoch): 25
2025-01-01 00:25:55.033 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:25:55.034 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:25:55.034 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:25:55.034 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:25:55.034 [GroupCoordinator 0]: Starting up.
2025-01-01 00:25:55.035 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:25:55.037 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:25:55.037 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:25:55.038 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:25:55.038 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:25:55.038 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:25:55.038 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:25:55.039 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:25:55.039 Feature ZK node created at path: /feature
2025-01-01 00:25:55.040 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:25:55.040 [Controller id=0] Registering handlers
2025-01-01 00:25:55.042 [/config/changes-event-process-thread]: Starting
2025-01-01 00:25:55.042 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:25:55.042 [Controller id=0] Deleting isr change notifications
2025-01-01 00:25:55.043 [Controller id=0] Initializing controller context
2025-01-01 00:25:55.044 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:25:55.044 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:25:55.044 Kafka version: 3.4.1
2025-01-01 00:25:55.044 Kafka commitId: 8a516edc2755df89
2025-01-01 00:25:55.044 Kafka startTimeMs: 1735662355044
2025-01-01 00:25:55.044 [KafkaServer id=0] started
2025-01-01 00:25:55.044 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60412]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:25:55.047 Kafka version: 3.4.1
2025-01-01 00:25:55.047 Kafka commitId: 8a516edc2755df89
2025-01-01 00:25:55.047 Kafka startTimeMs: 1735662355047
2025-01-01 00:25:55.047 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:25:55.047 [RequestSendThread controllerId=0] Starting
2025-01-01 00:25:55.047 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:25:55.047 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:25:55.047 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:25:55.049 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:25:55.049 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:25:55.049 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:25:55.049 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:25:55.049 [Controller id=0] Sending update metadata request
2025-01-01 00:25:55.049 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:25:55.050 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:25:55.050 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:25:55.050 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:25:55.050 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:25:55.050 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:25:55.050 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:25:55.050 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60412 (id: 0 rack: null) for sending state change requests
2025-01-01 00:25:55.051 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:25:55.051 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:25:55.051 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:25:55.051 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:25:55.051 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:25:55.053 [Controller id=0] Starting the controller scheduler
2025-01-01 00:25:55.105 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60412 (id: 0 rack: null)
2025-01-01 00:25:55.125 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60412 (id: 0 rack: null)
2025-01-01 00:25:55.157 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0))
2025-01-01 00:25:55.161 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(fi8MPrR4TFWHCB6vhkQQLA),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), pong-message-topic-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:25:55.163 [Controller id=0] New partition creation callback for pong-message-topic-0,pong-message-topic-1
2025-01-01 00:25:55.163 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:55.163 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:55.163 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:55.163 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:55.167 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:55.167 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:55.167 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions
2025-01-01 00:25:55.167 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 2 partitions
2025-01-01 00:25:55.168 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:55.168 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:25:55.168 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0, pong-message-topic-1)
2025-01-01 00:25:55.168 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
2025-01-01 00:25:55.175 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:55.175 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711\pong-message-topic-0 with properties {}
2025-01-01 00:25:55.175 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:25:55.175 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:25:55.175 [Broker id=0] Leader pong-message-topic-0 with topic id Some(fi8MPrR4TFWHCB6vhkQQLA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:55.184 [LogLoader partition=pong-message-topic-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:55.185 Created log for partition pong-message-topic-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711\pong-message-topic-1 with properties {}
2025-01-01 00:25:55.185 [Partition pong-message-topic-1 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-1
2025-01-01 00:25:55.185 [Partition pong-message-topic-1 broker=0] Log loaded for partition pong-message-topic-1 with initial high watermark 0
2025-01-01 00:25:55.185 [Broker id=0] Leader pong-message-topic-1 with topic id Some(fi8MPrR4TFWHCB6vhkQQLA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:55.189 [Broker id=0] Finished LeaderAndIsr request in 21ms correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:25:55.190 [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:25:55.192 App info kafka.admin.client for adminclient-3 unregistered
2025-01-01 00:25:55.192 Metrics scheduler closed
2025-01-01 00:25:55.192 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:25:55.192 Metrics reporters closed
2025-01-01 00:25:55.219 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/work/apache-maven-3.8.8/repo/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/work/apache-maven-3.8.8/repo/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:25:55.221 Starting KafkaSpec using Java 17.0.12 with PID 24192 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:25:55.221 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:25:55.319 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:25:55.319 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:25:55.328 Finished Spring Data repository scanning in 9 ms. Found 1 JPA repository interfaces.
2025-01-01 00:25:55.331 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:25:55.331 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:25:55.334 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:25:55.334 Finished Spring Data repository scanning in 2 ms. Found 0 Redis repository interfaces.
2025-01-01 00:25:55.380 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:25:55.386 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:25:55.388 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:25:55.388 HikariPool-3 - Starting...
2025-01-01 00:25:55.390 HikariPool-3 - Added connection conn20: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:25:55.390 HikariPool-3 - Start completed.
2025-01-01 00:25:55.397 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:25:55.414 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:25:55.426 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:25:55.553 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60412]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:25:55.559 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:25:55.559 Kafka version: 3.4.1
2025-01-01 00:25:55.559 Kafka commitId: 8a516edc2755df89
2025-01-01 00:25:55.559 Kafka startTimeMs: 1735662355559
2025-01-01 00:25:55.559 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:25:55.563 Started KafkaSpec in 0.367 seconds (process running for 26.586)
2025-01-01 00:25:55.564 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to fi8MPrR4TFWHCB6vhkQQLA
2025-01-01 00:25:55.564 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to fi8MPrR4TFWHCB6vhkQQLA
2025-01-01 00:25:55.564 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cluster ID: wpEzaX0aS7mjEsbm9SuUlA
2025-01-01 00:25:55.567 Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-01-01 00:25:55.572 [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(FVFR7Rh3RUmuZO9XqlM9qQ),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:25:55.574 [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-01-01 00:25:55.574 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:55.574 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:55.574 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:55.574 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:55.574 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:25:55.574 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:55.574 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:55.574 Idempotence will be disabled because acks is set to 0, not set to 'all'.
2025-01-01 00:25:55.574 ProducerConfig values: 
	acks = 0
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:60412]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-01-01 00:25:55.577 Kafka version: 3.4.1
2025-01-01 00:25:55.578 Kafka commitId: 8a516edc2755df89
2025-01-01 00:25:55.578 Kafka startTimeMs: 1735662355577
2025-01-01 00:25:55.580 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:55.581 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:55.581 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:55.581 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:55.581 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:25:55.581 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-01-01 00:25:55.581 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-01-01 00:25:55.581 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:25:55.581 [Producer clientId=producer-2] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to fi8MPrR4TFWHCB6vhkQQLA
2025-01-01 00:25:55.581 [Producer clientId=producer-2] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to fi8MPrR4TFWHCB6vhkQQLA
2025-01-01 00:25:55.581 [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:25:55.582 [Producer clientId=producer-2] Cluster ID: wpEzaX0aS7mjEsbm9SuUlA
2025-01-01 00:25:55.582 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-01-01 00:25:55.582 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-01-01 00:25:55.591 [LogLoader partition=__consumer_offsets-3, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:55.592 Created log for partition __consumer_offsets-3 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711\__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:25:55.594 [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-01-01 00:25:55.594 [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-01-01 00:25:55.594 [Broker id=0] Leader __consumer_offsets-3 with topic id Some(FVFR7Rh3RUmuZO9XqlM9qQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:55.604 [LogLoader partition=__consumer_offsets-2, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:55.604 Created log for partition __consumer_offsets-2 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711\__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:25:55.605 [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-01-01 00:25:55.605 [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-01-01 00:25:55.605 [Broker id=0] Leader __consumer_offsets-2 with topic id Some(FVFR7Rh3RUmuZO9XqlM9qQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:55.613 [LogLoader partition=__consumer_offsets-4, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:55.613 Created log for partition __consumer_offsets-4 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711\__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:25:55.613 [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-01-01 00:25:55.613 [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-01-01 00:25:55.613 [Broker id=0] Leader __consumer_offsets-4 with topic id Some(FVFR7Rh3RUmuZO9XqlM9qQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:55.622 [LogLoader partition=__consumer_offsets-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:55.623 Created log for partition __consumer_offsets-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711\__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:25:55.623 [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-01-01 00:25:55.623 [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-01-01 00:25:55.623 [Broker id=0] Leader __consumer_offsets-1 with topic id Some(FVFR7Rh3RUmuZO9XqlM9qQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:55.632 [LogLoader partition=__consumer_offsets-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711] Loading producer state till offset 0 with message format version 2
2025-01-01 00:25:55.633 Created log for partition __consumer_offsets-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e86ab37a-9831-4250-a14c-21575c60eaa92264715757480344711\__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:25:55.633 [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-01-01 00:25:55.633 [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-01-01 00:25:55.633 [Broker id=0] Leader __consumer_offsets-0 with topic id Some(FVFR7Rh3RUmuZO9XqlM9qQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:25:55.635 [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-01-01 00:25:55.636 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-01-01 00:25:55.636 [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-01-01 00:25:55.636 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-01-01 00:25:55.636 [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-01-01 00:25:55.636 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-01-01 00:25:55.636 [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-01-01 00:25:55.636 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-01-01 00:25:55.636 [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-01-01 00:25:55.636 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-01-01 00:25:55.636 [Broker id=0] Finished LeaderAndIsr request in 55ms correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:25:55.636 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:25:55.636 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:25:55.636 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:25:55.636 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:25:55.636 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:25:55.637 [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-01-01 00:25:55.670 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Discovered group coordinator localhost:60412 (id: 2147483647 rack: null)
2025-01-01 00:25:55.671 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] (Re-)joining group
2025-01-01 00:25:55.673 [GroupCoordinator 0]: Dynamic member with unknown member id joins group pong-service in Empty state. Created a new member id consumer-pong-service-3-c2a1eca4-db70-45c4-8214-697f82d08467 and request the member to rejoin with this id.
2025-01-01 00:25:55.674 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: need to re-join with the given member-id: consumer-pong-service-3-c2a1eca4-db70-45c4-8214-697f82d08467
2025-01-01 00:25:55.674 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2025-01-01 00:25:55.674 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] (Re-)joining group
2025-01-01 00:25:55.676 [GroupCoordinator 0]: Preparing to rebalance group pong-service in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-pong-service-3-c2a1eca4-db70-45c4-8214-697f82d08467 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)
2025-01-01 00:25:55.676 [GroupCoordinator 0]: Stabilized group pong-service generation 1 (__consumer_offsets-2) with 1 members
2025-01-01 00:25:55.677 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Successfully joined group with generation Generation{generationId=1, memberId='consumer-pong-service-3-c2a1eca4-db70-45c4-8214-697f82d08467', protocol='range'}
2025-01-01 00:25:55.677 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Finished assignment for group at generation 1: {consumer-pong-service-3-c2a1eca4-db70-45c4-8214-697f82d08467=Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])}
2025-01-01 00:25:55.678 [GroupCoordinator 0]: Assignment received from leader consumer-pong-service-3-c2a1eca4-db70-45c4-8214-697f82d08467 for group pong-service for generation 1. The group has 1 members, 0 of which are static.
2025-01-01 00:25:55.680 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Successfully synced group in generation Generation{generationId=1, memberId='consumer-pong-service-3-c2a1eca4-db70-45c4-8214-697f82d08467', protocol='range'}
2025-01-01 00:25:55.680 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Notifying assignor about the new Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])
2025-01-01 00:25:55.680 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Adding newly assigned partitions: pong-message-topic-0, pong-message-topic-1
2025-01-01 00:25:55.682 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:25:55.682 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:25:55.683 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:25:55.683 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:25:55.684 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting offset for partition pong-message-topic-0 to position FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60412 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:25:55.684 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting offset for partition pong-message-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60412 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:25:55.688 pong-service: partitions assigned: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:26:00.053 [Controller id=0] Processing automatic preferred replica leader election
2025-01-01 00:26:00.747 [KafkaServer id=0] shutting down
2025-01-01 00:26:00.747 [KafkaServer id=0] Starting controlled shutdown
2025-01-01 00:26:00.751 [Controller id=0] Shutting down broker 0
2025-01-01 00:26:00.752 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:26:00.753 [KafkaServer id=0] Controlled shutdown request returned successfully after 1ms
2025-01-01 00:26:00.753 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:26:00.753 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:26:00.753 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:26:00.753 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:26:00.754 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 2147483647 disconnected.
2025-01-01 00:26:00.754 [Producer clientId=producer-2] Node 0 disconnected.
2025-01-01 00:26:00.754 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Group coordinator localhost:60412 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2025-01-01 00:26:00.755 [Producer clientId=producer-2] Node -1 disconnected.
2025-01-01 00:26:00.755 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:26:00.755 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cancelled in-flight FETCH request with correlation id 25 due to node 0 being disconnected (elapsed time since creation: 34ms, elapsed time since send: 34ms, request timeout: 30000ms)
2025-01-01 00:26:00.755 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cancelled in-flight METADATA request with correlation id 27 due to node 0 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, request timeout: 30000ms)
2025-01-01 00:26:00.755 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:26:00.755 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Error sending fetch request (sessionId=202082109, epoch=10) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
2025-01-01 00:26:00.755 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:26:00.755 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:26:00.755 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:26:00.755 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:26:00.757 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:26:00.757 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:26:00.757 [KafkaApi-0] Shutdown complete.
2025-01-01 00:26:00.757 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:26:00.757 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:26:00.757 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:26:00.757 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:26:00.757 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:26:00.757 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:26:00.757 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:26:00.757 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:26:00.757 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:26:00.757 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:26:00.757 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:26:00.758 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:26:00.758 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:26:00.758 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:26:00.758 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:26:00.758 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:26:00.758 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:26:00.758 [ReplicaManager broker=0] Shutting down
2025-01-01 00:26:00.758 [LogDirFailureHandler]: Shutting down
2025-01-01 00:26:00.758 [LogDirFailureHandler]: Stopped
2025-01-01 00:26:00.758 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:26:00.758 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:26:00.758 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:26:00.758 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:26:00.758 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:26:00.758 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:26:00.758 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:26:00.758 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:26:00.758 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:26:00.759 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:26:00.759 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:26:00.759 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:26:00.759 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:26:00.759 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:26:00.759 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:26:00.759 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:26:00.759 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:26:00.764 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:26:00.764 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:26:00.764 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:26:00.764 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:26:00.766 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:26:00.766 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:26:00.766 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:26:00.766 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:26:00.766 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:26:00.766 Shutting down.
2025-01-01 00:26:00.766 Shutting down the log cleaner.
2025-01-01 00:26:00.766 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:26:00.766 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:26:00.766 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:26:00.778 [ProducerStateManager partition=pong-message-topic-0] Wrote producer snapshot at offset 1 with 0 producer ids in 2 ms.
2025-01-01 00:26:00.790 [ProducerStateManager partition=__consumer_offsets-2] Wrote producer snapshot at offset 3 with 0 producer ids in 2 ms.
2025-01-01 00:26:00.798 Shutdown complete.
2025-01-01 00:26:00.798 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:26:00.798 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:26:00.798 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:26:00.798 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:26:00.798 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:26:00.798 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:26:00.798 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:26:00.798 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:26:00.799 [Controller id=0] Resigned
2025-01-01 00:26:00.799 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:26:00.799 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:26:00.799 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:26:00.799 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:26:00.854 [Producer clientId=producer-2] Node 0 disconnected.
2025-01-01 00:26:00.855 [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:60412) could not be established. Broker may not be available.
2025-01-01 00:26:00.855 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:26:00.855 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60412) could not be established. Broker may not be available.
2025-01-01 00:26:00.901 Session: 0x1001824d6be0000 closed
2025-01-01 00:26:00.901 EventThread shut down for session: 0x1001824d6be0000
2025-01-01 00:26:00.901 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:26:00.901 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:26:00.901 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:26:00.901 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:26:00.901 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:26:00.902 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:26:00.902 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:26:00.902 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:26:00.902 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:26:00.902 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:26:00.902 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:26:00.902 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:26:00.902 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:26:00.902 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:26:00.907 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:26:00.907 Metrics scheduler closed
2025-01-01 00:26:00.907 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:26:00.907 Metrics reporters closed
2025-01-01 00:26:00.909 Broker and topic stats closed
2025-01-01 00:26:00.909 App info kafka.server for 0 unregistered
2025-01-01 00:26:00.909 [KafkaServer id=0] shut down completed
2025-01-01 00:26:00.917 ConnnectionExpirerThread interrupted
2025-01-01 00:26:00.917 accept thread exitted run method
2025-01-01 00:26:00.917 selector thread exitted run method
2025-01-01 00:26:00.917 selector thread exitted run method
2025-01-01 00:26:00.918 shutting down
2025-01-01 00:26:00.918 Shutting down
2025-01-01 00:26:00.918 Draining request throttler queue
2025-01-01 00:26:00.918 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:26:00.918 Shutting down
2025-01-01 00:26:00.918 Shutting down
2025-01-01 00:26:00.918 Shutting down
2025-01-01 00:26:00.918 PrepRequestProcessor exited loop!
2025-01-01 00:26:00.918 SyncRequestProcessor exited!
2025-01-01 00:26:00.918 shutdown of request processor complete
2025-01-01 00:26:00.934 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Revoke previously assigned partitions pong-message-topic-0, pong-message-topic-1
2025-01-01 00:26:00.934 pong-service: partitions revoked: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:26:00.934 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:26:00.934 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:26:00.934 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:26:00.934 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:26:00.934 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:26:00.934 Metrics scheduler closed
2025-01-01 00:26:00.934 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:26:00.934 Metrics reporters closed
2025-01-01 00:26:00.934 App info kafka.consumer for consumer-pong-service-3 unregistered
2025-01-01 00:26:00.934 pong-service: Consumer stopped
2025-01-01 00:26:00.934 [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-01 00:26:00.936 Metrics scheduler closed
2025-01-01 00:26:00.936 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:26:00.936 Metrics reporters closed
2025-01-01 00:26:00.936 App info kafka.producer for producer-2 unregistered
2025-01-01 00:26:00.936 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:26:00.936 HikariPool-3 - Shutdown initiated...
2025-01-01 00:26:00.936 HikariPool-3 - Shutdown completed.
2025-01-01 00:30:17.360 Could not detect default configuration classes for test class [com.demo.controller.PongControllerSpec]: PongControllerSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:30:17.497 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.controller.PongControllerSpec
2025-01-01 00:30:17.650 Could not detect default configuration classes for test class [com.demo.dao.MessageRepositorySpec]: MessageRepositorySpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:30:17.659 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.dao.MessageRepositorySpec
2025-01-01 00:30:17.677 Could not detect default configuration classes for test class [com.demo.kafka.KafkaSpec]: KafkaSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:30:17.689 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.kafka.KafkaSpec
2025-01-01 00:30:17.717 Could not detect default configuration classes for test class [com.demo.controller.PongControllerSpec]: PongControllerSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:30:17.718 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.controller.PongControllerSpec
2025-01-01 00:30:17.723 Could not detect default configuration classes for test class [com.demo.dao.MessageRepositorySpec]: MessageRepositorySpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:30:17.723 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.dao.MessageRepositorySpec
2025-01-01 00:30:17.728 Could not detect default configuration classes for test class [com.demo.kafka.KafkaSpec]: KafkaSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:30:17.728 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.kafka.KafkaSpec
2025-01-01 00:30:17.996 Registered kafka:type=kafka.Log4jController MBean
2025-01-01 00:30:18.061 
2025-01-01 00:30:18.061   ______                  _                                          
2025-01-01 00:30:18.061  |___  /                 | |                                         
2025-01-01 00:30:18.061     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __   
2025-01-01 00:30:18.061    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__|
2025-01-01 00:30:18.061   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |    
2025-01-01 00:30:18.061  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_|
2025-01-01 00:30:18.061                                               | |                     
2025-01-01 00:30:18.061                                               |_|                     
2025-01-01 00:30:18.061 
2025-01-01 00:30:23.288 Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:30:23.288 Server environment:host.name=Joe
2025-01-01 00:30:23.288 Server environment:java.version=17.0.12
2025-01-01 00:30:23.288 Server environment:java.vendor=Oracle Corporation
2025-01-01 00:30:23.288 Server environment:java.home=C:\work\jdk-17
2025-01-01 00:30:23.288 Server environment:java.class.path=C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\work\apache-maven-3.8.8\repo\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\work\apache-maven-3.8.8\repo\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\guava\guava\19.0\guava-19.0.jar;C:\work\apache-maven-3.8.8\repo\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\work\apache-maven-3.8.8\repo\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\work\apache-maven-3.8.8\repo\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\work\apache-maven-3.8.8\repo\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\work\apache-maven-3.8.8\repo\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\work\apache-maven-3.8.8\repo\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\work\apache-maven-3.8.8\repo\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\work\apache-maven-3.8.8\repo\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\work\apache-maven-3.8.8\repo\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\jline\jline\2.14.6\jline-2.14.6.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\junit\junit\4.13.2\junit-4.13.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\work\apache-maven-3.8.8\repo\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\work\apache-maven-3.8.8\repo\org\json\json\20220320\json-20220320.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\work\apache-maven-3.8.8\repo\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\work\apache-maven-3.8.8\repo\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\work\apache-maven-3.8.8\repo\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar;
2025-01-01 00:30:23.289 Server environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:30:23.289 Server environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:30:23.289 Server environment:java.compiler=<NA>
2025-01-01 00:30:23.289 Server environment:os.name=Windows 11
2025-01-01 00:30:23.292 Server environment:os.arch=amd64
2025-01-01 00:30:23.292 Server environment:os.version=10.0
2025-01-01 00:30:23.292 Server environment:user.name=joech
2025-01-01 00:30:23.292 Server environment:user.home=C:\Users\joech
2025-01-01 00:30:23.292 Server environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:30:23.292 Server environment:os.memory.free=97MB
2025-01-01 00:30:23.292 Server environment:os.memory.max=15816MB
2025-01-01 00:30:23.292 Server environment:os.memory.total=136MB
2025-01-01 00:30:23.292 zookeeper.enableEagerACLCheck = false
2025-01-01 00:30:23.292 zookeeper.digest.enabled = true
2025-01-01 00:30:23.292 zookeeper.closeSessionTxn.enabled = true
2025-01-01 00:30:23.292 zookeeper.flushDelay=0
2025-01-01 00:30:23.292 zookeeper.maxWriteQueuePollTime=0
2025-01-01 00:30:23.292 zookeeper.maxBatchSize=1000
2025-01-01 00:30:23.292 zookeeper.intBufferStartingSizeBytes = 1024
2025-01-01 00:30:23.292 zookeeper.snapshot.trust.empty : false
2025-01-01 00:30:23.306 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:30:23.306 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:30:23.306 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:30:23.306 zookeeper.commitLogCount=500
2025-01-01 00:30:23.306 Weighed connection throttling is disabled
2025-01-01 00:30:23.306 minSessionTimeout set to 1600
2025-01-01 00:30:23.306 maxSessionTimeout set to 16000
2025-01-01 00:30:23.306 Response cache size is initialized with value 400.
2025-01-01 00:30:23.306 Response cache size is initialized with value 400.
2025-01-01 00:30:23.306 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:30:23.306 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:30:23.306 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:30:23.306 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:30:23.306 zookeeper.pathStats.delay = 5
2025-01-01 00:30:23.306 zookeeper.pathStats.enabled = false
2025-01-01 00:30:23.318 The max bytes for all large requests are set to 104857600
2025-01-01 00:30:23.318 The large request threshold is set to -1
2025-01-01 00:30:23.318 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-15468896269438668906\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-3560299824287715618\version-2
2025-01-01 00:30:23.338 maxCnxns is not configured, using default value 0.
2025-01-01 00:30:23.349 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:30:23.351 binding to port /127.0.0.1:0
2025-01-01 00:30:23.367 zookeeper.snapshot.compression.method = CHECKED
2025-01-01 00:30:23.367 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-3560299824287715618\version-2\snapshot.0
2025-01-01 00:30:23.371 Snapshot loaded in 14 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:30:23.371 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-3560299824287715618\version-2\snapshot.0
2025-01-01 00:30:23.372 Snapshot taken in 1 ms
2025-01-01 00:30:23.380 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:30:23.381 zookeeper.request_throttler.shutdownTimeout = 10000
2025-01-01 00:30:23.580 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60475
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:30:23.602 Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2025-01-01 00:30:23.655 starting
2025-01-01 00:30:23.656 Connecting to zookeeper on 127.0.0.1:60475
2025-01-01 00:30:23.673 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60475.
2025-01-01 00:30:28.941 Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:30:28.941 Client environment:host.name=Joe
2025-01-01 00:30:28.941 Client environment:java.version=17.0.12
2025-01-01 00:30:28.941 Client environment:java.vendor=Oracle Corporation
2025-01-01 00:30:28.942 Client environment:java.home=C:\work\jdk-17
2025-01-01 00:30:28.942 Client environment:java.class.path=C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\work\apache-maven-3.8.8\repo\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\work\apache-maven-3.8.8\repo\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\guava\guava\19.0\guava-19.0.jar;C:\work\apache-maven-3.8.8\repo\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\work\apache-maven-3.8.8\repo\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\work\apache-maven-3.8.8\repo\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\work\apache-maven-3.8.8\repo\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\work\apache-maven-3.8.8\repo\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\work\apache-maven-3.8.8\repo\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\work\apache-maven-3.8.8\repo\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\work\apache-maven-3.8.8\repo\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\work\apache-maven-3.8.8\repo\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\jline\jline\2.14.6\jline-2.14.6.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\junit\junit\4.13.2\junit-4.13.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\work\apache-maven-3.8.8\repo\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\work\apache-maven-3.8.8\repo\org\json\json\20220320\json-20220320.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\work\apache-maven-3.8.8\repo\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\work\apache-maven-3.8.8\repo\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\work\apache-maven-3.8.8\repo\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar;
2025-01-01 00:30:28.945 Client environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:30:28.945 Client environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:30:28.945 Client environment:java.compiler=<NA>
2025-01-01 00:30:28.945 Client environment:os.name=Windows 11
2025-01-01 00:30:28.945 Client environment:os.arch=amd64
2025-01-01 00:30:28.945 Client environment:os.version=10.0
2025-01-01 00:30:28.945 Client environment:user.name=joech
2025-01-01 00:30:28.945 Client environment:user.home=C:\Users\joech
2025-01-01 00:30:28.945 Client environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:30:28.945 Client environment:os.memory.free=101MB
2025-01-01 00:30:28.945 Client environment:os.memory.max=15816MB
2025-01-01 00:30:28.945 Client environment:os.memory.total=136MB
2025-01-01 00:30:28.949 Initiating client connection, connectString=127.0.0.1:60475 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@299003eb
2025-01-01 00:30:28.952 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:30:28.958 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:30:28.959 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:30:28.959 Opening socket connection to server /127.0.0.1:60475.
2025-01-01 00:30:28.961 Socket connection established, initiating session, client: /127.0.0.1:60476, server: /127.0.0.1:60475
2025-01-01 00:30:28.966 Creating new log file: log.1
2025-01-01 00:30:28.974 ZooKeeper audit is disabled.
2025-01-01 00:30:28.976 Session establishment complete on server /127.0.0.1:60475, session id = 0x1001828ef4d0000, negotiated timeout = 16000
2025-01-01 00:30:28.978 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:30:29.242 Cluster ID = dXAHhMLaSmW7_KZF0kl2cA
2025-01-01 00:30:29.246 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222\meta.properties
2025-01-01 00:30:29.282 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60475
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:30:29.305 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:30:29.305 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:30:29.306 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:30:29.308 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:30:29.335 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222)
2025-01-01 00:30:29.338 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222 since no clean shutdown file was found
2025-01-01 00:30:29.346 Loaded 0 logs in 10ms.
2025-01-01 00:30:29.346 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:30:29.348 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:30:29.358 Starting the log cleaner
2025-01-01 00:30:29.367 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:30:29.379 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:30:29.387 Feature ZK node at path: /feature does not exist
2025-01-01 00:30:29.409 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:30:29.731 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:30:29.738 Awaiting socket connections on localhost:60477.
2025-01-01 00:30:29.762 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:30:29.769 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:30:29.784 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:30:29.784 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:30:29.784 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:30:29.786 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:30:29.801 [LogDirFailureHandler]: Starting
2025-01-01 00:30:29.823 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:30:29.840 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662629833,1735662629833,1,0,0,72084158130487296,204,0,25

2025-01-01 00:30:29.841 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60477, czxid (broker epoch): 25
2025-01-01 00:30:29.898 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:30:29.902 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:30:29.908 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:30:29.908 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:30:29.916 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:30:29.921 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:30:29.925 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:30:29.927 Feature ZK node created at path: /feature
2025-01-01 00:30:29.929 [GroupCoordinator 0]: Starting up.
2025-01-01 00:30:29.933 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:30:29.956 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:30:29.958 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:30:29.958 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:30:29.964 [Controller id=0] Registering handlers
2025-01-01 00:30:29.964 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:30:29.968 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:30:29.971 [Controller id=0] Deleting isr change notifications
2025-01-01 00:30:29.973 [Controller id=0] Initializing controller context
2025-01-01 00:30:29.994 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:30:30.000 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:30:30.014 [RequestSendThread controllerId=0] Starting
2025-01-01 00:30:30.017 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:30:30.017 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:30:30.018 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:30:30.018 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:30:30.021 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:30:30.022 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:30:30.022 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:30:30.022 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:30:30.024 [Controller id=0] Sending update metadata request
2025-01-01 00:30:30.028 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:30:30.041 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:30:30.041 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:30:30.042 [/config/changes-event-process-thread]: Starting
2025-01-01 00:30:30.046 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:30:30.047 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:30:30.048 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:30:30.051 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:30:30.054 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60477 (id: 0 rack: null) for sending state change requests
2025-01-01 00:30:30.056 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:30:30.059 Kafka version: 3.4.1
2025-01-01 00:30:30.059 Kafka commitId: 8a516edc2755df89
2025-01-01 00:30:30.060 Kafka startTimeMs: 1735662630057
2025-01-01 00:30:30.061 [KafkaServer id=0] started
2025-01-01 00:30:30.063 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:30:30.063 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:30:30.063 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:30:30.065 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:30:30.067 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:30:30.071 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60477]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:30:30.082 [Controller id=0] Starting the controller scheduler
2025-01-01 00:30:30.126 Kafka version: 3.4.1
2025-01-01 00:30:30.126 Kafka commitId: 8a516edc2755df89
2025-01-01 00:30:30.126 Kafka startTimeMs: 1735662630126
2025-01-01 00:30:30.173 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60477 (id: 0 rack: null)
2025-01-01 00:30:30.216 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60477 (id: 0 rack: null)
2025-01-01 00:30:30.225 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0))
2025-01-01 00:30:30.241 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(6AjmdSVgTu6Fqv-G2ntqZQ),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), pong-message-topic-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:30:30.242 [Controller id=0] New partition creation callback for pong-message-topic-0,pong-message-topic-1
2025-01-01 00:30:30.245 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:30.245 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:30.245 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:30.248 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:30.265 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:30.265 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:30.267 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions
2025-01-01 00:30:30.268 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 2 partitions
2025-01-01 00:30:30.269 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:30.274 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:30:30.301 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0, pong-message-topic-1)
2025-01-01 00:30:30.303 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
2025-01-01 00:30:30.375 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:30.391 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222\pong-message-topic-0 with properties {}
2025-01-01 00:30:30.393 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:30:30.394 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:30:30.395 [Broker id=0] Leader pong-message-topic-0 with topic id Some(6AjmdSVgTu6Fqv-G2ntqZQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:30.409 [LogLoader partition=pong-message-topic-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:30.411 Created log for partition pong-message-topic-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222\pong-message-topic-1 with properties {}
2025-01-01 00:30:30.411 [Partition pong-message-topic-1 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-1
2025-01-01 00:30:30.411 [Partition pong-message-topic-1 broker=0] Log loaded for partition pong-message-topic-1 with initial high watermark 0
2025-01-01 00:30:30.411 [Broker id=0] Leader pong-message-topic-1 with topic id Some(6AjmdSVgTu6Fqv-G2ntqZQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:30.421 [Broker id=0] Finished LeaderAndIsr request in 149ms correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:30:30.429 [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:30:30.435 App info kafka.admin.client for adminclient-1 unregistered
2025-01-01 00:30:30.439 Metrics scheduler closed
2025-01-01 00:30:30.439 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:30:30.439 Metrics reporters closed
2025-01-01 00:30:30.768 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/work/apache-maven-3.8.8/repo/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/work/apache-maven-3.8.8/repo/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:30:30.791 zookeeper.snapshot.trust.empty : false
2025-01-01 00:30:30.792 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:30:30.792 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:30:30.792 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:30:30.792 zookeeper.commitLogCount=500
2025-01-01 00:30:30.793 minSessionTimeout set to 1600
2025-01-01 00:30:30.793 maxSessionTimeout set to 16000
2025-01-01 00:30:30.793 Response cache size is initialized with value 400.
2025-01-01 00:30:30.793 Response cache size is initialized with value 400.
2025-01-01 00:30:30.793 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:30:30.793 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:30:30.793 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:30:30.793 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:30:30.793 zookeeper.pathStats.delay = 5
2025-01-01 00:30:30.793 zookeeper.pathStats.enabled = false
2025-01-01 00:30:30.793 The max bytes for all large requests are set to 104857600
2025-01-01 00:30:30.793 The large request threshold is set to -1
2025-01-01 00:30:30.793 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-7697513031289275206\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-15819290010634348896\version-2
2025-01-01 00:30:30.793 maxCnxns is not configured, using default value 0.
2025-01-01 00:30:30.793 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:30:30.797 binding to port /127.0.0.1:0
2025-01-01 00:30:30.800 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-15819290010634348896\version-2\snapshot.0
2025-01-01 00:30:30.801 Snapshot loaded in 2 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:30:30.801 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-15819290010634348896\version-2\snapshot.0
2025-01-01 00:30:30.801 Snapshot taken in 1 ms
2025-01-01 00:30:30.802 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:30:30.805 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.94454775-2bf5-4d05-b7a7-4a559e1bde436035987771263705754
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60481
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:30:30.809 starting
2025-01-01 00:30:30.809 Connecting to zookeeper on 127.0.0.1:60481
2025-01-01 00:30:30.810 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60481.
2025-01-01 00:30:30.810 Initiating client connection, connectString=127.0.0.1:60481 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@7a58b77b
2025-01-01 00:30:30.812 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:30:30.812 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:30:30.812 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:30:30.812 Opening socket connection to server /127.0.0.1:60481.
2025-01-01 00:30:30.814 Socket connection established, initiating session, client: /127.0.0.1:60482, server: /127.0.0.1:60481
2025-01-01 00:30:30.815 Creating new log file: log.1
2025-01-01 00:30:30.820 Session establishment complete on server /127.0.0.1:60481, session id = 0x10018290c520000, negotiated timeout = 16000
2025-01-01 00:30:30.820 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:30:30.860 Cluster ID = __5IJfwcSdSTkFVntMowWw
2025-01-01 00:30:30.860 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.94454775-2bf5-4d05-b7a7-4a559e1bde436035987771263705754\meta.properties
2025-01-01 00:30:30.872 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.94454775-2bf5-4d05-b7a7-4a559e1bde436035987771263705754
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60481
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:30:30.886 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:30:30.887 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:30:30.887 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:30:30.887 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:30:30.892 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.94454775-2bf5-4d05-b7a7-4a559e1bde436035987771263705754)
2025-01-01 00:30:30.892 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.94454775-2bf5-4d05-b7a7-4a559e1bde436035987771263705754 since no clean shutdown file was found
2025-01-01 00:30:30.894 Loaded 0 logs in 2ms.
2025-01-01 00:30:30.894 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:30:30.894 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:30:30.897 Starting the log cleaner
2025-01-01 00:30:30.898 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:30:30.898 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:30:30.900 Feature ZK node at path: /feature does not exist
2025-01-01 00:30:30.903 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:30:30.930 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:30:30.933 Awaiting socket connections on localhost:60483.
2025-01-01 00:30:30.943 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:30:30.945 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:30:30.947 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:30:30.947 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:30:30.948 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:30:30.948 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:30:30.950 [LogDirFailureHandler]: Starting
2025-01-01 00:30:30.954 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:30:30.956 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662630955,1735662630955,1,0,0,72084158617354240,204,0,25

2025-01-01 00:30:30.957 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60483, czxid (broker epoch): 25
2025-01-01 00:30:30.962 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:30:30.962 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:30:30.963 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:30:30.963 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:30:30.964 [GroupCoordinator 0]: Starting up.
2025-01-01 00:30:30.966 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:30:30.967 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:30:30.970 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:30:30.970 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:30:30.971 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:30:30.971 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:30:30.972 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:30:30.973 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:30:30.973 Feature ZK node created at path: /feature
2025-01-01 00:30:30.974 [Controller id=0] Registering handlers
2025-01-01 00:30:30.974 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:30:30.977 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:30:30.978 [/config/changes-event-process-thread]: Starting
2025-01-01 00:30:30.978 [Controller id=0] Deleting isr change notifications
2025-01-01 00:30:30.979 [Controller id=0] Initializing controller context
2025-01-01 00:30:30.982 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:30:30.983 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:30:30.984 Kafka version: 3.4.1
2025-01-01 00:30:30.984 Kafka commitId: 8a516edc2755df89
2025-01-01 00:30:30.984 Kafka startTimeMs: 1735662630984
2025-01-01 00:30:30.989 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:30:30.989 [RequestSendThread controllerId=0] Starting
2025-01-01 00:30:30.989 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:30:30.989 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:30:30.989 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:30:30.992 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:30:30.992 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:30:30.992 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:30:30.992 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:30:30.992 [Controller id=0] Sending update metadata request
2025-01-01 00:30:30.992 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:30:30.992 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:30:30.993 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:30:30.993 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:30:30.993 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:30:30.993 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:30:30.993 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:30:30.993 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60483 (id: 0 rack: null) for sending state change requests
2025-01-01 00:30:30.986 Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.server:type=app-info,id=0
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1865)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:960)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:895)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:523)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:563)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:188)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:182)
	at kafka.utils.TestUtils.createServer(TestUtils.scala)
	at org.springframework.kafka.test.EmbeddedKafkaBroker.afterPropertiesSet(EmbeddedKafkaBroker.java:369)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1817)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1766)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:410)
	at org.springframework.kafka.test.context.EmbeddedKafkaContextCustomizer.customizeContext(EmbeddedKafkaContextCustomizer.java:116)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextCustomizerAdapter.initialize(SpringBootContextLoader.java:435)
	at org.springframework.boot.SpringApplication.applyInitializers(SpringApplication.java:610)
	at org.springframework.boot.SpringApplication.prepareContext(SpringApplication.java:390)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:314)
	at org.springframework.boot.test.context.SpringBootContextLoader.lambda$loadContext$3(SpringBootContextLoader.java:137)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:58)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:46)
	at org.springframework.boot.SpringApplication.withHook(SpringApplication.java:1409)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextLoaderHook.run(SpringBootContextLoader.java:545)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:137)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:108)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContextInternal(DefaultCacheAwareContextLoaderDelegate.java:187)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:119)
	at org.springframework.test.context.support.DefaultTestContext.getApplicationContext(DefaultTestContext.java:127)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:141)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:97)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:241)
	at org.spockframework.spring.SpringTestContextManager.prepareTestInstance(SpringTestContextManager.java:56)
	at org.spockframework.spring.SpringInterceptor.interceptInitializerMethod(SpringInterceptor.java:46)
	at org.spockframework.runtime.extension.AbstractMethodInterceptor.intercept(AbstractMethodInterceptor.java:24)
	at org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)
	at org.spockframework.runtime.PlatformSpecRunner.runInitializer(PlatformSpecRunner.java:259)
	at org.spockframework.runtime.PlatformSpecRunner.runInitializer(PlatformSpecRunner.java:254)
	at org.spockframework.runtime.IterationNode.prepare(IterationNode.java:31)
	at org.spockframework.runtime.IterationNode.prepare(IterationNode.java:13)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$prepare$2(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.prepare(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:90)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
	at org.spockframework.runtime.ParameterizedFeatureChildExecutor.execute(ParameterizedFeatureChildExecutor.java:104)
	at org.spockframework.runtime.PlatformParameterizedSpecRunner$1.runIteration(PlatformParameterizedSpecRunner.java:72)
	at org.spockframework.runtime.extension.IDataDriver.lambda$static$0(IDataDriver.java:37)
	at org.spockframework.runtime.PlatformParameterizedSpecRunner.runParameterizedFeature(PlatformParameterizedSpecRunner.java:47)
	at org.spockframework.runtime.ParameterizedFeatureNode.execute(ParameterizedFeatureNode.java:40)
	at org.spockframework.runtime.ParameterizedFeatureNode.execute(ParameterizedFeatureNode.java:16)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)
	at org.spockframework.runtime.FeatureNode.lambda$around$0(FeatureNode.java:41)
	at org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunFeature$4(PlatformSpecRunner.java:199)
	at org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)
	at org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)
	at org.spockframework.runtime.PlatformSpecRunner.runFeature(PlatformSpecRunner.java:192)
	at org.spockframework.runtime.FeatureNode.around(FeatureNode.java:41)
	at org.spockframework.runtime.FeatureNode.around(FeatureNode.java:12)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)
	at org.spockframework.runtime.SpecNode.lambda$around$0(SpecNode.java:63)
	at org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunSpec$0(PlatformSpecRunner.java:61)
	at org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)
	at org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)
	at org.spockframework.runtime.PlatformSpecRunner.runSpec(PlatformSpecRunner.java:55)
	at org.spockframework.runtime.SpecNode.around(SpecNode.java:63)
	at org.spockframework.runtime.SpecNode.around(SpecNode.java:11)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2025-01-01 00:30:30.995 [KafkaServer id=0] started
2025-01-01 00:30:30.996 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:30:30.996 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:30:30.996 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:30:30.996 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:30:30.996 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:30:30.996 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60483]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:30:30.997 [Controller id=0] Starting the controller scheduler
2025-01-01 00:30:31.000 Kafka version: 3.4.1
2025-01-01 00:30:31.001 Kafka commitId: 8a516edc2755df89
2025-01-01 00:30:31.001 Kafka startTimeMs: 1735662631000
2025-01-01 00:30:31.004 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60483 (id: 0 rack: null)
2025-01-01 00:30:31.020 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-01-01 00:30:31.029 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(yN0IMJNSTxytpfi0DzNmeQ),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:30:31.029 [Controller id=0] New partition creation callback for pong-message-topic-0
2025-01-01 00:30:31.029 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:31.029 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:31.030 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:31.037 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:31.037 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
2025-01-01 00:30:31.037 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions
2025-01-01 00:30:31.039 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:31.039 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 1 partitions
2025-01-01 00:30:31.040 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0)
2025-01-01 00:30:31.040 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
2025-01-01 00:30:31.047 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60483 (id: 0 rack: null)
2025-01-01 00:30:31.050 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.94454775-2bf5-4d05-b7a7-4a559e1bde436035987771263705754] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:31.051 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.94454775-2bf5-4d05-b7a7-4a559e1bde436035987771263705754\pong-message-topic-0 with properties {}
2025-01-01 00:30:31.052 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:30:31.052 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:30:31.052 [Broker id=0] Leader pong-message-topic-0 with topic id Some(yN0IMJNSTxytpfi0DzNmeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:31.056 [Broker id=0] Finished LeaderAndIsr request in 17ms correlationId 1 from controller 0 for 1 partitions
2025-01-01 00:30:31.059 [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:30:31.061 App info kafka.admin.client for adminclient-2 unregistered
2025-01-01 00:30:31.063 Metrics scheduler closed
2025-01-01 00:30:31.063 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:30:31.063 Metrics reporters closed
2025-01-01 00:30:31.079 Starting PongControllerSpec using Java 17.0.12 with PID 27232 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:30:31.080 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:30:31.608 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:30:31.612 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:30:31.735 Finished Spring Data repository scanning in 114 ms. Found 1 JPA repository interfaces.
2025-01-01 00:30:31.753 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:30:31.755 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:30:31.770 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:30:31.772 Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2025-01-01 00:30:32.400 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:30:32.463 HHH000412: Hibernate ORM core version 6.2.9.Final
2025-01-01 00:30:32.467 HHH000406: Using bytecode reflection optimizer
2025-01-01 00:30:32.619 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:30:32.784 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:30:32.811 HikariPool-1 - Starting...
2025-01-01 00:30:33.065 HikariPool-1 - Added connection conn0: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:30:33.068 HikariPool-1 - Start completed.
2025-01-01 00:30:33.390 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:30:34.041 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:30:34.136 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:30:35.084 [Controller id=0] Processing automatic preferred replica leader election
2025-01-01 00:30:35.403 Netty started on port 60487
2025-01-01 00:30:35.437 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60477]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:30:35.474 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:30:35.474 Kafka version: 3.4.1
2025-01-01 00:30:35.474 Kafka commitId: 8a516edc2755df89
2025-01-01 00:30:35.474 Kafka startTimeMs: 1735662635474
2025-01-01 00:30:35.476 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:30:35.493 Started PongControllerSpec in 4.98 seconds (process running for 19.807)
2025-01-01 00:30:35.503 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to 6AjmdSVgTu6Fqv-G2ntqZQ
2025-01-01 00:30:35.503 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to 6AjmdSVgTu6Fqv-G2ntqZQ
2025-01-01 00:30:35.505 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cluster ID: dXAHhMLaSmW7_KZF0kl2cA
2025-01-01 00:30:35.507 Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-01-01 00:30:35.512 [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(gVQd21YLTvSKHlrNJJyqBQ),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:30:35.512 [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-01-01 00:30:35.512 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:35.512 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:35.512 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:35.512 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:35.512 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:35.513 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:35.513 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:35.522 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:35.522 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:35.522 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:35.522 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:35.522 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:35.522 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-01-01 00:30:35.522 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-01-01 00:30:35.523 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:35.523 [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:30:35.525 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-01-01 00:30:35.525 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-01-01 00:30:35.534 [LogLoader partition=__consumer_offsets-3, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:35.535 Created log for partition __consumer_offsets-3 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222\__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:30:35.537 [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-01-01 00:30:35.537 [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-01-01 00:30:35.537 [Broker id=0] Leader __consumer_offsets-3 with topic id Some(gVQd21YLTvSKHlrNJJyqBQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:35.546 [LogLoader partition=__consumer_offsets-2, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:35.546 Created log for partition __consumer_offsets-2 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222\__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:30:35.546 [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-01-01 00:30:35.546 [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-01-01 00:30:35.546 [Broker id=0] Leader __consumer_offsets-2 with topic id Some(gVQd21YLTvSKHlrNJJyqBQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:35.555 [LogLoader partition=__consumer_offsets-4, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:35.556 Created log for partition __consumer_offsets-4 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222\__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:30:35.556 [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-01-01 00:30:35.556 [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-01-01 00:30:35.556 [Broker id=0] Leader __consumer_offsets-4 with topic id Some(gVQd21YLTvSKHlrNJJyqBQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:35.565 [LogLoader partition=__consumer_offsets-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:35.566 Created log for partition __consumer_offsets-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222\__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:30:35.566 [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-01-01 00:30:35.566 [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-01-01 00:30:35.566 [Broker id=0] Leader __consumer_offsets-1 with topic id Some(gVQd21YLTvSKHlrNJJyqBQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:35.574 [LogLoader partition=__consumer_offsets-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:35.575 Created log for partition __consumer_offsets-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.e68be9a9-2c51-4d59-b071-e066308f2e8815239114239751819222\__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:30:35.575 [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-01-01 00:30:35.575 [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-01-01 00:30:35.575 [Broker id=0] Leader __consumer_offsets-0 with topic id Some(gVQd21YLTvSKHlrNJJyqBQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:35.578 [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-01-01 00:30:35.579 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-01-01 00:30:35.580 [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-01-01 00:30:35.580 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-01-01 00:30:35.580 [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-01-01 00:30:35.580 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-01-01 00:30:35.580 [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-01-01 00:30:35.580 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-01-01 00:30:35.580 [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-01-01 00:30:35.580 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-01-01 00:30:35.580 [Broker id=0] Finished LeaderAndIsr request in 57ms correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:30:35.583 [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-01-01 00:30:35.585 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 4 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler.
2025-01-01 00:30:35.585 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler.
2025-01-01 00:30:35.585 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler.
2025-01-01 00:30:35.585 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler.
2025-01-01 00:30:35.586 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler.
2025-01-01 00:30:35.607 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Discovered group coordinator localhost:60477 (id: 2147483647 rack: null)
2025-01-01 00:30:35.609 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:30:35.625 [GroupCoordinator 0]: Dynamic member with unknown member id joins group pong-service in Empty state. Created a new member id consumer-pong-service-1-1e458dc7-8cd0-4486-b063-236bf7c1c6f4 and request the member to rejoin with this id.
2025-01-01 00:30:35.628 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: need to re-join with the given member-id: consumer-pong-service-1-1e458dc7-8cd0-4486-b063-236bf7c1c6f4
2025-01-01 00:30:35.628 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2025-01-01 00:30:35.628 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:30:35.633 [GroupCoordinator 0]: Preparing to rebalance group pong-service in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-pong-service-1-1e458dc7-8cd0-4486-b063-236bf7c1c6f4 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)
2025-01-01 00:30:35.638 [GroupCoordinator 0]: Stabilized group pong-service generation 1 (__consumer_offsets-2) with 1 members
2025-01-01 00:30:35.640 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully joined group with generation Generation{generationId=1, memberId='consumer-pong-service-1-1e458dc7-8cd0-4486-b063-236bf7c1c6f4', protocol='range'}
2025-01-01 00:30:35.641 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Finished assignment for group at generation 1: {consumer-pong-service-1-1e458dc7-8cd0-4486-b063-236bf7c1c6f4=Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])}
2025-01-01 00:30:35.648 [GroupCoordinator 0]: Assignment received from leader consumer-pong-service-1-1e458dc7-8cd0-4486-b063-236bf7c1c6f4 for group pong-service for generation 1. The group has 1 members, 0 of which are static.
2025-01-01 00:30:35.685 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully synced group in generation Generation{generationId=1, memberId='consumer-pong-service-1-1e458dc7-8cd0-4486-b063-236bf7c1c6f4', protocol='range'}
2025-01-01 00:30:35.686 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Notifying assignor about the new Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])
2025-01-01 00:30:35.688 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Adding newly assigned partitions: pong-message-topic-0, pong-message-topic-1
2025-01-01 00:30:35.694 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:30:35.694 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:30:35.697 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:30:35.697 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:30:35.706 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60477 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:30:35.707 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60477 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:30:35.721 pong-service: partitions assigned: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:30:35.999 [Controller id=0] Processing automatic preferred replica leader election
2025-01-01 00:30:36.318 Received ping message: Hello
2025-01-01 00:30:36.326 Idempotence will be disabled because acks is set to 0, not set to 'all'.
2025-01-01 00:30:36.326 ProducerConfig values: 
	acks = 0
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:60477]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-01-01 00:30:36.346 Kafka version: 3.4.1
2025-01-01 00:30:36.346 Kafka commitId: 8a516edc2755df89
2025-01-01 00:30:36.346 Kafka startTimeMs: 1735662636346
2025-01-01 00:30:36.350 [Producer clientId=producer-1] Cluster ID: dXAHhMLaSmW7_KZF0kl2cA
2025-01-01 00:30:36.352 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to 6AjmdSVgTu6Fqv-G2ntqZQ
2025-01-01 00:30:36.352 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to 6AjmdSVgTu6Fqv-G2ntqZQ
2025-01-01 00:30:37.626 Received ping message: Hello
2025-01-01 00:30:37.627 Throttling request as too many requests in the same second
2025-01-01 00:30:37.627 Received ping message: Hello
2025-01-01 00:30:37.896 [KafkaServer id=0] shutting down
2025-01-01 00:30:37.897 [KafkaServer id=0] Starting controlled shutdown
2025-01-01 00:30:37.904 [Controller id=0] Shutting down broker 0
2025-01-01 00:30:37.906 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:37.909 [KafkaServer id=0] Controlled shutdown request returned successfully after 7ms
2025-01-01 00:30:37.910 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:30:37.912 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:30:37.912 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:30:37.912 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:30:37.915 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:30:37.915 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:30:37.916 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cancelled in-flight FETCH request with correlation id 22 due to node 0 being disconnected (elapsed time since creation: 278ms, elapsed time since send: 278ms, request timeout: 30000ms)
2025-01-01 00:30:37.916 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 2147483647 disconnected.
2025-01-01 00:30:37.916 [Producer clientId=producer-1] Node -1 disconnected.
2025-01-01 00:30:37.916 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:30:37.916 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Error sending fetch request (sessionId=104385326, epoch=6) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
2025-01-01 00:30:37.916 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Group coordinator localhost:60477 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2025-01-01 00:30:37.918 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:30:37.919 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:30:37.920 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:30:37.922 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:30:37.924 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:30:37.924 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:30:37.924 [KafkaApi-0] Shutdown complete.
2025-01-01 00:30:37.925 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:30:37.925 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:30:37.925 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:30:37.926 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:30:37.926 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:30:37.926 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:30:37.926 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:30:37.926 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:30:37.928 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:30:37.928 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:30:37.928 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:30:37.928 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:30:37.928 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:30:37.928 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:30:37.929 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:30:37.929 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:30:37.929 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:30:37.930 [ReplicaManager broker=0] Shutting down
2025-01-01 00:30:37.930 [LogDirFailureHandler]: Shutting down
2025-01-01 00:30:37.930 [LogDirFailureHandler]: Stopped
2025-01-01 00:30:37.930 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:30:37.930 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:30:37.931 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:30:37.931 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:30:37.931 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:30:37.931 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:30:37.931 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:30:37.931 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:30:37.931 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:30:37.932 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:30:37.932 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:30:37.932 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:30:37.932 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:30:37.932 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:30:37.932 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:30:37.932 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:30:37.932 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:30:37.937 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:30:37.938 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:30:37.938 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:30:37.938 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:30:37.939 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:30:37.940 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:30:37.940 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:30:37.940 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:30:37.940 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:30:37.941 Shutting down.
2025-01-01 00:30:37.941 Shutting down the log cleaner.
2025-01-01 00:30:37.942 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:30:37.943 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:30:37.943 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:30:37.968 [ProducerStateManager partition=pong-message-topic-1] Wrote producer snapshot at offset 3 with 0 producer ids in 4 ms.
2025-01-01 00:30:37.976 [ProducerStateManager partition=__consumer_offsets-2] Wrote producer snapshot at offset 6 with 0 producer ids in 2 ms.
2025-01-01 00:30:37.988 Shutdown complete.
2025-01-01 00:30:37.988 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:30:37.988 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:30:37.988 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:30:37.989 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:30:37.990 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:30:37.990 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:30:37.990 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:30:37.990 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:30:37.991 [Controller id=0] Resigned
2025-01-01 00:30:37.991 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:30:37.992 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:30:37.992 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:30:37.992 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:30:38.017 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:30:38.017 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:30:38.017 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60477) could not be established. Broker may not be available.
2025-01-01 00:30:38.017 [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:60477) could not be established. Broker may not be available.
2025-01-01 00:30:38.097 Session: 0x1001828ef4d0000 closed
2025-01-01 00:30:38.097 EventThread shut down for session: 0x1001828ef4d0000
2025-01-01 00:30:38.097 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:30:38.098 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:30:38.100 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:30:38.100 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:30:38.100 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:30:38.100 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:30:38.100 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:30:38.101 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:30:38.101 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:30:38.101 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:30:38.101 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:30:38.101 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:30:38.101 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:30:38.101 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:30:38.116 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:30:38.117 Metrics scheduler closed
2025-01-01 00:30:38.117 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:30:38.117 Metrics reporters closed
2025-01-01 00:30:38.117 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:30:38.117 [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:60477) could not be established. Broker may not be available.
2025-01-01 00:30:38.118 Broker and topic stats closed
2025-01-01 00:30:38.118 App info kafka.server for 0 unregistered
2025-01-01 00:30:38.118 [KafkaServer id=0] shut down completed
2025-01-01 00:30:38.127 ConnnectionExpirerThread interrupted
2025-01-01 00:30:38.128 selector thread exitted run method
2025-01-01 00:30:38.128 accept thread exitted run method
2025-01-01 00:30:38.128 selector thread exitted run method
2025-01-01 00:30:38.128 shutting down
2025-01-01 00:30:38.128 Shutting down
2025-01-01 00:30:38.128 Draining request throttler queue
2025-01-01 00:30:38.128 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:30:38.128 Shutting down
2025-01-01 00:30:38.128 Shutting down
2025-01-01 00:30:38.128 Shutting down
2025-01-01 00:30:38.128 PrepRequestProcessor exited loop!
2025-01-01 00:30:38.130 SyncRequestProcessor exited!
2025-01-01 00:30:38.130 shutdown of request processor complete
2025-01-01 00:30:38.146 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:30:38.147 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60477) could not be established. Broker may not be available.
2025-01-01 00:30:38.147 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Revoke previously assigned partitions pong-message-topic-0, pong-message-topic-1
2025-01-01 00:30:38.147 pong-service: partitions revoked: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:30:38.148 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:30:38.148 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:30:38.148 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:30:38.148 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:30:38.148 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:30:38.148 Metrics scheduler closed
2025-01-01 00:30:38.148 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:30:38.148 Metrics reporters closed
2025-01-01 00:30:38.149 App info kafka.consumer for consumer-pong-service-1 unregistered
2025-01-01 00:30:38.149 pong-service: Consumer stopped
2025-01-01 00:30:38.153 [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-01 00:30:38.153 Metrics scheduler closed
2025-01-01 00:30:38.153 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:30:38.154 Metrics reporters closed
2025-01-01 00:30:38.154 App info kafka.producer for producer-1 unregistered
2025-01-01 00:30:38.165 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:30:38.167 HikariPool-1 - Shutdown initiated...
2025-01-01 00:30:38.168 HikariPool-1 - Shutdown completed.
2025-01-01 00:30:38.689 SessionTrackerImpl exited loop!
2025-01-01 00:30:40.266 [KafkaServer id=0] shutting down
2025-01-01 00:30:40.266 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:30:40.266 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:30:40.266 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:30:40.266 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:30:40.268 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:30:40.268 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:30:40.268 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:30:40.269 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:30:40.269 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:30:40.269 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:30:40.269 [KafkaApi-0] Shutdown complete.
2025-01-01 00:30:40.269 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:30:40.269 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:30:40.269 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:30:40.269 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:30:40.269 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:30:40.269 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:30:40.269 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:30:40.269 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:30:40.270 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:30:40.270 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:30:40.270 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:30:40.270 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:30:40.270 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:30:40.270 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:30:40.272 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:30:40.272 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:30:40.272 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:30:40.272 [ReplicaManager broker=0] Shutting down
2025-01-01 00:30:40.272 [LogDirFailureHandler]: Shutting down
2025-01-01 00:30:40.272 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:30:40.272 [LogDirFailureHandler]: Stopped
2025-01-01 00:30:40.272 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:30:40.272 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:30:40.272 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:30:40.272 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:30:40.272 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:30:40.272 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:30:40.272 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:30:40.272 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:30:40.272 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:30:40.272 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:30:40.272 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:30:40.272 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:30:40.272 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:30:40.272 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:30:40.273 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:30:40.273 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:30:40.275 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:30:40.275 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:30:40.275 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:30:40.275 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:30:40.276 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:30:40.276 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:30:40.276 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:30:40.276 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:30:40.276 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:30:40.276 Shutting down.
2025-01-01 00:30:40.277 Shutting down the log cleaner.
2025-01-01 00:30:40.277 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:30:40.277 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:30:40.277 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:30:40.284 Shutdown complete.
2025-01-01 00:30:40.284 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:30:40.284 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:30:40.284 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:30:40.286 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:30:40.286 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:30:40.286 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:30:40.286 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:30:40.286 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:30:40.287 [Controller id=0] Resigned
2025-01-01 00:30:40.287 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:30:40.287 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:30:40.287 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:30:40.287 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:30:40.389 Session: 0x10018290c520000 closed
2025-01-01 00:30:40.389 EventThread shut down for session: 0x10018290c520000
2025-01-01 00:30:40.389 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:30:40.389 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:30:40.389 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:30:40.389 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:30:40.389 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:30:40.389 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:30:40.389 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:30:40.389 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:30:40.390 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:30:40.390 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:30:40.390 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:30:40.390 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:30:40.390 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:30:40.390 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:30:40.396 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:30:40.396 Metrics scheduler closed
2025-01-01 00:30:40.396 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:30:40.396 Metrics reporters closed
2025-01-01 00:30:40.396 Broker and topic stats closed
2025-01-01 00:30:40.397 App info kafka.server for 0 unregistered
2025-01-01 00:30:40.397 [KafkaServer id=0] shut down completed
2025-01-01 00:30:40.400 ConnnectionExpirerThread interrupted
2025-01-01 00:30:40.400 accept thread exitted run method
2025-01-01 00:30:40.400 selector thread exitted run method
2025-01-01 00:30:40.400 selector thread exitted run method
2025-01-01 00:30:40.402 shutting down
2025-01-01 00:30:40.402 Shutting down
2025-01-01 00:30:40.402 Draining request throttler queue
2025-01-01 00:30:40.402 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:30:40.402 Shutting down
2025-01-01 00:30:40.402 Shutting down
2025-01-01 00:30:40.402 Shutting down
2025-01-01 00:30:40.402 PrepRequestProcessor exited loop!
2025-01-01 00:30:40.402 SyncRequestProcessor exited!
2025-01-01 00:30:40.402 shutdown of request processor complete
2025-01-01 00:30:40.737 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/work/apache-maven-3.8.8/repo/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/work/apache-maven-3.8.8/repo/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:30:40.740 Starting MessageRepositorySpec using Java 17.0.12 with PID 27232 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:30:40.740 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:30:40.845 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:30:40.845 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:30:40.859 Finished Spring Data repository scanning in 13 ms. Found 1 JPA repository interfaces.
2025-01-01 00:30:40.871 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:30:40.875 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:30:40.880 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:30:40.880 Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2025-01-01 00:30:40.949 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:30:40.957 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:30:40.957 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:30:40.958 HikariPool-2 - Starting...
2025-01-01 00:30:40.960 HikariPool-2 - Added connection conn10: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:30:40.960 HikariPool-2 - Start completed.
2025-01-01 00:30:40.968 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:30:40.987 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:30:41.003 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:30:41.089 SessionTrackerImpl exited loop!
2025-01-01 00:30:41.173 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60477]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:30:41.180 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:30:41.180 Kafka version: 3.4.1
2025-01-01 00:30:41.180 Kafka commitId: 8a516edc2755df89
2025-01-01 00:30:41.180 Kafka startTimeMs: 1735662641180
2025-01-01 00:30:41.180 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:30:41.181 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:30:41.181 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Connection to node -1 (/127.0.0.1:60477) could not be established. Broker may not be available.
2025-01-01 00:30:41.181 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Bootstrap broker 127.0.0.1:60477 (id: -1 rack: null) disconnected
2025-01-01 00:30:41.181 Started MessageRepositorySpec in 0.469 seconds (process running for 25.497)
2025-01-01 00:30:41.206 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:30:41.206 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:30:41.206 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:30:41.206 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:30:41.206 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:30:41.206 Metrics scheduler closed
2025-01-01 00:30:41.206 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:30:41.206 Metrics reporters closed
2025-01-01 00:30:41.206 App info kafka.consumer for consumer-pong-service-2 unregistered
2025-01-01 00:30:41.206 pong-service: Consumer stopped
2025-01-01 00:30:41.213 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:30:41.213 HikariPool-2 - Shutdown initiated...
2025-01-01 00:30:41.213 HikariPool-2 - Shutdown completed.
2025-01-01 00:30:41.230 zookeeper.snapshot.trust.empty : false
2025-01-01 00:30:41.230 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:30:41.230 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:30:41.230 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:30:41.230 zookeeper.commitLogCount=500
2025-01-01 00:30:41.230 minSessionTimeout set to 1600
2025-01-01 00:30:41.230 maxSessionTimeout set to 16000
2025-01-01 00:30:41.230 Response cache size is initialized with value 400.
2025-01-01 00:30:41.230 Response cache size is initialized with value 400.
2025-01-01 00:30:41.230 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:30:41.230 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:30:41.230 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:30:41.230 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:30:41.230 zookeeper.pathStats.delay = 5
2025-01-01 00:30:41.230 zookeeper.pathStats.enabled = false
2025-01-01 00:30:41.230 The max bytes for all large requests are set to 104857600
2025-01-01 00:30:41.230 The large request threshold is set to -1
2025-01-01 00:30:41.230 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-2452846564085936247\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-13953996808225845663\version-2
2025-01-01 00:30:41.230 maxCnxns is not configured, using default value 0.
2025-01-01 00:30:41.230 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:30:41.230 binding to port /127.0.0.1:0
2025-01-01 00:30:41.237 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-13953996808225845663\version-2\snapshot.0
2025-01-01 00:30:41.237 Snapshot loaded in 1 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:30:41.237 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-13953996808225845663\version-2\snapshot.0
2025-01-01 00:30:41.237 Snapshot taken in 1 ms
2025-01-01 00:30:41.238 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:30:41.239 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60507
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:30:41.243 starting
2025-01-01 00:30:41.243 Connecting to zookeeper on 127.0.0.1:60507
2025-01-01 00:30:41.243 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60507.
2025-01-01 00:30:41.243 Initiating client connection, connectString=127.0.0.1:60507 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@5b00d62
2025-01-01 00:30:41.244 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:30:41.245 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:30:41.245 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:30:41.245 Opening socket connection to server /127.0.0.1:60507.
2025-01-01 00:30:41.246 Socket connection established, initiating session, client: /127.0.0.1:60508, server: /127.0.0.1:60507
2025-01-01 00:30:41.246 Creating new log file: log.1
2025-01-01 00:30:41.251 Session establishment complete on server /127.0.0.1:60507, session id = 0x100182935160000, negotiated timeout = 16000
2025-01-01 00:30:41.251 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:30:41.280 Cluster ID = UAlaDHn3RlCZpKZeIspZBA
2025-01-01 00:30:41.281 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995\meta.properties
2025-01-01 00:30:41.287 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60507
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:30:41.291 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:30:41.291 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:30:41.291 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:30:41.291 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:30:41.294 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995)
2025-01-01 00:30:41.295 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995 since no clean shutdown file was found
2025-01-01 00:30:41.295 Loaded 0 logs in 1ms.
2025-01-01 00:30:41.296 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:30:41.296 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:30:41.298 Starting the log cleaner
2025-01-01 00:30:41.298 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:30:41.298 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:30:41.300 Feature ZK node at path: /feature does not exist
2025-01-01 00:30:41.301 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:30:41.314 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:30:41.315 Awaiting socket connections on localhost:60509.
2025-01-01 00:30:41.321 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:30:41.323 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:30:41.325 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:30:41.325 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:30:41.325 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:30:41.325 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:30:41.326 [LogDirFailureHandler]: Starting
2025-01-01 00:30:41.328 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:30:41.331 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662641329,1735662641329,1,0,0,72084159301287936,204,0,25

2025-01-01 00:30:41.331 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60509, czxid (broker epoch): 25
2025-01-01 00:30:41.335 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:30:41.335 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:30:41.336 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:30:41.336 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:30:41.336 [GroupCoordinator 0]: Starting up.
2025-01-01 00:30:41.336 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:30:41.338 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:30:41.339 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:30:41.339 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:30:41.340 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:30:41.340 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:30:41.340 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:30:41.341 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:30:41.341 Feature ZK node created at path: /feature
2025-01-01 00:30:41.342 [Controller id=0] Registering handlers
2025-01-01 00:30:41.342 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:30:41.342 [/config/changes-event-process-thread]: Starting
2025-01-01 00:30:41.344 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:30:41.344 [Controller id=0] Deleting isr change notifications
2025-01-01 00:30:41.345 [Controller id=0] Initializing controller context
2025-01-01 00:30:41.345 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:30:41.345 Kafka version: 3.4.1
2025-01-01 00:30:41.345 Kafka commitId: 8a516edc2755df89
2025-01-01 00:30:41.345 Kafka startTimeMs: 1735662641345
2025-01-01 00:30:41.345 [KafkaServer id=0] started
2025-01-01 00:30:41.347 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60509]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:30:41.347 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:30:41.349 Kafka version: 3.4.1
2025-01-01 00:30:41.349 Kafka commitId: 8a516edc2755df89
2025-01-01 00:30:41.349 Kafka startTimeMs: 1735662641349
2025-01-01 00:30:41.351 [RequestSendThread controllerId=0] Starting
2025-01-01 00:30:41.351 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:30:41.351 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:30:41.351 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:30:41.351 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:30:41.352 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:30:41.352 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:30:41.352 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:30:41.353 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:30:41.353 [Controller id=0] Sending update metadata request
2025-01-01 00:30:41.353 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:30:41.353 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:30:41.353 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:30:41.353 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:30:41.353 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:30:41.353 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:30:41.353 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:30:41.354 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60509 (id: 0 rack: null) for sending state change requests
2025-01-01 00:30:41.355 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:30:41.355 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:30:41.355 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:30:41.355 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:30:41.355 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:30:41.357 [Controller id=0] Starting the controller scheduler
2025-01-01 00:30:41.401 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60509 (id: 0 rack: null)
2025-01-01 00:30:41.424 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60509 (id: 0 rack: null)
2025-01-01 00:30:41.457 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0))
2025-01-01 00:30:41.464 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(XPco2sGUTVuxEK2704j8Ow),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), pong-message-topic-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:30:41.464 [Controller id=0] New partition creation callback for pong-message-topic-0,pong-message-topic-1
2025-01-01 00:30:41.464 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:41.464 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:41.464 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:41.464 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:41.469 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:41.469 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:41.469 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions
2025-01-01 00:30:41.469 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 2 partitions
2025-01-01 00:30:41.470 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:41.470 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:30:41.471 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0, pong-message-topic-1)
2025-01-01 00:30:41.471 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
2025-01-01 00:30:41.476 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:41.477 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995\pong-message-topic-0 with properties {}
2025-01-01 00:30:41.477 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:30:41.478 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:30:41.478 [Broker id=0] Leader pong-message-topic-0 with topic id Some(XPco2sGUTVuxEK2704j8Ow) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:41.486 [LogLoader partition=pong-message-topic-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:41.486 Created log for partition pong-message-topic-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995\pong-message-topic-1 with properties {}
2025-01-01 00:30:41.487 [Partition pong-message-topic-1 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-1
2025-01-01 00:30:41.487 [Partition pong-message-topic-1 broker=0] Log loaded for partition pong-message-topic-1 with initial high watermark 0
2025-01-01 00:30:41.487 [Broker id=0] Leader pong-message-topic-1 with topic id Some(XPco2sGUTVuxEK2704j8Ow) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:41.489 [Broker id=0] Finished LeaderAndIsr request in 19ms correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:30:41.491 [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:30:41.491 App info kafka.admin.client for adminclient-3 unregistered
2025-01-01 00:30:41.493 Metrics scheduler closed
2025-01-01 00:30:41.493 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:30:41.493 Metrics reporters closed
2025-01-01 00:30:41.519 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/work/apache-maven-3.8.8/repo/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/work/apache-maven-3.8.8/repo/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:30:41.521 Starting KafkaSpec using Java 17.0.12 with PID 27232 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:30:41.521 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:30:41.614 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:30:41.614 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:30:41.627 Finished Spring Data repository scanning in 11 ms. Found 1 JPA repository interfaces.
2025-01-01 00:30:41.629 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:30:41.629 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:30:41.632 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:30:41.632 Finished Spring Data repository scanning in 2 ms. Found 0 Redis repository interfaces.
2025-01-01 00:30:41.691 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:30:41.697 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:30:41.697 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:30:41.699 HikariPool-3 - Starting...
2025-01-01 00:30:41.701 HikariPool-3 - Added connection conn20: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:30:41.701 HikariPool-3 - Start completed.
2025-01-01 00:30:41.708 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:30:41.727 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:30:41.740 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:30:41.868 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60509]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:30:41.872 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:30:41.872 Kafka version: 3.4.1
2025-01-01 00:30:41.872 Kafka commitId: 8a516edc2755df89
2025-01-01 00:30:41.872 Kafka startTimeMs: 1735662641872
2025-01-01 00:30:41.872 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:30:41.876 Started KafkaSpec in 0.38 seconds (process running for 26.19)
2025-01-01 00:30:41.877 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to XPco2sGUTVuxEK2704j8Ow
2025-01-01 00:30:41.877 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to XPco2sGUTVuxEK2704j8Ow
2025-01-01 00:30:41.877 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cluster ID: UAlaDHn3RlCZpKZeIspZBA
2025-01-01 00:30:41.880 Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-01-01 00:30:41.885 [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(9iWFVb49QUex2oAarL7qWg),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:30:41.885 [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-01-01 00:30:41.887 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:41.887 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:41.887 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:41.887 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:41.887 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:30:41.887 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:41.887 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:41.893 Idempotence will be disabled because acks is set to 0, not set to 'all'.
2025-01-01 00:30:41.894 ProducerConfig values: 
	acks = 0
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:60509]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-01-01 00:30:41.897 Kafka version: 3.4.1
2025-01-01 00:30:41.897 Kafka commitId: 8a516edc2755df89
2025-01-01 00:30:41.897 Kafka startTimeMs: 1735662641897
2025-01-01 00:30:41.901 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:41.901 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:41.901 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:41.901 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:41.901 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:30:41.901 [Producer clientId=producer-2] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to XPco2sGUTVuxEK2704j8Ow
2025-01-01 00:30:41.901 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-01-01 00:30:41.901 [Producer clientId=producer-2] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to XPco2sGUTVuxEK2704j8Ow
2025-01-01 00:30:41.901 [Producer clientId=producer-2] Cluster ID: UAlaDHn3RlCZpKZeIspZBA
2025-01-01 00:30:41.901 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-01-01 00:30:41.901 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:41.902 [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:30:41.902 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-01-01 00:30:41.903 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-01-01 00:30:41.910 [LogLoader partition=__consumer_offsets-3, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:41.910 Created log for partition __consumer_offsets-3 in C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995\__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:30:41.911 [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-01-01 00:30:41.911 [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-01-01 00:30:41.911 [Broker id=0] Leader __consumer_offsets-3 with topic id Some(9iWFVb49QUex2oAarL7qWg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:41.920 [LogLoader partition=__consumer_offsets-2, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:41.921 Created log for partition __consumer_offsets-2 in C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995\__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:30:41.921 [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-01-01 00:30:41.921 [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-01-01 00:30:41.921 [Broker id=0] Leader __consumer_offsets-2 with topic id Some(9iWFVb49QUex2oAarL7qWg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:41.929 [LogLoader partition=__consumer_offsets-4, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:41.931 Created log for partition __consumer_offsets-4 in C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995\__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:30:41.931 [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-01-01 00:30:41.931 [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-01-01 00:30:41.931 [Broker id=0] Leader __consumer_offsets-4 with topic id Some(9iWFVb49QUex2oAarL7qWg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:41.939 [LogLoader partition=__consumer_offsets-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:41.940 Created log for partition __consumer_offsets-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995\__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:30:41.940 [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-01-01 00:30:41.940 [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-01-01 00:30:41.940 [Broker id=0] Leader __consumer_offsets-1 with topic id Some(9iWFVb49QUex2oAarL7qWg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:41.949 [LogLoader partition=__consumer_offsets-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995] Loading producer state till offset 0 with message format version 2
2025-01-01 00:30:41.949 Created log for partition __consumer_offsets-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.77d05f17-2002-4852-9085-b41eeff5b9fc8519343280650064995\__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:30:41.950 [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-01-01 00:30:41.950 [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-01-01 00:30:41.950 [Broker id=0] Leader __consumer_offsets-0 with topic id Some(9iWFVb49QUex2oAarL7qWg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:30:41.952 [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-01-01 00:30:41.953 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-01-01 00:30:41.953 [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-01-01 00:30:41.953 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-01-01 00:30:41.953 [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-01-01 00:30:41.953 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-01-01 00:30:41.953 [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-01-01 00:30:41.953 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-01-01 00:30:41.953 [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-01-01 00:30:41.953 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-01-01 00:30:41.953 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:30:41.953 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:30:41.953 [Broker id=0] Finished LeaderAndIsr request in 51ms correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:30:41.953 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:30:41.953 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:30:41.953 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:30:41.955 [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-01-01 00:30:41.981 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Discovered group coordinator localhost:60509 (id: 2147483647 rack: null)
2025-01-01 00:30:41.983 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] (Re-)joining group
2025-01-01 00:30:41.985 [GroupCoordinator 0]: Dynamic member with unknown member id joins group pong-service in Empty state. Created a new member id consumer-pong-service-3-a757b520-f36b-4686-9af0-7e6f8afdd641 and request the member to rejoin with this id.
2025-01-01 00:30:41.987 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: need to re-join with the given member-id: consumer-pong-service-3-a757b520-f36b-4686-9af0-7e6f8afdd641
2025-01-01 00:30:41.987 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2025-01-01 00:30:41.987 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] (Re-)joining group
2025-01-01 00:30:41.988 [GroupCoordinator 0]: Preparing to rebalance group pong-service in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-pong-service-3-a757b520-f36b-4686-9af0-7e6f8afdd641 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)
2025-01-01 00:30:41.989 [GroupCoordinator 0]: Stabilized group pong-service generation 1 (__consumer_offsets-2) with 1 members
2025-01-01 00:30:41.990 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Successfully joined group with generation Generation{generationId=1, memberId='consumer-pong-service-3-a757b520-f36b-4686-9af0-7e6f8afdd641', protocol='range'}
2025-01-01 00:30:41.990 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Finished assignment for group at generation 1: {consumer-pong-service-3-a757b520-f36b-4686-9af0-7e6f8afdd641=Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])}
2025-01-01 00:30:41.990 [GroupCoordinator 0]: Assignment received from leader consumer-pong-service-3-a757b520-f36b-4686-9af0-7e6f8afdd641 for group pong-service for generation 1. The group has 1 members, 0 of which are static.
2025-01-01 00:30:41.992 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Successfully synced group in generation Generation{generationId=1, memberId='consumer-pong-service-3-a757b520-f36b-4686-9af0-7e6f8afdd641', protocol='range'}
2025-01-01 00:30:41.993 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Notifying assignor about the new Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])
2025-01-01 00:30:41.993 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Adding newly assigned partitions: pong-message-topic-0, pong-message-topic-1
2025-01-01 00:30:41.994 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:30:41.994 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:30:41.995 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:30:41.995 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:30:41.996 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting offset for partition pong-message-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60509 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:30:41.996 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting offset for partition pong-message-topic-1 to position FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60509 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:30:42.000 pong-service: partitions assigned: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:30:46.366 [Controller id=0] Processing automatic preferred replica leader election
2025-01-01 00:30:47.211 [KafkaServer id=0] shutting down
2025-01-01 00:30:47.211 [KafkaServer id=0] Starting controlled shutdown
2025-01-01 00:30:47.215 [Controller id=0] Shutting down broker 0
2025-01-01 00:30:47.215 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:30:47.216 [KafkaServer id=0] Controlled shutdown request returned successfully after 2ms
2025-01-01 00:30:47.217 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:30:47.217 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:30:47.217 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:30:47.218 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:30:47.218 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:30:47.218 [Producer clientId=producer-2] Node -1 disconnected.
2025-01-01 00:30:47.219 [Producer clientId=producer-2] Node 0 disconnected.
2025-01-01 00:30:47.219 [Producer clientId=producer-2] Cancelled in-flight METADATA request with correlation id 4 due to node 0 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, request timeout: 30000ms)
2025-01-01 00:30:47.219 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:30:47.219 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cancelled in-flight FETCH request with correlation id 25 due to node 0 being disconnected (elapsed time since creation: 149ms, elapsed time since send: 149ms, request timeout: 30000ms)
2025-01-01 00:30:47.219 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cancelled in-flight METADATA request with correlation id 26 due to node 0 being disconnected (elapsed time since creation: 42ms, elapsed time since send: 42ms, request timeout: 30000ms)
2025-01-01 00:30:47.219 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 2147483647 disconnected.
2025-01-01 00:30:47.219 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Error sending fetch request (sessionId=302730754, epoch=10) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
2025-01-01 00:30:47.219 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Group coordinator localhost:60509 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2025-01-01 00:30:47.219 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:30:47.220 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:30:47.220 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:30:47.220 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:30:47.221 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:30:47.221 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:30:47.221 [KafkaApi-0] Shutdown complete.
2025-01-01 00:30:47.221 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:30:47.221 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:30:47.221 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:30:47.221 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:30:47.221 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:30:47.221 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:30:47.222 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:30:47.222 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:30:47.222 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:30:47.222 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:30:47.222 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:30:47.222 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:30:47.222 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:30:47.223 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:30:47.223 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:30:47.223 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:30:47.223 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:30:47.223 [ReplicaManager broker=0] Shutting down
2025-01-01 00:30:47.223 [LogDirFailureHandler]: Shutting down
2025-01-01 00:30:47.223 [LogDirFailureHandler]: Stopped
2025-01-01 00:30:47.223 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:30:47.223 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:30:47.223 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:30:47.223 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:30:47.223 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:30:47.223 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:30:47.223 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:30:47.224 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:30:47.224 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:30:47.224 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:30:47.224 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:30:47.224 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:30:47.224 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:30:47.224 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:30:47.224 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:30:47.224 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:30:47.224 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:30:47.230 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:30:47.230 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:30:47.230 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:30:47.230 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:30:47.231 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:30:47.231 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:30:47.231 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:30:47.231 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:30:47.231 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:30:47.231 Shutting down.
2025-01-01 00:30:47.231 Shutting down the log cleaner.
2025-01-01 00:30:47.231 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:30:47.231 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:30:47.231 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:30:47.248 [ProducerStateManager partition=pong-message-topic-1] Wrote producer snapshot at offset 1 with 0 producer ids in 2 ms.
2025-01-01 00:30:47.256 [ProducerStateManager partition=__consumer_offsets-2] Wrote producer snapshot at offset 3 with 0 producer ids in 2 ms.
2025-01-01 00:30:47.264 Shutdown complete.
2025-01-01 00:30:47.264 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:30:47.264 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:30:47.264 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:30:47.264 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:30:47.265 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:30:47.265 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:30:47.265 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:30:47.265 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:30:47.266 [Controller id=0] Resigned
2025-01-01 00:30:47.266 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:30:47.266 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:30:47.266 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:30:47.266 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:30:47.333 [Producer clientId=producer-2] Node 0 disconnected.
2025-01-01 00:30:47.334 [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:60509) could not be established. Broker may not be available.
2025-01-01 00:30:47.333 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:30:47.334 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60509) could not be established. Broker may not be available.
2025-01-01 00:30:47.381 Session: 0x100182935160000 closed
2025-01-01 00:30:47.381 EventThread shut down for session: 0x100182935160000
2025-01-01 00:30:47.382 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:30:47.382 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:30:47.382 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:30:47.382 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:30:47.382 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:30:47.382 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:30:47.382 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:30:47.382 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:30:47.382 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:30:47.382 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:30:47.382 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:30:47.382 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:30:47.382 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:30:47.382 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:30:47.392 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:30:47.392 Metrics scheduler closed
2025-01-01 00:30:47.392 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:30:47.392 Metrics reporters closed
2025-01-01 00:30:47.392 Broker and topic stats closed
2025-01-01 00:30:47.393 App info kafka.server for 0 unregistered
2025-01-01 00:30:47.393 [KafkaServer id=0] shut down completed
2025-01-01 00:30:47.404 ConnnectionExpirerThread interrupted
2025-01-01 00:30:47.404 selector thread exitted run method
2025-01-01 00:30:47.405 accept thread exitted run method
2025-01-01 00:30:47.405 selector thread exitted run method
2025-01-01 00:30:47.405 shutting down
2025-01-01 00:30:47.405 Shutting down
2025-01-01 00:30:47.405 Draining request throttler queue
2025-01-01 00:30:47.405 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:30:47.405 Shutting down
2025-01-01 00:30:47.405 Shutting down
2025-01-01 00:30:47.405 Shutting down
2025-01-01 00:30:47.405 PrepRequestProcessor exited loop!
2025-01-01 00:30:47.405 SyncRequestProcessor exited!
2025-01-01 00:30:47.405 shutdown of request processor complete
2025-01-01 00:30:47.420 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Revoke previously assigned partitions pong-message-topic-0, pong-message-topic-1
2025-01-01 00:30:47.420 pong-service: partitions revoked: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:30:47.420 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:30:47.420 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:30:47.421 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:30:47.421 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:30:47.421 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:30:47.421 Metrics scheduler closed
2025-01-01 00:30:47.421 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:30:47.421 Metrics reporters closed
2025-01-01 00:30:47.422 App info kafka.consumer for consumer-pong-service-3 unregistered
2025-01-01 00:30:47.422 pong-service: Consumer stopped
2025-01-01 00:30:47.422 [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-01 00:30:47.423 Metrics scheduler closed
2025-01-01 00:30:47.423 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:30:47.423 Metrics reporters closed
2025-01-01 00:30:47.423 App info kafka.producer for producer-2 unregistered
2025-01-01 00:30:47.427 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:30:47.428 HikariPool-3 - Shutdown initiated...
2025-01-01 00:30:47.428 HikariPool-3 - Shutdown completed.
2025-01-01 00:30:47.490 SessionTrackerImpl exited loop!
2025-01-01 00:33:25.215 Could not detect default configuration classes for test class [com.demo.controller.PongControllerSpec]: PongControllerSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:33:25.343 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.controller.PongControllerSpec
2025-01-01 00:33:25.500 Could not detect default configuration classes for test class [com.demo.dao.MessageRepositorySpec]: MessageRepositorySpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:33:25.508 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.dao.MessageRepositorySpec
2025-01-01 00:33:25.521 Could not detect default configuration classes for test class [com.demo.kafka.KafkaSpec]: KafkaSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:33:25.536 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.kafka.KafkaSpec
2025-01-01 00:33:25.566 Could not detect default configuration classes for test class [com.demo.kafka.KafkaSpec]: KafkaSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:33:25.566 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.kafka.KafkaSpec
2025-01-01 00:33:25.573 Could not detect default configuration classes for test class [com.demo.dao.MessageRepositorySpec]: MessageRepositorySpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:33:25.573 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.dao.MessageRepositorySpec
2025-01-01 00:33:25.573 Could not detect default configuration classes for test class [com.demo.controller.PongControllerSpec]: PongControllerSpec does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-01-01 00:33:25.582 Found @SpringBootConfiguration com.demo.PongApplication for test class com.demo.controller.PongControllerSpec
2025-01-01 00:33:26.151 Registered kafka:type=kafka.Log4jController MBean
2025-01-01 00:33:26.213 
2025-01-01 00:33:26.213   ______                  _                                          
2025-01-01 00:33:26.213  |___  /                 | |                                         
2025-01-01 00:33:26.213     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __   
2025-01-01 00:33:26.213    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__|
2025-01-01 00:33:26.213   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |    
2025-01-01 00:33:26.213  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_|
2025-01-01 00:33:26.213                                               | |                     
2025-01-01 00:33:26.213                                               |_|                     
2025-01-01 00:33:26.213 
2025-01-01 00:33:31.408 Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:33:31.408 Server environment:host.name=Joe
2025-01-01 00:33:31.408 Server environment:java.version=17.0.12
2025-01-01 00:33:31.408 Server environment:java.vendor=Oracle Corporation
2025-01-01 00:33:31.408 Server environment:java.home=C:\work\jdk-17
2025-01-01 00:33:31.408 Server environment:java.class.path=C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\work\apache-maven-3.8.8\repo\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\work\apache-maven-3.8.8\repo\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\guava\guava\19.0\guava-19.0.jar;C:\work\apache-maven-3.8.8\repo\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\work\apache-maven-3.8.8\repo\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\work\apache-maven-3.8.8\repo\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\work\apache-maven-3.8.8\repo\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\work\apache-maven-3.8.8\repo\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\work\apache-maven-3.8.8\repo\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\work\apache-maven-3.8.8\repo\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\work\apache-maven-3.8.8\repo\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\work\apache-maven-3.8.8\repo\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\jline\jline\2.14.6\jline-2.14.6.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\junit\junit\4.13.2\junit-4.13.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\work\apache-maven-3.8.8\repo\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\work\apache-maven-3.8.8\repo\org\json\json\20220320\json-20220320.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\work\apache-maven-3.8.8\repo\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\work\apache-maven-3.8.8\repo\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\work\apache-maven-3.8.8\repo\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar;
2025-01-01 00:33:31.408 Server environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:33:31.408 Server environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:33:31.408 Server environment:java.compiler=<NA>
2025-01-01 00:33:31.408 Server environment:os.name=Windows 11
2025-01-01 00:33:31.408 Server environment:os.arch=amd64
2025-01-01 00:33:31.408 Server environment:os.version=10.0
2025-01-01 00:33:31.408 Server environment:user.name=joech
2025-01-01 00:33:31.408 Server environment:user.home=C:\Users\joech
2025-01-01 00:33:31.408 Server environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:33:31.408 Server environment:os.memory.free=500MB
2025-01-01 00:33:31.408 Server environment:os.memory.max=15816MB
2025-01-01 00:33:31.408 Server environment:os.memory.total=568MB
2025-01-01 00:33:31.408 zookeeper.enableEagerACLCheck = false
2025-01-01 00:33:31.408 zookeeper.digest.enabled = true
2025-01-01 00:33:31.408 zookeeper.closeSessionTxn.enabled = true
2025-01-01 00:33:31.408 zookeeper.flushDelay=0
2025-01-01 00:33:31.408 zookeeper.maxWriteQueuePollTime=0
2025-01-01 00:33:31.408 zookeeper.maxBatchSize=1000
2025-01-01 00:33:31.408 zookeeper.intBufferStartingSizeBytes = 1024
2025-01-01 00:33:31.408 zookeeper.snapshot.trust.empty : false
2025-01-01 00:33:31.428 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:33:31.428 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:33:31.428 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:33:31.428 zookeeper.commitLogCount=500
2025-01-01 00:33:31.443 Weighed connection throttling is disabled
2025-01-01 00:33:31.443 minSessionTimeout set to 1600
2025-01-01 00:33:31.443 maxSessionTimeout set to 16000
2025-01-01 00:33:31.443 Response cache size is initialized with value 400.
2025-01-01 00:33:31.443 Response cache size is initialized with value 400.
2025-01-01 00:33:31.443 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:33:31.443 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:33:31.443 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:33:31.443 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:33:31.443 zookeeper.pathStats.delay = 5
2025-01-01 00:33:31.443 zookeeper.pathStats.enabled = false
2025-01-01 00:33:31.443 The max bytes for all large requests are set to 104857600
2025-01-01 00:33:31.443 The large request threshold is set to -1
2025-01-01 00:33:31.443 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-16428003285217762297\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-1465910521374082363\version-2
2025-01-01 00:33:31.487 maxCnxns is not configured, using default value 0.
2025-01-01 00:33:31.487 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:33:31.487 binding to port /127.0.0.1:0
2025-01-01 00:33:31.514 zookeeper.snapshot.compression.method = CHECKED
2025-01-01 00:33:31.514 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-1465910521374082363\version-2\snapshot.0
2025-01-01 00:33:31.519 Snapshot loaded in 18 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:33:31.520 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-1465910521374082363\version-2\snapshot.0
2025-01-01 00:33:31.520 Snapshot taken in 0 ms
2025-01-01 00:33:31.533 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:33:31.534 zookeeper.request_throttler.shutdownTimeout = 10000
2025-01-01 00:33:31.774 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60552
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:33:31.804 Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2025-01-01 00:33:31.871 starting
2025-01-01 00:33:31.871 Connecting to zookeeper on 127.0.0.1:60552
2025-01-01 00:33:31.887 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60552.
2025-01-01 00:33:37.172 Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2025-01-01 00:33:37.172 Client environment:host.name=Joe
2025-01-01 00:33:37.172 Client environment:java.version=17.0.12
2025-01-01 00:33:37.172 Client environment:java.vendor=Oracle Corporation
2025-01-01 00:33:37.172 Client environment:java.home=C:\work\jdk-17
2025-01-01 00:33:37.172 Client environment:java.class.path=C:\code\hsbc\demo\pong\target\test-classes;C:\code\hsbc\demo\pong\target\classes;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-webflux\3.1.4\spring-boot-starter-webflux-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter\3.1.4\spring-boot-starter-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot\3.1.4\spring-boot-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-autoconfigure\3.1.4\spring-boot-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-logging\3.1.4\spring-boot-starter-logging-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-classic\1.4.11\logback-classic-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\ch\qos\logback\logback-core\1.4.11\logback-core-1.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-to-slf4j\2.20.0\log4j-to-slf4j-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\logging\log4j\log4j-api\2.20.0\log4j-api-2.20.0.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\jul-to-slf4j\2.0.9\jul-to-slf4j-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\jakarta\annotation\jakarta.annotation-api\2.1.1\jakarta.annotation-api-2.1.1.jar;C:\work\apache-maven-3.8.8\repo\org\yaml\snakeyaml\1.33\snakeyaml-1.33.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-json\3.1.4\spring-boot-starter-json-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-databind\2.15.2\jackson-databind-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-annotations\2.15.2\jackson-annotations-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\core\jackson-core\2.15.2\jackson-core-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.15.2\jackson-datatype-jdk8-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\datatype\jackson-datatype-jsr310\2.15.2\jackson-datatype-jsr310-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-parameter-names\2.15.2\jackson-module-parameter-names-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-reactor-netty\3.1.4\spring-boot-starter-reactor-netty-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-http\1.1.11\reactor-netty-http-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http\4.1.97.Final\netty-codec-http-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-http2\4.1.97.Final\netty-codec-http2-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns\4.1.97.Final\netty-resolver-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-dns\4.1.97.Final\netty-codec-dns-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-native-macos\4.1.97.Final\netty-resolver-dns-native-macos-4.1.97.Final-osx-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver-dns-classes-macos\4.1.97.Final\netty-resolver-dns-classes-macos-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final-linux-x86_64.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\netty\reactor-netty-core\1.1.11\reactor-netty-core-1.1.11.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler-proxy\4.1.97.Final\netty-handler-proxy-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec-socks\4.1.97.Final\netty-codec-socks-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-web\6.0.12\spring-web-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-beans\6.0.12\spring-beans-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-webflux\6.0.12\spring-webflux-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\projectreactor\reactor-core\3.5.10\reactor-core-3.5.10.jar;C:\work\apache-maven-3.8.8\repo\org\reactivestreams\reactive-streams\1.0.4\reactive-streams-1.0.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\guava\guava\19.0\guava-19.0.jar;C:\work\apache-maven-3.8.8\repo\commons-io\commons-io\2.11.0\commons-io-2.11.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-redis\3.1.4\spring-boot-starter-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-redis\3.1.4\spring-data-redis-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-keyvalue\3.1.4\spring-data-keyvalue-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-oxm\6.0.12\spring-oxm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aop\6.0.12\spring-aop-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context-support\6.0.12\spring-context-support-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\io\lettuce\lettuce-core\6.2.6.RELEASE\lettuce-core-6.2.6.RELEASE.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-common\4.1.97.Final\netty-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-handler\4.1.97.Final\netty-handler-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-resolver\4.1.97.Final\netty-resolver-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-buffer\4.1.97.Final\netty-buffer-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-unix-common\4.1.97.Final\netty-transport-native-unix-common-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-codec\4.1.97.Final\netty-codec-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport\4.1.97.Final\netty-transport-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-data-jpa\3.1.4\spring-boot-starter-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-aop\3.1.4\spring-boot-starter-aop-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\aspectj\aspectjweaver\1.9.20\aspectjweaver-1.9.20.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-jdbc\3.1.4\spring-boot-starter-jdbc-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\zaxxer\HikariCP\5.0.1\HikariCP-5.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jdbc\6.0.12\spring-jdbc-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\orm\hibernate-core\6.2.9.Final\hibernate-core-6.2.9.Final.jar;C:\work\apache-maven-3.8.8\repo\jakarta\persistence\jakarta.persistence-api\3.1.0\jakarta.persistence-api-3.1.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\transaction\jakarta.transaction-api\2.0.1\jakarta.transaction-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\jboss\logging\jboss-logging\3.5.3.Final\jboss-logging-3.5.3.Final.jar;C:\work\apache-maven-3.8.8\repo\org\hibernate\common\hibernate-commons-annotations\6.0.6.Final\hibernate-commons-annotations-6.0.6.Final.jar;C:\work\apache-maven-3.8.8\repo\io\smallrye\jandex\3.0.5\jandex-3.0.5.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\classmate\1.5.1\classmate-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy\1.14.8\byte-buddy-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-runtime\4.0.3\jaxb-runtime-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\jaxb-core\4.0.3\jaxb-core-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\eclipse\angus\angus-activation\2.0.1\angus-activation-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\glassfish\jaxb\txw2\4.0.3\txw2-4.0.3.jar;C:\work\apache-maven-3.8.8\repo\com\sun\istack\istack-commons-runtime\4.1.2\istack-commons-runtime-4.1.2.jar;C:\work\apache-maven-3.8.8\repo\jakarta\inject\jakarta.inject-api\2.0.1\jakarta.inject-api-2.0.1.jar;C:\work\apache-maven-3.8.8\repo\org\antlr\antlr4-runtime\4.10.1\antlr4-runtime-4.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-jpa\3.1.4\spring-data-jpa-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\data\spring-data-commons\3.1.4\spring-data-commons-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-orm\6.0.12\spring-orm-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-aspects\6.0.12\spring-aspects-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\postgresql\postgresql\42.6.0\postgresql-42.6.0.jar;C:\work\apache-maven-3.8.8\repo\org\checkerframework\checker-qual\3.31.0\checker-qual-3.31.0.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka\3.0.11\spring-kafka-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-context\6.0.12\spring-context-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-expression\6.0.12\spring-expression-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-messaging\6.0.12\spring-messaging-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-tx\6.0.12\spring-tx-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\retry\spring-retry\2.0.3\spring-retry-2.0.3.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-observation\1.11.4\micrometer-observation-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\io\micrometer\micrometer-commons\1.11.4\micrometer-commons-1.11.4.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\github\luben\zstd-jni\1.5.2-1\zstd-jni-1.5.2-1.jar;C:\work\apache-maven-3.8.8\repo\org\lz4\lz4-java\1.8.0\lz4-java-1.8.0.jar;C:\work\apache-maven-3.8.8\repo\org\xerial\snappy\snappy-java\1.1.8.4\snappy-java-1.1.8.4.jar;C:\work\apache-maven-3.8.8\repo\org\slf4j\slf4j-api\2.0.9\slf4j-api-2.0.9.jar;C:\work\apache-maven-3.8.8\repo\com\h2database\h2\1.4.200\h2-1.4.200.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-starter-test\3.1.4\spring-boot-starter-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test\3.1.4\spring-boot-test-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\boot\spring-boot-test-autoconfigure\3.1.4\spring-boot-test-autoconfigure-3.1.4.jar;C:\work\apache-maven-3.8.8\repo\com\jayway\jsonpath\json-path\2.8.0\json-path-2.8.0.jar;C:\work\apache-maven-3.8.8\repo\jakarta\xml\bind\jakarta.xml.bind-api\4.0.1\jakarta.xml.bind-api-4.0.1.jar;C:\work\apache-maven-3.8.8\repo\jakarta\activation\jakarta.activation-api\2.1.2\jakarta.activation-api-2.1.2.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\json-smart\2.4.11\json-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\net\minidev\accessors-smart\2.4.11\accessors-smart-2.4.11.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm\9.3\asm-9.3.jar;C:\work\apache-maven-3.8.8\repo\org\assertj\assertj-core\3.24.2\assertj-core-3.24.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest\2.2\hamcrest-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter\5.9.3\junit-jupiter-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-params\5.9.3\junit-jupiter-params-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-engine\5.9.3\junit-jupiter-engine-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-core\5.3.1\mockito-core-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\net\bytebuddy\byte-buddy-agent\1.14.8\byte-buddy-agent-1.14.8.jar;C:\work\apache-maven-3.8.8\repo\org\objenesis\objenesis\3.3\objenesis-3.3.jar;C:\work\apache-maven-3.8.8\repo\org\mockito\mockito-junit-jupiter\5.3.1\mockito-junit-jupiter-5.3.1.jar;C:\work\apache-maven-3.8.8\repo\org\skyscreamer\jsonassert\1.5.1\jsonassert-1.5.1.jar;C:\work\apache-maven-3.8.8\repo\com\vaadin\external\google\android-json\0.0.20131108.vaadin1\android-json-0.0.20131108.vaadin1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-core\6.0.12\spring-core-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-jcl\6.0.12\spring-jcl-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\spring-test\6.0.12\spring-test-6.0.12.jar;C:\work\apache-maven-3.8.8\repo\org\xmlunit\xmlunit-core\2.9.1\xmlunit-core-2.9.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy\4.0.15\groovy-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-ant\4.0.15\groovy-ant-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant\1.10.14\ant-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-junit\1.10.14\ant-junit-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-launcher\1.10.14\ant-launcher-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ant\ant-antlr\1.10.14\ant-antlr-1.10.14.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-cli-picocli\4.0.15\groovy-cli-picocli-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\info\picocli\picocli\4.7.5\picocli-4.7.5.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-console\4.0.15\groovy-console-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\github\javaparser\javaparser-core\3.25.5\javaparser-core-3.25.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-util\9.5\asm-util-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-tree\9.5\asm-tree-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\ow2\asm\asm-analysis\9.5\asm-analysis-9.5.jar;C:\work\apache-maven-3.8.8\repo\org\abego\treelayout\org.abego.treelayout.core\1.0.3\org.abego.treelayout.core-1.0.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\ivy\ivy\2.5.2\ivy-2.5.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-datetime\4.0.15\groovy-datetime-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-docgenerator\4.0.15\groovy-docgenerator-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\qdox\qdox\1.12.1\qdox-1.12.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovydoc\4.0.15\groovy-groovydoc-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-groovysh\4.0.15\groovy-groovysh-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\jline\jline\2.14.6\jline-2.14.6.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jmx\4.0.15\groovy-jmx-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-json\4.0.15\groovy-json-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-jsr223\4.0.15\groovy-jsr223-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-macro\4.0.15\groovy-macro-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-nio\4.0.15\groovy-nio-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-servlet\4.0.15\groovy-servlet-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-sql\4.0.15\groovy-sql-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-swing\4.0.15\groovy-swing-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-templates\4.0.15\groovy-templates-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test\4.0.15\groovy-test-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\junit\junit\4.13.2\junit-4.13.2.jar;C:\work\apache-maven-3.8.8\repo\org\hamcrest\hamcrest-core\2.2\hamcrest-core-2.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-test-junit5\4.0.15\groovy-test-junit5-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-xml\4.0.15\groovy-xml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\org\apache\groovy\groovy-yaml\4.0.15\groovy-yaml-4.0.15.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.15.2\jackson-dataformat-yaml-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-core\2.4-M1-groovy-4.0\spock-core-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-engine\1.9.3\junit-platform-engine-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\opentest4j\opentest4j\1.2.0\opentest4j-1.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-commons\1.9.3\junit-platform-commons-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\apiguardian\apiguardian-api\1.1.2\apiguardian-api-1.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\spockframework\spock-spring\2.4-M1-groovy-4.0\spock-spring-2.4-M1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\athaydes\spock-reports\2.5.1-groovy-4.0\spock-reports-2.5.1-groovy-4.0.jar;C:\work\apache-maven-3.8.8\repo\com\github\codemonstur\embedded-redis\1.4.2\embedded-redis-1.4.2.jar;C:\work\apache-maven-3.8.8\repo\redis\clients\jedis\4.3.2\jedis-4.3.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\commons\commons-pool2\2.11.1\commons-pool2-2.11.1.jar;C:\work\apache-maven-3.8.8\repo\org\json\json\20220320\json-20220320.jar;C:\work\apache-maven-3.8.8\repo\com\google\code\gson\gson\2.10.1\gson-2.10.1.jar;C:\work\apache-maven-3.8.8\repo\org\springframework\kafka\spring-kafka-test\3.0.11\spring-kafka-test-3.0.11.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper\3.6.3\zookeeper-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\zookeeper\zookeeper-jute\3.6.3\zookeeper-jute-3.6.3.jar;C:\work\apache-maven-3.8.8\repo\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-native-epoll\4.1.97.Final\netty-transport-native-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\io\netty\netty-transport-classes-epoll\4.1.97.Final\netty-transport-classes-epoll-4.1.97.Final.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-clients\3.4.1\kafka-clients-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-metadata\3.4.1\kafka-metadata-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-server-common\3.4.1\kafka-server-common-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-raft\3.4.1\kafka-raft-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams-test-utils\3.4.1\kafka-streams-test-utils-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-streams\3.4.1\kafka-streams-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\rocksdb\rocksdbjni\7.1.2\rocksdbjni-7.1.2.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-library\2.13.10\scala-library-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-group-coordinator\3.4.1\kafka-group-coordinator-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage\3.4.1\kafka-storage-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka-storage-api\3.4.1\kafka-storage-api-3.4.1.jar;C:\work\apache-maven-3.8.8\repo\net\sourceforge\argparse4j\argparse4j\0.7.0\argparse4j-0.7.0.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\module\jackson-module-scala_2.13\2.15.2\jackson-module-scala_2.13-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\work\apache-maven-3.8.8\repo\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.15.2\jackson-dataformat-csv-2.15.2.jar;C:\work\apache-maven-3.8.8\repo\net\sf\jopt-simple\jopt-simple\5.0.4\jopt-simple-5.0.4.jar;C:\work\apache-maven-3.8.8\repo\org\bitbucket\b_c\jose4j\0.9.3\jose4j-0.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-collection-compat_2.13\2.10.0\scala-collection-compat_2.13-2.10.0.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\modules\scala-java8-compat_2.13\1.0.2\scala-java8-compat_2.13-1.0.2.jar;C:\work\apache-maven-3.8.8\repo\org\scala-lang\scala-reflect\2.13.10\scala-reflect-2.13.10.jar;C:\work\apache-maven-3.8.8\repo\com\typesafe\scala-logging\scala-logging_2.13\3.9.4\scala-logging_2.13-3.9.4.jar;C:\work\apache-maven-3.8.8\repo\io\dropwizard\metrics\metrics-core\4.2.19\metrics-core-4.2.19.jar;C:\work\apache-maven-3.8.8\repo\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\work\apache-maven-3.8.8\repo\org\apache\kafka\kafka_2.13\3.4.1\kafka_2.13-3.4.1-test.jar;C:\work\apache-maven-3.8.8\repo\org\junit\jupiter\junit-jupiter-api\5.9.3\junit-jupiter-api-5.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\junit\platform\junit-platform-launcher\1.9.3\junit-platform-launcher-1.9.3.jar;C:\work\apache-maven-3.8.8\repo\org\awaitility\awaitility\4.2.0\awaitility-4.2.0.jar;
2025-01-01 00:33:37.173 Client environment:java.library.path=C:\work\jdk-17\bin;C:\windows\Sun\Java\bin;C:\windows\system32;C:\windows;C:\work\python312\Scripts\;C:\work\python312\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program\Git\cmd;C:\Program\PowerShell\7\;C:\Program Files\dotnet\;C:\Users\joech\AppData\Local\Microsoft\WindowsApps;C:\Users\joech\AppData\Local\GitHubDesktop\bin;C:\Program\Microsoft VS Code\bin;C:\work\apache-maven-3.8.8\bin;C:\Program Files\Tesseract-OCR;C:\work\jdk-17\bin;;.
2025-01-01 00:33:37.174 Client environment:java.io.tmpdir=C:\Users\joech\AppData\Local\Temp\
2025-01-01 00:33:37.175 Client environment:java.compiler=<NA>
2025-01-01 00:33:37.175 Client environment:os.name=Windows 11
2025-01-01 00:33:37.175 Client environment:os.arch=amd64
2025-01-01 00:33:37.175 Client environment:os.version=10.0
2025-01-01 00:33:37.175 Client environment:user.name=joech
2025-01-01 00:33:37.175 Client environment:user.home=C:\Users\joech
2025-01-01 00:33:37.175 Client environment:user.dir=C:\code\hsbc\demo\pong
2025-01-01 00:33:37.175 Client environment:os.memory.free=90MB
2025-01-01 00:33:37.176 Client environment:os.memory.max=15816MB
2025-01-01 00:33:37.176 Client environment:os.memory.total=160MB
2025-01-01 00:33:37.180 Initiating client connection, connectString=127.0.0.1:60552 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@536e3aab
2025-01-01 00:33:37.184 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:33:37.188 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:33:37.189 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:33:37.189 Opening socket connection to server /127.0.0.1:60552.
2025-01-01 00:33:37.191 Socket connection established, initiating session, client: /127.0.0.1:60553, server: /127.0.0.1:60552
2025-01-01 00:33:37.197 Creating new log file: log.1
2025-01-01 00:33:37.204 ZooKeeper audit is disabled.
2025-01-01 00:33:37.207 Session establishment complete on server /127.0.0.1:60552, session id = 0x100182bce420000, negotiated timeout = 16000
2025-01-01 00:33:37.210 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:33:37.489 Cluster ID = eKpWte7yQ32FGwrk0Kut3Q
2025-01-01 00:33:37.505 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023\meta.properties
2025-01-01 00:33:37.543 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60552
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:33:37.552 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:33:37.552 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:33:37.552 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:33:37.568 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:33:37.584 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023)
2025-01-01 00:33:37.596 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023 since no clean shutdown file was found
2025-01-01 00:33:37.606 Loaded 0 logs in 13ms.
2025-01-01 00:33:37.606 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:33:37.606 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:33:37.615 Starting the log cleaner
2025-01-01 00:33:37.631 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:33:37.643 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:33:37.656 Feature ZK node at path: /feature does not exist
2025-01-01 00:33:37.679 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:33:38.064 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:33:38.072 Awaiting socket connections on localhost:60554.
2025-01-01 00:33:38.105 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:33:38.111 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:33:38.127 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:33:38.127 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:33:38.128 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:33:38.129 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:33:38.144 [LogDirFailureHandler]: Starting
2025-01-01 00:33:38.166 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:33:38.185 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662818178,1735662818178,1,0,0,72084170461020160,204,0,25

2025-01-01 00:33:38.187 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60554, czxid (broker epoch): 25
2025-01-01 00:33:38.247 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:33:38.252 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:33:38.260 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:33:38.261 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:33:38.270 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:33:38.281 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:33:38.286 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:33:38.290 Feature ZK node created at path: /feature
2025-01-01 00:33:38.290 [GroupCoordinator 0]: Starting up.
2025-01-01 00:33:38.299 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:33:38.325 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:33:38.328 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:33:38.328 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:33:38.338 [Controller id=0] Registering handlers
2025-01-01 00:33:38.338 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:33:38.343 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:33:38.345 [Controller id=0] Deleting isr change notifications
2025-01-01 00:33:38.347 [Controller id=0] Initializing controller context
2025-01-01 00:33:38.365 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:33:38.370 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:33:38.391 [RequestSendThread controllerId=0] Starting
2025-01-01 00:33:38.393 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:33:38.394 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:33:38.394 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:33:38.394 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:33:38.398 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:33:38.398 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:33:38.399 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:33:38.399 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:33:38.402 [Controller id=0] Sending update metadata request
2025-01-01 00:33:38.405 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:33:38.414 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:33:38.414 [/config/changes-event-process-thread]: Starting
2025-01-01 00:33:38.415 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:33:38.421 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:33:38.422 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:33:38.422 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:33:38.422 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:33:38.425 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60554 (id: 0 rack: null) for sending state change requests
2025-01-01 00:33:38.427 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:33:38.427 Kafka version: 3.4.1
2025-01-01 00:33:38.427 Kafka commitId: 8a516edc2755df89
2025-01-01 00:33:38.427 Kafka startTimeMs: 1735662818424
2025-01-01 00:33:38.429 [KafkaServer id=0] started
2025-01-01 00:33:38.433 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:33:38.433 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:33:38.433 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:33:38.435 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:33:38.436 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:33:38.439 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60554]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:33:38.453 [Controller id=0] Starting the controller scheduler
2025-01-01 00:33:38.491 Kafka version: 3.4.1
2025-01-01 00:33:38.491 Kafka commitId: 8a516edc2755df89
2025-01-01 00:33:38.491 Kafka startTimeMs: 1735662818491
2025-01-01 00:33:38.512 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60554 (id: 0 rack: null)
2025-01-01 00:33:38.512 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60554 (id: 0 rack: null)
2025-01-01 00:33:38.613 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0))
2025-01-01 00:33:38.638 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(q0DO5SncT2eBOa6FPyn2sg),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), pong-message-topic-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:33:38.640 [Controller id=0] New partition creation callback for pong-message-topic-0,pong-message-topic-1
2025-01-01 00:33:38.642 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:38.642 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:38.642 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:38.646 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:38.667 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:38.667 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:38.669 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions
2025-01-01 00:33:38.671 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 2 partitions
2025-01-01 00:33:38.672 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:38.676 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:33:38.716 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0, pong-message-topic-1)
2025-01-01 00:33:38.718 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
2025-01-01 00:33:38.793 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:38.807 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023\pong-message-topic-0 with properties {}
2025-01-01 00:33:38.808 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:33:38.808 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:33:38.810 [Broker id=0] Leader pong-message-topic-0 with topic id Some(q0DO5SncT2eBOa6FPyn2sg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:38.821 [LogLoader partition=pong-message-topic-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:38.822 Created log for partition pong-message-topic-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023\pong-message-topic-1 with properties {}
2025-01-01 00:33:38.822 [Partition pong-message-topic-1 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-1
2025-01-01 00:33:38.822 [Partition pong-message-topic-1 broker=0] Log loaded for partition pong-message-topic-1 with initial high watermark 0
2025-01-01 00:33:38.822 [Broker id=0] Leader pong-message-topic-1 with topic id Some(q0DO5SncT2eBOa6FPyn2sg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:38.830 [Broker id=0] Finished LeaderAndIsr request in 155ms correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:33:38.836 [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:33:38.840 App info kafka.admin.client for adminclient-1 unregistered
2025-01-01 00:33:38.843 Metrics scheduler closed
2025-01-01 00:33:38.844 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:33:38.844 Metrics reporters closed
2025-01-01 00:33:39.089 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/work/apache-maven-3.8.8/repo/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/work/apache-maven-3.8.8/repo/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:33:39.109 Starting KafkaSpec using Java 17.0.12 with PID 8840 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:33:39.109 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:33:39.744 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:33:39.760 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:33:39.891 Finished Spring Data repository scanning in 121 ms. Found 1 JPA repository interfaces.
2025-01-01 00:33:39.912 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:33:39.918 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:33:39.930 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:33:39.930 Finished Spring Data repository scanning in 6 ms. Found 0 Redis repository interfaces.
2025-01-01 00:33:40.360 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:33:40.428 HHH000412: Hibernate ORM core version 6.2.9.Final
2025-01-01 00:33:40.433 HHH000406: Using bytecode reflection optimizer
2025-01-01 00:33:40.601 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:33:40.746 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:33:40.770 HikariPool-1 - Starting...
2025-01-01 00:33:41.042 HikariPool-1 - Added connection conn0: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:33:41.045 HikariPool-1 - Start completed.
2025-01-01 00:33:41.324 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:33:42.031 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:33:42.114 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:33:43.329 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60554]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:33:43.376 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:33:43.376 Kafka version: 3.4.1
2025-01-01 00:33:43.376 Kafka commitId: 8a516edc2755df89
2025-01-01 00:33:43.376 Kafka startTimeMs: 1735662823376
2025-01-01 00:33:43.376 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:33:43.398 Started KafkaSpec in 4.529 seconds (process running for 19.986)
2025-01-01 00:33:43.413 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to q0DO5SncT2eBOa6FPyn2sg
2025-01-01 00:33:43.413 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to q0DO5SncT2eBOa6FPyn2sg
2025-01-01 00:33:43.415 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cluster ID: eKpWte7yQ32FGwrk0Kut3Q
2025-01-01 00:33:43.415 Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-01-01 00:33:43.415 [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(vQnuRHv5RYedghnDTkLg-w),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:33:43.415 [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-01-01 00:33:43.415 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:43.415 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:43.415 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:43.415 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:43.415 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:43.415 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:43.415 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:43.431 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:43.431 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:43.431 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:43.431 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:43.431 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:43.431 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-01-01 00:33:43.432 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-01-01 00:33:43.432 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:43.432 [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:33:43.432 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-01-01 00:33:43.432 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-01-01 00:33:43.432 [LogLoader partition=__consumer_offsets-3, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:43.432 Created log for partition __consumer_offsets-3 in C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023\__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:33:43.446 [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-01-01 00:33:43.446 [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-01-01 00:33:43.446 [Broker id=0] Leader __consumer_offsets-3 with topic id Some(vQnuRHv5RYedghnDTkLg-w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:43.450 [LogLoader partition=__consumer_offsets-2, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:43.450 Created log for partition __consumer_offsets-2 in C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023\__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:33:43.450 [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-01-01 00:33:43.450 [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-01-01 00:33:43.450 [Broker id=0] Leader __consumer_offsets-2 with topic id Some(vQnuRHv5RYedghnDTkLg-w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:43.462 [Controller id=0] Processing automatic preferred replica leader election
2025-01-01 00:33:43.462 [LogLoader partition=__consumer_offsets-4, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:43.462 Created log for partition __consumer_offsets-4 in C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023\__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:33:43.462 [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-01-01 00:33:43.462 [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-01-01 00:33:43.462 [Broker id=0] Leader __consumer_offsets-4 with topic id Some(vQnuRHv5RYedghnDTkLg-w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:43.462 [LogLoader partition=__consumer_offsets-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:43.462 Created log for partition __consumer_offsets-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023\__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:33:43.462 [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-01-01 00:33:43.462 [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-01-01 00:33:43.462 [Broker id=0] Leader __consumer_offsets-1 with topic id Some(vQnuRHv5RYedghnDTkLg-w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:43.478 [LogLoader partition=__consumer_offsets-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:43.478 Created log for partition __consumer_offsets-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.ee83e16f-4dfa-4292-b1af-5392da67c32116340623104317431023\__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:33:43.478 [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-01-01 00:33:43.478 [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-01-01 00:33:43.478 [Broker id=0] Leader __consumer_offsets-0 with topic id Some(vQnuRHv5RYedghnDTkLg-w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:43.483 [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-01-01 00:33:43.483 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-01-01 00:33:43.483 [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-01-01 00:33:43.483 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-01-01 00:33:43.483 [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-01-01 00:33:43.483 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-01-01 00:33:43.483 [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-01-01 00:33:43.483 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-01-01 00:33:43.483 [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-01-01 00:33:43.483 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-01-01 00:33:43.483 [Broker id=0] Finished LeaderAndIsr request in 51ms correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:33:43.483 [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-01-01 00:33:43.483 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:33:43.483 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:33:43.483 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:33:43.483 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:33:43.483 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:33:43.514 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Discovered group coordinator localhost:60554 (id: 2147483647 rack: null)
2025-01-01 00:33:43.516 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:33:43.531 [GroupCoordinator 0]: Dynamic member with unknown member id joins group pong-service in Empty state. Created a new member id consumer-pong-service-1-b89c9eac-8951-477f-a910-8eadb2494fa1 and request the member to rejoin with this id.
2025-01-01 00:33:43.534 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: need to re-join with the given member-id: consumer-pong-service-1-b89c9eac-8951-477f-a910-8eadb2494fa1
2025-01-01 00:33:43.535 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2025-01-01 00:33:43.535 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] (Re-)joining group
2025-01-01 00:33:43.541 [GroupCoordinator 0]: Preparing to rebalance group pong-service in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-pong-service-1-b89c9eac-8951-477f-a910-8eadb2494fa1 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)
2025-01-01 00:33:43.546 [GroupCoordinator 0]: Stabilized group pong-service generation 1 (__consumer_offsets-2) with 1 members
2025-01-01 00:33:43.548 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully joined group with generation Generation{generationId=1, memberId='consumer-pong-service-1-b89c9eac-8951-477f-a910-8eadb2494fa1', protocol='range'}
2025-01-01 00:33:43.549 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Finished assignment for group at generation 1: {consumer-pong-service-1-b89c9eac-8951-477f-a910-8eadb2494fa1=Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])}
2025-01-01 00:33:43.554 [GroupCoordinator 0]: Assignment received from leader consumer-pong-service-1-b89c9eac-8951-477f-a910-8eadb2494fa1 for group pong-service for generation 1. The group has 1 members, 0 of which are static.
2025-01-01 00:33:43.591 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Successfully synced group in generation Generation{generationId=1, memberId='consumer-pong-service-1-b89c9eac-8951-477f-a910-8eadb2494fa1', protocol='range'}
2025-01-01 00:33:43.592 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Notifying assignor about the new Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])
2025-01-01 00:33:43.594 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Adding newly assigned partitions: pong-message-topic-0, pong-message-topic-1
2025-01-01 00:33:43.603 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:33:43.603 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:33:43.606 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:33:43.607 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:33:43.619 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60554 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:33:43.619 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting offset for partition pong-message-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60554 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:33:43.635 pong-service: partitions assigned: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:33:43.955 Idempotence will be disabled because acks is set to 0, not set to 'all'.
2025-01-01 00:33:43.955 ProducerConfig values: 
	acks = 0
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:60554]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-01-01 00:33:43.969 Kafka version: 3.4.1
2025-01-01 00:33:43.969 Kafka commitId: 8a516edc2755df89
2025-01-01 00:33:43.969 Kafka startTimeMs: 1735662823969
2025-01-01 00:33:43.973 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to q0DO5SncT2eBOa6FPyn2sg
2025-01-01 00:33:43.973 [Producer clientId=producer-1] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to q0DO5SncT2eBOa6FPyn2sg
2025-01-01 00:33:43.973 [Producer clientId=producer-1] Cluster ID: eKpWte7yQ32FGwrk0Kut3Q
2025-01-01 00:33:44.124 [KafkaServer id=0] shutting down
2025-01-01 00:33:44.125 [KafkaServer id=0] Starting controlled shutdown
2025-01-01 00:33:44.128 [Controller id=0] Shutting down broker 0
2025-01-01 00:33:44.131 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:44.133 [KafkaServer id=0] Controlled shutdown request returned successfully after 5ms
2025-01-01 00:33:44.133 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:33:44.134 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:33:44.134 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:33:44.134 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:33:44.135 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:33:44.137 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:33:44.137 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Cancelled in-flight FETCH request with correlation id 15 due to node 0 being disconnected (elapsed time since creation: 122ms, elapsed time since send: 122ms, request timeout: 30000ms)
2025-01-01 00:33:44.137 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 2147483647 disconnected.
2025-01-01 00:33:44.137 [Producer clientId=producer-1] Node -1 disconnected.
2025-01-01 00:33:44.137 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:33:44.138 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Group coordinator localhost:60554 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2025-01-01 00:33:44.138 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:33:44.139 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:33:44.137 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Error sending fetch request (sessionId=983816535, epoch=1) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
2025-01-01 00:33:44.139 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:33:44.141 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:33:44.142 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:33:44.142 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:33:44.143 [KafkaApi-0] Shutdown complete.
2025-01-01 00:33:44.143 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:33:44.144 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:33:44.144 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:33:44.145 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:33:44.145 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:33:44.145 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:33:44.145 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:33:44.145 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:33:44.146 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:33:44.146 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:33:44.146 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:33:44.146 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:33:44.146 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:33:44.146 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:33:44.146 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:33:44.146 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:33:44.148 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:33:44.148 [ReplicaManager broker=0] Shutting down
2025-01-01 00:33:44.148 [LogDirFailureHandler]: Shutting down
2025-01-01 00:33:44.148 [LogDirFailureHandler]: Stopped
2025-01-01 00:33:44.148 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:33:44.148 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:33:44.148 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:33:44.149 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:33:44.149 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:33:44.149 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:33:44.149 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:33:44.149 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:33:44.149 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:33:44.149 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:33:44.149 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:33:44.149 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:33:44.149 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:33:44.149 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:33:44.149 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:33:44.150 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:33:44.150 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:33:44.156 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:33:44.156 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:33:44.156 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:33:44.156 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:33:44.157 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:33:44.157 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:33:44.158 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:33:44.158 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:33:44.158 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:33:44.158 Shutting down.
2025-01-01 00:33:44.158 Shutting down the log cleaner.
2025-01-01 00:33:44.159 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:33:44.159 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:33:44.159 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:33:44.180 [ProducerStateManager partition=pong-message-topic-1] Wrote producer snapshot at offset 1 with 0 producer ids in 2 ms.
2025-01-01 00:33:44.188 [ProducerStateManager partition=__consumer_offsets-2] Wrote producer snapshot at offset 4 with 0 producer ids in 2 ms.
2025-01-01 00:33:44.199 Shutdown complete.
2025-01-01 00:33:44.199 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:33:44.199 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:33:44.199 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:33:44.200 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:33:44.201 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:33:44.201 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:33:44.201 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:33:44.201 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:33:44.202 [Controller id=0] Resigned
2025-01-01 00:33:44.202 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:33:44.202 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:33:44.202 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:33:44.202 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:33:44.237 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:33:44.237 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60554) could not be established. Broker may not be available.
2025-01-01 00:33:44.239 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:33:44.239 [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:60554) could not be established. Broker may not be available.
2025-01-01 00:33:44.307 Session: 0x100182bce420000 closed
2025-01-01 00:33:44.307 EventThread shut down for session: 0x100182bce420000
2025-01-01 00:33:44.308 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:33:44.308 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:33:44.311 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:33:44.311 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:33:44.311 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:33:44.311 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:33:44.311 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:33:44.311 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:33:44.311 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:33:44.311 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:33:44.311 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:33:44.311 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:33:44.311 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:33:44.312 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:33:44.322 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:33:44.323 Metrics scheduler closed
2025-01-01 00:33:44.323 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:33:44.323 Metrics reporters closed
2025-01-01 00:33:44.323 Broker and topic stats closed
2025-01-01 00:33:44.323 App info kafka.server for 0 unregistered
2025-01-01 00:33:44.324 [KafkaServer id=0] shut down completed
2025-01-01 00:33:44.333 ConnnectionExpirerThread interrupted
2025-01-01 00:33:44.333 selector thread exitted run method
2025-01-01 00:33:44.333 accept thread exitted run method
2025-01-01 00:33:44.333 selector thread exitted run method
2025-01-01 00:33:44.333 shutting down
2025-01-01 00:33:44.333 Shutting down
2025-01-01 00:33:44.333 Draining request throttler queue
2025-01-01 00:33:44.333 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:33:44.335 Shutting down
2025-01-01 00:33:44.335 Shutting down
2025-01-01 00:33:44.335 Shutting down
2025-01-01 00:33:44.335 PrepRequestProcessor exited loop!
2025-01-01 00:33:44.335 SyncRequestProcessor exited!
2025-01-01 00:33:44.335 shutdown of request processor complete
2025-01-01 00:33:44.339 [Producer clientId=producer-1] Node 0 disconnected.
2025-01-01 00:33:44.339 [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:60554) could not be established. Broker may not be available.
2025-01-01 00:33:44.349 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Revoke previously assigned partitions pong-message-topic-0, pong-message-topic-1
2025-01-01 00:33:44.349 pong-service: partitions revoked: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:33:44.349 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:33:44.349 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:33:44.349 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:33:44.349 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:33:44.349 [Consumer clientId=consumer-pong-service-1, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:33:44.349 Metrics scheduler closed
2025-01-01 00:33:44.349 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:33:44.349 Metrics reporters closed
2025-01-01 00:33:44.351 App info kafka.consumer for consumer-pong-service-1 unregistered
2025-01-01 00:33:44.351 pong-service: Consumer stopped
2025-01-01 00:33:44.351 [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-01 00:33:44.353 Metrics scheduler closed
2025-01-01 00:33:44.353 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:33:44.353 Metrics reporters closed
2025-01-01 00:33:44.353 App info kafka.producer for producer-1 unregistered
2025-01-01 00:33:44.364 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:33:44.375 HikariPool-1 - Shutdown initiated...
2025-01-01 00:33:44.375 HikariPool-1 - Shutdown completed.
2025-01-01 00:33:44.412 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/work/apache-maven-3.8.8/repo/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/work/apache-maven-3.8.8/repo/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:33:44.412 Starting MessageRepositorySpec using Java 17.0.12 with PID 8840 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:33:44.412 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:33:44.503 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:33:44.503 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:33:44.512 Finished Spring Data repository scanning in 9 ms. Found 1 JPA repository interfaces.
2025-01-01 00:33:44.512 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:33:44.512 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:33:44.518 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:33:44.518 Finished Spring Data repository scanning in 3 ms. Found 0 Redis repository interfaces.
2025-01-01 00:33:44.570 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:33:44.582 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:33:44.582 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:33:44.582 HikariPool-2 - Starting...
2025-01-01 00:33:44.588 HikariPool-2 - Added connection conn10: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:33:44.588 HikariPool-2 - Start completed.
2025-01-01 00:33:44.597 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:33:44.617 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:33:44.632 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:33:44.788 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60554]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:33:44.796 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:33:44.796 Kafka version: 3.4.1
2025-01-01 00:33:44.796 Kafka commitId: 8a516edc2755df89
2025-01-01 00:33:44.796 Kafka startTimeMs: 1735662824796
2025-01-01 00:33:44.796 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:33:44.796 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:33:44.796 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Connection to node -1 (/127.0.0.1:60554) could not be established. Broker may not be available.
2025-01-01 00:33:44.796 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Bootstrap broker 127.0.0.1:60554 (id: -1 rack: null) disconnected
2025-01-01 00:33:44.796 Started MessageRepositorySpec in 0.407 seconds (process running for 21.381)
2025-01-01 00:33:44.837 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:33:44.837 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:33:44.837 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:33:44.837 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:33:44.837 [Consumer clientId=consumer-pong-service-2, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:33:44.837 Metrics scheduler closed
2025-01-01 00:33:44.837 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:33:44.837 Metrics reporters closed
2025-01-01 00:33:44.839 App info kafka.consumer for consumer-pong-service-2 unregistered
2025-01-01 00:33:44.839 pong-service: Consumer stopped
2025-01-01 00:33:44.841 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:33:44.841 HikariPool-2 - Shutdown initiated...
2025-01-01 00:33:44.843 HikariPool-2 - Shutdown completed.
2025-01-01 00:33:44.863 zookeeper.snapshot.trust.empty : false
2025-01-01 00:33:44.863 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:33:44.863 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:33:44.863 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:33:44.863 zookeeper.commitLogCount=500
2025-01-01 00:33:44.863 minSessionTimeout set to 1600
2025-01-01 00:33:44.863 maxSessionTimeout set to 16000
2025-01-01 00:33:44.863 Response cache size is initialized with value 400.
2025-01-01 00:33:44.863 Response cache size is initialized with value 400.
2025-01-01 00:33:44.863 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:33:44.863 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:33:44.863 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:33:44.863 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:33:44.863 zookeeper.pathStats.delay = 5
2025-01-01 00:33:44.863 zookeeper.pathStats.enabled = false
2025-01-01 00:33:44.863 The max bytes for all large requests are set to 104857600
2025-01-01 00:33:44.863 The large request threshold is set to -1
2025-01-01 00:33:44.863 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-9730016453629627546\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-12390872581075182078\version-2
2025-01-01 00:33:44.863 maxCnxns is not configured, using default value 0.
2025-01-01 00:33:44.863 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:33:44.867 binding to port /127.0.0.1:0
2025-01-01 00:33:44.871 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-12390872581075182078\version-2\snapshot.0
2025-01-01 00:33:44.871 Snapshot loaded in 1 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:33:44.872 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-12390872581075182078\version-2\snapshot.0
2025-01-01 00:33:44.872 Snapshot taken in 1 ms
2025-01-01 00:33:44.872 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:33:44.874 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60569
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:33:44.877 starting
2025-01-01 00:33:44.878 Connecting to zookeeper on 127.0.0.1:60569
2025-01-01 00:33:44.878 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60569.
2025-01-01 00:33:44.878 Initiating client connection, connectString=127.0.0.1:60569 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@180dd8a9
2025-01-01 00:33:44.879 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:33:44.880 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:33:44.880 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:33:44.880 Opening socket connection to server /127.0.0.1:60569.
2025-01-01 00:33:44.880 Socket connection established, initiating session, client: /127.0.0.1:60570, server: /127.0.0.1:60569
2025-01-01 00:33:44.882 Creating new log file: log.1
2025-01-01 00:33:44.886 Session establishment complete on server /127.0.0.1:60569, session id = 0x100182c02680000, negotiated timeout = 16000
2025-01-01 00:33:44.886 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:33:44.923 Cluster ID = 6rkPX64FT-udWfqvJqeuSg
2025-01-01 00:33:44.923 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654\meta.properties
2025-01-01 00:33:44.931 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60569
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:33:44.936 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:33:44.938 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:33:44.938 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:33:44.938 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:33:44.942 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654)
2025-01-01 00:33:44.942 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654 since no clean shutdown file was found
2025-01-01 00:33:44.943 Loaded 0 logs in 1ms.
2025-01-01 00:33:44.943 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:33:44.944 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:33:44.945 Starting the log cleaner
2025-01-01 00:33:44.947 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:33:44.947 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:33:44.948 Feature ZK node at path: /feature does not exist
2025-01-01 00:33:44.951 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:33:44.965 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:33:44.966 Awaiting socket connections on localhost:60571.
2025-01-01 00:33:44.972 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:33:44.974 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:33:44.974 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:33:44.974 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:33:44.976 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:33:44.976 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:33:44.977 [LogDirFailureHandler]: Starting
2025-01-01 00:33:44.978 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:33:44.981 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662824980,1735662824980,1,0,0,72084171335925760,204,0,25

2025-01-01 00:33:44.981 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60571, czxid (broker epoch): 25
2025-01-01 00:33:44.985 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:33:44.986 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:33:44.986 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:33:44.986 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:33:44.986 [GroupCoordinator 0]: Starting up.
2025-01-01 00:33:44.987 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:33:44.989 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:33:44.990 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:33:44.990 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:33:44.990 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:33:44.990 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:33:44.991 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:33:44.991 Feature ZK node created at path: /feature
2025-01-01 00:33:44.993 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:33:44.993 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:33:44.993 [Controller id=0] Registering handlers
2025-01-01 00:33:44.994 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:33:44.995 [/config/changes-event-process-thread]: Starting
2025-01-01 00:33:44.995 [Controller id=0] Deleting isr change notifications
2025-01-01 00:33:44.996 [Controller id=0] Initializing controller context
2025-01-01 00:33:44.997 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:33:44.997 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:33:44.999 Kafka version: 3.4.1
2025-01-01 00:33:44.999 Kafka commitId: 8a516edc2755df89
2025-01-01 00:33:44.999 Kafka startTimeMs: 1735662824999
2025-01-01 00:33:44.999 [KafkaServer id=0] started
2025-01-01 00:33:44.999 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60571]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:33:45.002 Kafka version: 3.4.1
2025-01-01 00:33:45.003 Kafka commitId: 8a516edc2755df89
2025-01-01 00:33:45.003 Kafka startTimeMs: 1735662825002
2025-01-01 00:33:45.003 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:33:45.003 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:33:45.003 [RequestSendThread controllerId=0] Starting
2025-01-01 00:33:45.003 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:33:45.003 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:33:45.004 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:33:45.004 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:33:45.004 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:33:45.004 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:33:45.004 [Controller id=0] Sending update metadata request
2025-01-01 00:33:45.004 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:33:45.004 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:33:45.004 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:33:45.004 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:33:45.004 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:33:45.004 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:33:45.004 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:33:45.004 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60571 (id: 0 rack: null) for sending state change requests
2025-01-01 00:33:45.005 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:33:45.005 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:33:45.005 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:33:45.005 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:33:45.005 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:33:45.007 [Controller id=0] Starting the controller scheduler
2025-01-01 00:33:45.011 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0))
2025-01-01 00:33:45.016 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(FZcDAS02TW62SFYRy8RIPg),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), pong-message-topic-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:33:45.016 [Controller id=0] New partition creation callback for pong-message-topic-0,pong-message-topic-1
2025-01-01 00:33:45.016 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:45.016 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:45.016 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:45.016 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:45.021 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:45.021 [Controller id=0 epoch=1] Changed partition pong-message-topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:45.021 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions
2025-01-01 00:33:45.021 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 2 partitions
2025-01-01 00:33:45.021 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:45.021 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:33:45.023 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0, pong-message-topic-1)
2025-01-01 00:33:45.023 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
2025-01-01 00:33:45.028 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:45.029 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654\pong-message-topic-0 with properties {}
2025-01-01 00:33:45.029 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:33:45.029 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:33:45.029 [Broker id=0] Leader pong-message-topic-0 with topic id Some(FZcDAS02TW62SFYRy8RIPg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:45.035 [LogLoader partition=pong-message-topic-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:45.035 Created log for partition pong-message-topic-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654\pong-message-topic-1 with properties {}
2025-01-01 00:33:45.035 [Partition pong-message-topic-1 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-1
2025-01-01 00:33:45.035 [Partition pong-message-topic-1 broker=0] Log loaded for partition pong-message-topic-1 with initial high watermark 0
2025-01-01 00:33:45.035 [Broker id=0] Leader pong-message-topic-1 with topic id Some(FZcDAS02TW62SFYRy8RIPg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:45.037 [Broker id=0] Finished LeaderAndIsr request in 16ms correlationId 1 from controller 0 for 2 partitions
2025-01-01 00:33:45.040 [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:33:45.042 App info kafka.admin.client for adminclient-2 unregistered
2025-01-01 00:33:45.042 Metrics scheduler closed
2025-01-01 00:33:45.043 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:33:45.043 Metrics reporters closed
2025-01-01 00:33:45.051 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60571 (id: 0 rack: null)
2025-01-01 00:33:45.074 

Found multiple occurrences of org.json.JSONObject on the class path:

	jar:file:/C:/work/apache-maven-3.8.8/repo/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar!/org/json/JSONObject.class
	jar:file:/C:/work/apache-maven-3.8.8/repo/org/json/json/20220320/json-20220320.jar!/org/json/JSONObject.class

You may wish to exclude one of them to ensure predictable runtime behavior

2025-01-01 00:33:45.076 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60571 (id: 0 rack: null)
2025-01-01 00:33:45.078 zookeeper.snapshot.trust.empty : false
2025-01-01 00:33:45.079 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:33:45.079 Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-01-01 00:33:45.079 zookeeper.snapshotSizeFactor = 0.33
2025-01-01 00:33:45.079 zookeeper.commitLogCount=500
2025-01-01 00:33:45.079 minSessionTimeout set to 1600
2025-01-01 00:33:45.079 maxSessionTimeout set to 16000
2025-01-01 00:33:45.080 Response cache size is initialized with value 400.
2025-01-01 00:33:45.080 Response cache size is initialized with value 400.
2025-01-01 00:33:45.080 zookeeper.pathStats.slotCapacity = 60
2025-01-01 00:33:45.080 zookeeper.pathStats.slotDuration = 15
2025-01-01 00:33:45.080 zookeeper.pathStats.maxDepth = 6
2025-01-01 00:33:45.080 zookeeper.pathStats.initialDelay = 5
2025-01-01 00:33:45.080 zookeeper.pathStats.delay = 5
2025-01-01 00:33:45.080 zookeeper.pathStats.enabled = false
2025-01-01 00:33:45.080 The max bytes for all large requests are set to 104857600
2025-01-01 00:33:45.080 The large request threshold is set to -1
2025-01-01 00:33:45.080 Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir C:\Users\joech\AppData\Local\Temp\kafka-10784591601872313518\version-2 snapdir C:\Users\joech\AppData\Local\Temp\kafka-10956541561290772185\version-2
2025-01-01 00:33:45.080 maxCnxns is not configured, using default value 0.
2025-01-01 00:33:45.080 Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers.
2025-01-01 00:33:45.082 binding to port /127.0.0.1:0
2025-01-01 00:33:45.083 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-10956541561290772185\version-2\snapshot.0
2025-01-01 00:33:45.083 Snapshot loaded in 1 ms, highest zxid is 0x0, digest is 1371985504
2025-01-01 00:33:45.083 Snapshotting: 0x0 to C:\Users\joech\AppData\Local\Temp\kafka-10956541561290772185\version-2\snapshot.0
2025-01-01 00:33:45.084 Snapshot taken in 0 ms
2025-01-01 00:33:45.084 PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-01-01 00:33:45.086 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.bb90f2e1-e671-47f5-92df-c3d35a769c0a15813140817256318929
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60575
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:33:45.089 starting
2025-01-01 00:33:45.089 Connecting to zookeeper on 127.0.0.1:60575
2025-01-01 00:33:45.089 [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:60575.
2025-01-01 00:33:45.089 Initiating client connection, connectString=127.0.0.1:60575 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@3390ee8
2025-01-01 00:33:45.090 SessionTrackerImpl exited loop!
2025-01-01 00:33:45.090 jute.maxbuffer value is 4194304 Bytes
2025-01-01 00:33:45.090 zookeeper.request.timeout value is 0. feature enabled=false
2025-01-01 00:33:45.090 [ZooKeeperClient Kafka server] Waiting until connected.
2025-01-01 00:33:45.090 Opening socket connection to server /127.0.0.1:60575.
2025-01-01 00:33:45.091 Socket connection established, initiating session, client: /127.0.0.1:60576, server: /127.0.0.1:60575
2025-01-01 00:33:45.091 Creating new log file: log.1
2025-01-01 00:33:45.093 Session establishment complete on server /127.0.0.1:60575, session id = 0x100182c033c0000, negotiated timeout = 16000
2025-01-01 00:33:45.093 [ZooKeeperClient Kafka server] Connected.
2025-01-01 00:33:45.114 Cluster ID = 60tnJ95aQSKZ1oXMcUBLpA
2025-01-01 00:33:45.114 No meta.properties file under dir C:\Users\joech\AppData\Local\Temp\spring.kafka.bb90f2e1-e671-47f5-92df-c3d35a769c0a15813140817256318929\meta.properties
2025-01-01 00:33:45.120 KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\joech\AppData\Local\Temp\spring.kafka.bb90f2e1-e671-47f5-92df-c3d35a769c0a15813140817256318929
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:60575
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-01-01 00:33:45.126 [ThrottledChannelReaper-Fetch]: Starting
2025-01-01 00:33:45.128 [ThrottledChannelReaper-Produce]: Starting
2025-01-01 00:33:45.128 [ThrottledChannelReaper-Request]: Starting
2025-01-01 00:33:45.128 [ThrottledChannelReaper-ControllerMutation]: Starting
2025-01-01 00:33:45.131 Loading logs from log dirs ArraySeq(C:\Users\joech\AppData\Local\Temp\spring.kafka.bb90f2e1-e671-47f5-92df-c3d35a769c0a15813140817256318929)
2025-01-01 00:33:45.131 Attempting recovery for all logs in C:\Users\joech\AppData\Local\Temp\spring.kafka.bb90f2e1-e671-47f5-92df-c3d35a769c0a15813140817256318929 since no clean shutdown file was found
2025-01-01 00:33:45.133 Loaded 0 logs in 2ms.
2025-01-01 00:33:45.133 Starting log cleanup with a period of 300000 ms.
2025-01-01 00:33:45.133 Starting log flusher with a default period of 9223372036854775807 ms.
2025-01-01 00:33:45.134 Starting the log cleaner
2025-01-01 00:33:45.135 [kafka-log-cleaner-thread-0]: Starting
2025-01-01 00:33:45.135 [feature-zk-node-event-process-thread]: Starting
2025-01-01 00:33:45.136 Feature ZK node at path: /feature does not exist
2025-01-01 00:33:45.138 [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2025-01-01 00:33:45.147 Updated connection-accept-rate max connection creation rate to 2147483647
2025-01-01 00:33:45.149 Awaiting socket connections on localhost:60577.
2025-01-01 00:33:45.154 [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-01-01 00:33:45.156 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
2025-01-01 00:33:45.156 [ExpirationReaper-0-Produce]: Starting
2025-01-01 00:33:45.157 [ExpirationReaper-0-Fetch]: Starting
2025-01-01 00:33:45.157 [ExpirationReaper-0-DeleteRecords]: Starting
2025-01-01 00:33:45.157 [ExpirationReaper-0-ElectLeader]: Starting
2025-01-01 00:33:45.158 [LogDirFailureHandler]: Starting
2025-01-01 00:33:45.160 Creating /brokers/ids/0 (is it secure? false)
2025-01-01 00:33:45.161 Stat of the created znode at /brokers/ids/0 is: 25,25,1735662825160,1735662825160,1,0,0,72084171349819392,204,0,25

2025-01-01 00:33:45.161 Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:60577, czxid (broker epoch): 25
2025-01-01 00:33:45.165 [ControllerEventThread controllerId=0] Starting
2025-01-01 00:33:45.166 [ExpirationReaper-0-topic]: Starting
2025-01-01 00:33:45.166 [ExpirationReaper-0-Heartbeat]: Starting
2025-01-01 00:33:45.166 [ExpirationReaper-0-Rebalance]: Starting
2025-01-01 00:33:45.166 [GroupCoordinator 0]: Starting up.
2025-01-01 00:33:45.167 [GroupCoordinator 0]: Startup complete.
2025-01-01 00:33:45.167 Successfully created /controller_epoch with initial epoch 0
2025-01-01 00:33:45.170 [TransactionCoordinator id=0] Starting up.
2025-01-01 00:33:45.170 [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-01-01 00:33:45.171 [TransactionCoordinator id=0] Startup complete.
2025-01-01 00:33:45.171 [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-01-01 00:33:45.171 [Transaction Marker Channel Manager 0]: Starting
2025-01-01 00:33:45.171 [ExpirationReaper-0-AlterAcls]: Starting
2025-01-01 00:33:45.172 Feature ZK node created at path: /feature
2025-01-01 00:33:45.173 [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
2025-01-01 00:33:45.173 [Controller id=0] Registering handlers
2025-01-01 00:33:45.173 [/config/changes-event-process-thread]: Starting
2025-01-01 00:33:45.173 [Controller id=0] Deleting log dir event notifications
2025-01-01 00:33:45.175 [Controller id=0] Deleting isr change notifications
2025-01-01 00:33:45.175 [Controller id=0] Initializing controller context
2025-01-01 00:33:45.176 [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-01-01 00:33:45.177 Kafka version: 3.4.1
2025-01-01 00:33:45.177 [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-01-01 00:33:45.177 Kafka commitId: 8a516edc2755df89
2025-01-01 00:33:45.177 Kafka startTimeMs: 1735662825177
2025-01-01 00:33:45.177 Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.server:type=app-info,id=0
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1865)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:960)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:895)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:523)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:563)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:188)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:182)
	at kafka.utils.TestUtils.createServer(TestUtils.scala)
	at org.springframework.kafka.test.EmbeddedKafkaBroker.afterPropertiesSet(EmbeddedKafkaBroker.java:369)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1817)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1766)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:410)
	at org.springframework.kafka.test.context.EmbeddedKafkaContextCustomizer.customizeContext(EmbeddedKafkaContextCustomizer.java:116)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextCustomizerAdapter.initialize(SpringBootContextLoader.java:435)
	at org.springframework.boot.SpringApplication.applyInitializers(SpringApplication.java:610)
	at org.springframework.boot.SpringApplication.prepareContext(SpringApplication.java:390)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:314)
	at org.springframework.boot.test.context.SpringBootContextLoader.lambda$loadContext$3(SpringBootContextLoader.java:137)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:58)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:46)
	at org.springframework.boot.SpringApplication.withHook(SpringApplication.java:1409)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextLoaderHook.run(SpringBootContextLoader.java:545)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:137)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:108)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContextInternal(DefaultCacheAwareContextLoaderDelegate.java:187)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:119)
	at org.springframework.test.context.support.DefaultTestContext.getApplicationContext(DefaultTestContext.java:127)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:141)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:97)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:241)
	at org.spockframework.spring.SpringTestContextManager.prepareTestInstance(SpringTestContextManager.java:56)
	at org.spockframework.spring.SpringInterceptor.interceptInitializerMethod(SpringInterceptor.java:46)
	at org.spockframework.runtime.extension.AbstractMethodInterceptor.intercept(AbstractMethodInterceptor.java:24)
	at org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)
	at org.spockframework.runtime.PlatformSpecRunner.runInitializer(PlatformSpecRunner.java:259)
	at org.spockframework.runtime.PlatformSpecRunner.runInitializer(PlatformSpecRunner.java:254)
	at org.spockframework.runtime.IterationNode.prepare(IterationNode.java:31)
	at org.spockframework.runtime.IterationNode.prepare(IterationNode.java:13)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$prepare$2(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.prepare(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:90)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
	at org.spockframework.runtime.ParameterizedFeatureChildExecutor.execute(ParameterizedFeatureChildExecutor.java:104)
	at org.spockframework.runtime.PlatformParameterizedSpecRunner$1.runIteration(PlatformParameterizedSpecRunner.java:72)
	at org.spockframework.runtime.extension.IDataDriver.lambda$static$0(IDataDriver.java:37)
	at org.spockframework.runtime.PlatformParameterizedSpecRunner.runParameterizedFeature(PlatformParameterizedSpecRunner.java:47)
	at org.spockframework.runtime.ParameterizedFeatureNode.execute(ParameterizedFeatureNode.java:40)
	at org.spockframework.runtime.ParameterizedFeatureNode.execute(ParameterizedFeatureNode.java:16)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)
	at org.spockframework.runtime.FeatureNode.lambda$around$0(FeatureNode.java:41)
	at org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunFeature$4(PlatformSpecRunner.java:199)
	at org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)
	at org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)
	at org.spockframework.runtime.PlatformSpecRunner.runFeature(PlatformSpecRunner.java:192)
	at org.spockframework.runtime.FeatureNode.around(FeatureNode.java:41)
	at org.spockframework.runtime.FeatureNode.around(FeatureNode.java:12)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)
	at org.spockframework.runtime.SpecNode.lambda$around$0(SpecNode.java:63)
	at org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunSpec$0(PlatformSpecRunner.java:61)
	at org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)
	at org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)
	at org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)
	at org.spockframework.runtime.PlatformSpecRunner.runSpec(PlatformSpecRunner.java:55)
	at org.spockframework.runtime.SpecNode.around(SpecNode.java:63)
	at org.spockframework.runtime.SpecNode.around(SpecNode.java:11)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2025-01-01 00:33:45.178 [KafkaServer id=0] started
2025-01-01 00:33:45.178 AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [127.0.0.1:60577]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-01-01 00:33:45.180 [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-01-01 00:33:45.180 [RequestSendThread controllerId=0] Starting
2025-01-01 00:33:45.180 [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-01-01 00:33:45.180 [Controller id=0] Current list of topics in the cluster: HashSet()
2025-01-01 00:33:45.180 [Controller id=0] Fetching topic deletions in progress
2025-01-01 00:33:45.180 Kafka version: 3.4.1
2025-01-01 00:33:45.180 Kafka commitId: 8a516edc2755df89
2025-01-01 00:33:45.180 Kafka startTimeMs: 1735662825180
2025-01-01 00:33:45.181 [Controller id=0] List of topics to be deleted: 
2025-01-01 00:33:45.181 [Controller id=0] List of topics ineligible for deletion: 
2025-01-01 00:33:45.181 [Controller id=0] Initializing topic deletion manager
2025-01-01 00:33:45.181 [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-01-01 00:33:45.181 [Controller id=0] Sending update metadata request
2025-01-01 00:33:45.181 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-01-01 00:33:45.183 [ReplicaStateMachine controllerId=0] Initializing replica state
2025-01-01 00:33:45.183 [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-01-01 00:33:45.183 [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-01-01 00:33:45.183 [PartitionStateMachine controllerId=0] Initializing partition state
2025-01-01 00:33:45.183 [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-01-01 00:33:45.183 [Controller id=0] Ready to serve as the new controller with epoch 1
2025-01-01 00:33:45.183 [RequestSendThread controllerId=0] Controller 0 connected to localhost:60577 (id: 0 rack: null) for sending state change requests
2025-01-01 00:33:45.184 [Controller id=0] Partitions undergoing preferred replica election: 
2025-01-01 00:33:45.184 [Controller id=0] Partitions that completed preferred replica election: 
2025-01-01 00:33:45.184 [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-01-01 00:33:45.184 [Controller id=0] Resuming preferred replica election for partitions: 
2025-01-01 00:33:45.184 [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-01-01 00:33:45.185 [Controller id=0] Starting the controller scheduler
2025-01-01 00:33:45.191 Creating topic pong-message-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-01-01 00:33:45.201 [Controller id=0] New topics: [Set(pong-message-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(pong-message-topic,Some(Jn2ZIVl1SNe4k3o85vkJlg),Map(pong-message-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:33:45.201 [Controller id=0] New partition creation callback for pong-message-topic-0
2025-01-01 00:33:45.202 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:45.202 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:45.202 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:45.206 [Controller id=0 epoch=1] Changed partition pong-message-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:45.206 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
2025-01-01 00:33:45.206 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions
2025-01-01 00:33:45.207 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:45.207 [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 1 partitions
2025-01-01 00:33:45.208 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pong-message-topic-0)
2025-01-01 00:33:45.208 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
2025-01-01 00:33:45.215 [LogLoader partition=pong-message-topic-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.bb90f2e1-e671-47f5-92df-c3d35a769c0a15813140817256318929] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:45.215 Created log for partition pong-message-topic-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.bb90f2e1-e671-47f5-92df-c3d35a769c0a15813140817256318929\pong-message-topic-0 with properties {}
2025-01-01 00:33:45.215 [Partition pong-message-topic-0 broker=0] No checkpointed highwatermark is found for partition pong-message-topic-0
2025-01-01 00:33:45.215 [Partition pong-message-topic-0 broker=0] Log loaded for partition pong-message-topic-0 with initial high watermark 0
2025-01-01 00:33:45.215 [Broker id=0] Leader pong-message-topic-0 with topic id Some(Jn2ZIVl1SNe4k3o85vkJlg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:45.217 [Broker id=0] Finished LeaderAndIsr request in 10ms correlationId 1 from controller 0 for 1 partitions
2025-01-01 00:33:45.220 [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-01-01 00:33:45.221 App info kafka.admin.client for adminclient-3 unregistered
2025-01-01 00:33:45.223 Metrics scheduler closed
2025-01-01 00:33:45.223 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:33:45.223 Metrics reporters closed
2025-01-01 00:33:45.224 Starting PongControllerSpec using Java 17.0.12 with PID 8840 (started by joech in C:\code\hsbc\demo\pong)
2025-01-01 00:33:45.224 No active profile set, falling back to 1 default profile: "default"
2025-01-01 00:33:45.238 [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:60577 (id: 0 rack: null)
2025-01-01 00:33:45.256 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:60577 (id: 0 rack: null)
2025-01-01 00:33:45.324 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:33:45.324 Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-01 00:33:45.337 Finished Spring Data repository scanning in 12 ms. Found 1 JPA repository interfaces.
2025-01-01 00:33:45.341 Multiple Spring Data modules found, entering strict repository configuration mode
2025-01-01 00:33:45.341 Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-01-01 00:33:45.345 Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.demo.dao.MessageRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-01-01 00:33:45.345 Finished Spring Data repository scanning in 3 ms. Found 0 Redis repository interfaces.
2025-01-01 00:33:45.475 HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-01 00:33:45.482 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:33:45.483 No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-01 00:33:45.483 HikariPool-3 - Starting...
2025-01-01 00:33:45.485 HikariPool-3 - Added connection conn20: url=jdbc:h2:mem:testdb user=SA
2025-01-01 00:33:45.485 HikariPool-3 - Start completed.
2025-01-01 00:33:45.492 HHH000021: Bytecode provider name : bytebuddy
2025-01-01 00:33:45.513 HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2025-01-01 00:33:45.526 Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:33:45.781 Netty started on port 60581
2025-01-01 00:33:45.784 ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:60571]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pong-service-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pong-service
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-01-01 00:33:45.786 These configurations '[spring.json.trusted.packages]' were supplied but are not used yet.
2025-01-01 00:33:45.788 Kafka version: 3.4.1
2025-01-01 00:33:45.788 Kafka commitId: 8a516edc2755df89
2025-01-01 00:33:45.788 Kafka startTimeMs: 1735662825786
2025-01-01 00:33:45.788 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Subscribed to topic(s): pong-message-topic
2025-01-01 00:33:45.790 Started PongControllerSpec in 0.738 seconds (process running for 22.372)
2025-01-01 00:33:45.791 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to FZcDAS02TW62SFYRy8RIPg
2025-01-01 00:33:45.793 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to FZcDAS02TW62SFYRy8RIPg
2025-01-01 00:33:45.793 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cluster ID: 6rkPX64FT-udWfqvJqeuSg
2025-01-01 00:33:45.795 Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-01-01 00:33:45.802 [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(xHGCMwiHSHiTr_juGDR5RQ),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-01-01 00:33:45.802 [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-01-01 00:33:45.802 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:45.802 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:45.802 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:45.802 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:45.802 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-01-01 00:33:45.802 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:45.802 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:45.810 [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:45.810 [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:45.810 [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:45.810 [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:45.810 [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-01-01 00:33:45.811 [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-01-01 00:33:45.811 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-01-01 00:33:45.811 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:45.812 [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:33:45.814 [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-01-01 00:33:45.814 [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-01-01 00:33:45.822 [LogLoader partition=__consumer_offsets-3, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:45.822 Created log for partition __consumer_offsets-3 in C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654\__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:33:45.824 [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-01-01 00:33:45.824 [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-01-01 00:33:45.824 [Broker id=0] Leader __consumer_offsets-3 with topic id Some(xHGCMwiHSHiTr_juGDR5RQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:45.833 [LogLoader partition=__consumer_offsets-2, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:45.834 Created log for partition __consumer_offsets-2 in C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654\__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:33:45.834 [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-01-01 00:33:45.834 [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-01-01 00:33:45.834 [Broker id=0] Leader __consumer_offsets-2 with topic id Some(xHGCMwiHSHiTr_juGDR5RQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:45.841 [LogLoader partition=__consumer_offsets-4, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:45.843 Created log for partition __consumer_offsets-4 in C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654\__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:33:45.843 [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-01-01 00:33:45.843 [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-01-01 00:33:45.843 [Broker id=0] Leader __consumer_offsets-4 with topic id Some(xHGCMwiHSHiTr_juGDR5RQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:45.852 [LogLoader partition=__consumer_offsets-1, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:45.853 Created log for partition __consumer_offsets-1 in C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654\__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:33:45.853 [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-01-01 00:33:45.853 [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-01-01 00:33:45.853 [Broker id=0] Leader __consumer_offsets-1 with topic id Some(xHGCMwiHSHiTr_juGDR5RQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:45.862 [LogLoader partition=__consumer_offsets-0, dir=C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654] Loading producer state till offset 0 with message format version 2
2025-01-01 00:33:45.863 Created log for partition __consumer_offsets-0 in C:\Users\joech\AppData\Local\Temp\spring.kafka.adf29d07-2cb2-410c-81fb-79a9b49e67d010336984224859639654\__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-01-01 00:33:45.863 [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-01-01 00:33:45.863 [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-01-01 00:33:45.863 [Broker id=0] Leader __consumer_offsets-0 with topic id Some(xHGCMwiHSHiTr_juGDR5RQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
2025-01-01 00:33:45.866 [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-01-01 00:33:45.866 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-01-01 00:33:45.866 [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-01-01 00:33:45.866 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-01-01 00:33:45.866 [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-01-01 00:33:45.866 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-01-01 00:33:45.866 [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-01-01 00:33:45.867 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-01-01 00:33:45.867 [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-01-01 00:33:45.867 [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-01-01 00:33:45.867 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:33:45.867 [Broker id=0] Finished LeaderAndIsr request in 55ms correlationId 3 from controller 0 for 5 partitions
2025-01-01 00:33:45.867 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler.
2025-01-01 00:33:45.867 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler.
2025-01-01 00:33:45.867 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:33:45.867 [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-01-01 00:33:45.869 [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-01-01 00:33:45.896 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Discovered group coordinator localhost:60571 (id: 2147483647 rack: null)
2025-01-01 00:33:45.897 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] (Re-)joining group
2025-01-01 00:33:45.900 [GroupCoordinator 0]: Dynamic member with unknown member id joins group pong-service in Empty state. Created a new member id consumer-pong-service-3-796e6521-9f51-4e77-9e9e-d00537f0e97c and request the member to rejoin with this id.
2025-01-01 00:33:45.901 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: need to re-join with the given member-id: consumer-pong-service-3-796e6521-9f51-4e77-9e9e-d00537f0e97c
2025-01-01 00:33:45.901 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2025-01-01 00:33:45.901 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] (Re-)joining group
2025-01-01 00:33:45.902 [GroupCoordinator 0]: Preparing to rebalance group pong-service in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-pong-service-3-796e6521-9f51-4e77-9e9e-d00537f0e97c with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)
2025-01-01 00:33:45.902 [GroupCoordinator 0]: Stabilized group pong-service generation 1 (__consumer_offsets-2) with 1 members
2025-01-01 00:33:45.904 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Successfully joined group with generation Generation{generationId=1, memberId='consumer-pong-service-3-796e6521-9f51-4e77-9e9e-d00537f0e97c', protocol='range'}
2025-01-01 00:33:45.904 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Finished assignment for group at generation 1: {consumer-pong-service-3-796e6521-9f51-4e77-9e9e-d00537f0e97c=Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])}
2025-01-01 00:33:45.905 [GroupCoordinator 0]: Assignment received from leader consumer-pong-service-3-796e6521-9f51-4e77-9e9e-d00537f0e97c for group pong-service for generation 1. The group has 1 members, 0 of which are static.
2025-01-01 00:33:45.907 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Successfully synced group in generation Generation{generationId=1, memberId='consumer-pong-service-3-796e6521-9f51-4e77-9e9e-d00537f0e97c', protocol='range'}
2025-01-01 00:33:45.907 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Notifying assignor about the new Assignment(partitions=[pong-message-topic-0, pong-message-topic-1])
2025-01-01 00:33:45.907 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Adding newly assigned partitions: pong-message-topic-0, pong-message-topic-1
2025-01-01 00:33:45.908 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:33:45.908 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:33:45.910 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-0
2025-01-01 00:33:45.910 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Found no committed offset for partition pong-message-topic-1
2025-01-01 00:33:45.911 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting offset for partition pong-message-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60571 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:33:45.911 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting offset for partition pong-message-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:60571 (id: 0 rack: null)], epoch=0}}.
2025-01-01 00:33:45.914 pong-service: partitions assigned: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:33:46.087 Received ping message: Hello
2025-01-01 00:33:46.088 Idempotence will be disabled because acks is set to 0, not set to 'all'.
2025-01-01 00:33:46.088 ProducerConfig values: 
	acks = 0
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:60571]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-01-01 00:33:46.092 Kafka version: 3.4.1
2025-01-01 00:33:46.092 Kafka commitId: 8a516edc2755df89
2025-01-01 00:33:46.092 Kafka startTimeMs: 1735662826092
2025-01-01 00:33:46.095 [Producer clientId=producer-2] Resetting the last seen epoch of partition pong-message-topic-0 to 0 since the associated topicId changed from null to FZcDAS02TW62SFYRy8RIPg
2025-01-01 00:33:46.095 [Producer clientId=producer-2] Resetting the last seen epoch of partition pong-message-topic-1 to 0 since the associated topicId changed from null to FZcDAS02TW62SFYRy8RIPg
2025-01-01 00:33:46.097 [Producer clientId=producer-2] Cluster ID: 6rkPX64FT-udWfqvJqeuSg
2025-01-01 00:33:47.279 Received ping message: Hello
2025-01-01 00:33:47.283 Throttling request as too many requests in the same second
2025-01-01 00:33:47.283 Throttling request as too many requests in the same second
2025-01-01 00:33:47.317 [KafkaServer id=0] shutting down
2025-01-01 00:33:47.317 [KafkaServer id=0] Starting controlled shutdown
2025-01-01 00:33:47.319 [Controller id=0] Shutting down broker 0
2025-01-01 00:33:47.319 [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-01-01 00:33:47.320 [KafkaServer id=0] Controlled shutdown request returned successfully after 1ms
2025-01-01 00:33:47.320 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:33:47.321 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:33:47.321 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:33:47.321 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:33:47.321 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 2147483647 disconnected.
2025-01-01 00:33:47.321 [Producer clientId=producer-2] Node -1 disconnected.
2025-01-01 00:33:47.321 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Group coordinator localhost:60571 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2025-01-01 00:33:47.322 [Producer clientId=producer-2] Node 0 disconnected.
2025-01-01 00:33:47.322 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:33:47.322 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cancelled in-flight FETCH request with correlation id 19 due to node 0 being disconnected (elapsed time since creation: 39ms, elapsed time since send: 39ms, request timeout: 30000ms)
2025-01-01 00:33:47.322 [Producer clientId=producer-2] Cancelled in-flight METADATA request with correlation id 5 due to node 0 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, request timeout: 30000ms)
2025-01-01 00:33:47.322 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Cancelled in-flight METADATA request with correlation id 22 due to node 0 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, request timeout: 30000ms)
2025-01-01 00:33:47.322 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:33:47.322 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node -1 disconnected.
2025-01-01 00:33:47.322 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:33:47.322 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Error sending fetch request (sessionId=1464971190, epoch=4) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
2025-01-01 00:33:47.322 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:33:47.322 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:33:47.323 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:33:47.323 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:33:47.323 [KafkaApi-0] Shutdown complete.
2025-01-01 00:33:47.323 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:33:47.323 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:33:47.323 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:33:47.323 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:33:47.323 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:33:47.323 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:33:47.324 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:33:47.324 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:33:47.324 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:33:47.324 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:33:47.324 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:33:47.324 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:33:47.324 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:33:47.324 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:33:47.324 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:33:47.324 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:33:47.324 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:33:47.324 [ReplicaManager broker=0] Shutting down
2025-01-01 00:33:47.324 [LogDirFailureHandler]: Shutting down
2025-01-01 00:33:47.324 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:33:47.324 [LogDirFailureHandler]: Stopped
2025-01-01 00:33:47.324 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:33:47.325 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:33:47.325 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:33:47.325 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:33:47.325 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:33:47.325 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:33:47.325 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:33:47.325 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:33:47.325 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:33:47.325 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:33:47.325 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:33:47.325 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:33:47.325 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:33:47.325 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:33:47.326 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:33:47.326 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:33:47.329 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:33:47.329 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:33:47.329 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:33:47.329 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:33:47.330 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:33:47.330 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:33:47.330 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:33:47.330 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:33:47.330 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:33:47.330 Shutting down.
2025-01-01 00:33:47.330 Shutting down the log cleaner.
2025-01-01 00:33:47.330 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:33:47.330 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:33:47.330 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:33:47.343 [ProducerStateManager partition=pong-message-topic-1] Wrote producer snapshot at offset 2 with 0 producer ids in 1 ms.
2025-01-01 00:33:47.351 [ProducerStateManager partition=__consumer_offsets-2] Wrote producer snapshot at offset 5 with 0 producer ids in 1 ms.
2025-01-01 00:33:47.356 Shutdown complete.
2025-01-01 00:33:47.356 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:33:47.356 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:33:47.356 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:33:47.358 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:33:47.358 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:33:47.358 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:33:47.358 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:33:47.358 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:33:47.359 [Controller id=0] Resigned
2025-01-01 00:33:47.359 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:33:47.359 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:33:47.359 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:33:47.359 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:33:47.424 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Node 0 disconnected.
2025-01-01 00:33:47.424 [Producer clientId=producer-2] Node 0 disconnected.
2025-01-01 00:33:47.424 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Connection to node 0 (localhost/127.0.0.1:60571) could not be established. Broker may not be available.
2025-01-01 00:33:47.424 [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:60571) could not be established. Broker may not be available.
2025-01-01 00:33:47.461 EventThread shut down for session: 0x100182c02680000
2025-01-01 00:33:47.461 Session: 0x100182c02680000 closed
2025-01-01 00:33:47.461 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:33:47.461 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:33:47.461 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:33:47.461 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:33:47.461 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:33:47.461 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:33:47.461 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:33:47.461 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:33:47.461 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:33:47.461 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:33:47.461 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:33:47.461 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:33:47.461 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:33:47.462 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:33:47.469 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:33:47.469 Metrics scheduler closed
2025-01-01 00:33:47.469 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:33:47.469 Metrics reporters closed
2025-01-01 00:33:47.469 Broker and topic stats closed
2025-01-01 00:33:47.469 App info kafka.server for 0 unregistered
2025-01-01 00:33:47.469 [KafkaServer id=0] shut down completed
2025-01-01 00:33:47.481 ConnnectionExpirerThread interrupted
2025-01-01 00:33:47.481 accept thread exitted run method
2025-01-01 00:33:47.482 selector thread exitted run method
2025-01-01 00:33:47.482 selector thread exitted run method
2025-01-01 00:33:47.482 shutting down
2025-01-01 00:33:47.482 Shutting down
2025-01-01 00:33:47.482 Draining request throttler queue
2025-01-01 00:33:47.482 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:33:47.482 Shutting down
2025-01-01 00:33:47.482 Shutting down
2025-01-01 00:33:47.482 Shutting down
2025-01-01 00:33:47.483 SyncRequestProcessor exited!
2025-01-01 00:33:47.483 PrepRequestProcessor exited loop!
2025-01-01 00:33:47.483 shutdown of request processor complete
2025-01-01 00:33:47.489 SessionTrackerImpl exited loop!
2025-01-01 00:33:47.498 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Revoke previously assigned partitions pong-message-topic-0, pong-message-topic-1
2025-01-01 00:33:47.498 pong-service: partitions revoked: [pong-message-topic-0, pong-message-topic-1]
2025-01-01 00:33:47.498 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:33:47.498 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:33:47.498 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Unsubscribed all topics or patterns and assigned partitions
2025-01-01 00:33:47.498 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-01 00:33:47.498 [Consumer clientId=consumer-pong-service-3, groupId=pong-service] Request joining group due to: consumer pro-actively leaving the group
2025-01-01 00:33:47.498 Metrics scheduler closed
2025-01-01 00:33:47.498 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:33:47.498 Metrics reporters closed
2025-01-01 00:33:47.499 App info kafka.consumer for consumer-pong-service-3 unregistered
2025-01-01 00:33:47.499 pong-service: Consumer stopped
2025-01-01 00:33:47.502 [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-01 00:33:47.503 Metrics scheduler closed
2025-01-01 00:33:47.504 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:33:47.504 Metrics reporters closed
2025-01-01 00:33:47.504 App info kafka.producer for producer-2 unregistered
2025-01-01 00:33:47.506 Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-01 00:33:47.507 HikariPool-3 - Shutdown initiated...
2025-01-01 00:33:47.508 HikariPool-3 - Shutdown completed.
2025-01-01 00:33:49.529 [KafkaServer id=0] shutting down
2025-01-01 00:33:49.529 [/config/changes-event-process-thread]: Shutting down
2025-01-01 00:33:49.529 [/config/changes-event-process-thread]: Stopped
2025-01-01 00:33:49.529 [/config/changes-event-process-thread]: Shutdown completed
2025-01-01 00:33:49.529 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-01-01 00:33:49.531 [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-01-01 00:33:49.531 [data-plane Kafka Request Handler on Broker 0], shutting down
2025-01-01 00:33:49.531 [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-01-01 00:33:49.531 [ExpirationReaper-0-AlterAcls]: Shutting down
2025-01-01 00:33:49.533 [ExpirationReaper-0-AlterAcls]: Stopped
2025-01-01 00:33:49.533 [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-01-01 00:33:49.533 [KafkaApi-0] Shutdown complete.
2025-01-01 00:33:49.533 [ExpirationReaper-0-topic]: Shutting down
2025-01-01 00:33:49.533 [ExpirationReaper-0-topic]: Stopped
2025-01-01 00:33:49.533 [ExpirationReaper-0-topic]: Shutdown completed
2025-01-01 00:33:49.533 [TransactionCoordinator id=0] Shutting down.
2025-01-01 00:33:49.533 [Transaction State Manager 0]: Shutdown complete
2025-01-01 00:33:49.533 [Transaction Marker Channel Manager 0]: Shutting down
2025-01-01 00:33:49.533 [Transaction Marker Channel Manager 0]: Stopped
2025-01-01 00:33:49.533 [Transaction Marker Channel Manager 0]: Shutdown completed
2025-01-01 00:33:49.534 [TransactionCoordinator id=0] Shutdown complete.
2025-01-01 00:33:49.534 [GroupCoordinator 0]: Shutting down.
2025-01-01 00:33:49.534 [ExpirationReaper-0-Heartbeat]: Shutting down
2025-01-01 00:33:49.534 [ExpirationReaper-0-Heartbeat]: Stopped
2025-01-01 00:33:49.534 [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-01-01 00:33:49.534 [ExpirationReaper-0-Rebalance]: Shutting down
2025-01-01 00:33:49.534 [ExpirationReaper-0-Rebalance]: Stopped
2025-01-01 00:33:49.534 [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-01-01 00:33:49.534 [GroupCoordinator 0]: Shutdown complete.
2025-01-01 00:33:49.534 [ReplicaManager broker=0] Shutting down
2025-01-01 00:33:49.534 [LogDirFailureHandler]: Shutting down
2025-01-01 00:33:49.534 [LogDirFailureHandler]: Stopped
2025-01-01 00:33:49.534 [LogDirFailureHandler]: Shutdown completed
2025-01-01 00:33:49.534 [ReplicaFetcherManager on broker 0] shutting down
2025-01-01 00:33:49.534 [ReplicaFetcherManager on broker 0] shutdown completed
2025-01-01 00:33:49.534 [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-01-01 00:33:49.534 [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-01-01 00:33:49.534 [ExpirationReaper-0-Fetch]: Shutting down
2025-01-01 00:33:49.535 [ExpirationReaper-0-Fetch]: Stopped
2025-01-01 00:33:49.535 [ExpirationReaper-0-Fetch]: Shutdown completed
2025-01-01 00:33:49.535 [ExpirationReaper-0-Produce]: Shutting down
2025-01-01 00:33:49.535 [ExpirationReaper-0-Produce]: Stopped
2025-01-01 00:33:49.535 [ExpirationReaper-0-Produce]: Shutdown completed
2025-01-01 00:33:49.535 [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-01-01 00:33:49.535 [ExpirationReaper-0-DeleteRecords]: Stopped
2025-01-01 00:33:49.535 [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-01-01 00:33:49.535 [ExpirationReaper-0-ElectLeader]: Shutting down
2025-01-01 00:33:49.535 [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-01-01 00:33:49.535 [ExpirationReaper-0-ElectLeader]: Stopped
2025-01-01 00:33:49.540 [ReplicaManager broker=0] Shut down completely
2025-01-01 00:33:49.540 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
2025-01-01 00:33:49.540 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
2025-01-01 00:33:49.540 [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
2025-01-01 00:33:49.541 Broker to controller channel manager for alterPartition shutdown
2025-01-01 00:33:49.541 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2025-01-01 00:33:49.541 [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2025-01-01 00:33:49.541 [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2025-01-01 00:33:49.541 Broker to controller channel manager for forwarding shutdown
2025-01-01 00:33:49.541 Shutting down.
2025-01-01 00:33:49.542 Shutting down the log cleaner.
2025-01-01 00:33:49.542 [kafka-log-cleaner-thread-0]: Shutting down
2025-01-01 00:33:49.542 [kafka-log-cleaner-thread-0]: Shutdown completed
2025-01-01 00:33:49.542 [kafka-log-cleaner-thread-0]: Stopped
2025-01-01 00:33:49.548 Shutdown complete.
2025-01-01 00:33:49.548 [ControllerEventThread controllerId=0] Shutting down
2025-01-01 00:33:49.549 [ControllerEventThread controllerId=0] Stopped
2025-01-01 00:33:49.549 [ControllerEventThread controllerId=0] Shutdown completed
2025-01-01 00:33:49.549 [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-01-01 00:33:49.549 [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-01-01 00:33:49.549 [RequestSendThread controllerId=0] Shutting down
2025-01-01 00:33:49.549 [RequestSendThread controllerId=0] Stopped
2025-01-01 00:33:49.549 [RequestSendThread controllerId=0] Shutdown completed
2025-01-01 00:33:49.550 [Controller id=0] Resigned
2025-01-01 00:33:49.550 [feature-zk-node-event-process-thread]: Shutting down
2025-01-01 00:33:49.550 [feature-zk-node-event-process-thread]: Stopped
2025-01-01 00:33:49.550 [feature-zk-node-event-process-thread]: Shutdown completed
2025-01-01 00:33:49.550 [ZooKeeperClient Kafka server] Closing.
2025-01-01 00:33:49.655 EventThread shut down for session: 0x100182c033c0000
2025-01-01 00:33:49.655 Session: 0x100182c033c0000 closed
2025-01-01 00:33:49.655 [ZooKeeperClient Kafka server] Closed.
2025-01-01 00:33:49.655 [ThrottledChannelReaper-Fetch]: Shutting down
2025-01-01 00:33:49.655 [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-01-01 00:33:49.655 [ThrottledChannelReaper-Produce]: Shutting down
2025-01-01 00:33:49.655 [ThrottledChannelReaper-Fetch]: Stopped
2025-01-01 00:33:49.655 [ThrottledChannelReaper-Produce]: Stopped
2025-01-01 00:33:49.655 [ThrottledChannelReaper-Produce]: Shutdown completed
2025-01-01 00:33:49.655 [ThrottledChannelReaper-Request]: Shutting down
2025-01-01 00:33:49.655 [ThrottledChannelReaper-Request]: Shutdown completed
2025-01-01 00:33:49.655 [ThrottledChannelReaper-Request]: Stopped
2025-01-01 00:33:49.655 [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-01-01 00:33:49.656 [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-01-01 00:33:49.656 [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-01-01 00:33:49.656 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-01-01 00:33:49.661 [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-01-01 00:33:49.661 Metrics scheduler closed
2025-01-01 00:33:49.661 Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-01 00:33:49.661 Metrics reporters closed
2025-01-01 00:33:49.661 Broker and topic stats closed
2025-01-01 00:33:49.661 App info kafka.server for 0 unregistered
2025-01-01 00:33:49.661 [KafkaServer id=0] shut down completed
2025-01-01 00:33:49.665 ConnnectionExpirerThread interrupted
2025-01-01 00:33:49.665 selector thread exitted run method
2025-01-01 00:33:49.665 accept thread exitted run method
2025-01-01 00:33:49.665 selector thread exitted run method
2025-01-01 00:33:49.665 shutting down
2025-01-01 00:33:49.665 Shutting down
2025-01-01 00:33:49.665 Draining request throttler queue
2025-01-01 00:33:49.665 RequestThrottler shutdown. Dropped 0 requests
2025-01-01 00:33:49.665 Shutting down
2025-01-01 00:33:49.665 Shutting down
2025-01-01 00:33:49.665 Shutting down
2025-01-01 00:33:49.665 PrepRequestProcessor exited loop!
2025-01-01 00:33:49.665 SyncRequestProcessor exited!
2025-01-01 00:33:49.665 shutdown of request processor complete
